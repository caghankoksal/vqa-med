{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mlmi_evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jJNBvQn_5AOR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycocoevalcap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKOX0sljHPVf",
        "outputId": "a55b3a17-7744-4436-f502-b4e8a569bd58"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pycocoevalcap in /usr/local/lib/python3.7/dist-packages (1.2)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from pycocoevalcap) (2.0.4)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools>=2.0.2->pycocoevalcap) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pycocotools>=2.0.2->pycocoevalcap) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.4.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for scores\n",
        "from sklearn.metrics import roc_auc_score, f1_score, recall_score, precision_score\n",
        "\n",
        "from pycocoevalcap.bleu.bleu import Bleu\n",
        "from pycocoevalcap.meteor.meteor import Meteor\n",
        "from pycocoevalcap.rouge.rouge import Rouge\n",
        "from pycocoevalcap.cider.cider import Cider"
      ],
      "metadata": {
        "id": "EaplDOu3HVX4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# just a temporary tokenizer, TODO: replace with our tokenizer"
      ],
      "metadata": {
        "id": "4IBaY3tMLb6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install nltk package\n",
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3xzRoFKLWGM",
        "outputId": "c4008648-100a-4d05-b2e2-395492f982ef"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "print('Using NLTK version {}.'.format(nltk.__version__))\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PA-RmwsxLolw",
        "outputId": "428b3c96-94a2-424a-fa93-f7d425f79157"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using NLTK version 3.7.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLMI-CXR Report Generation METRICS\n",
        "## I. BLEU:\n",
        "\n",
        "Based on the paper: *BLEU: a Method for Automatic Evaluation of Machine Translation* \n",
        "-https://aclanthology.org/P02-1040.pdf \n",
        "\n",
        "- **RANGE:** [0: low correlation - 1: high correlation]\n",
        "\n",
        "- **ABOUT:** BLEU (BiLingual Evaluation Understudy) is a\n",
        "popular metric for Machine Translation (MT) evaluation. It\n",
        "computes an n-gram based precision for the candidate sentence with respect to the references. The key idea of BLEU\n",
        "is to compute precision by clipping. Clipping computes precision for a word, based on the maximum number of times\n",
        "it occurs in any reference sentence. Thus, a candidate sentence saying “The The The”, would get credit for saying\n",
        "only one “The”, if the word occurs at most once across individual references. BLEU computes the geometric mean\n",
        "of the n-gram precisions and adds a brevity-penalty to discourage overly short sentences. The most common formulation of BLEU is BLEU4, which uses 1-grams up to 4-grams, though lower-order variations such as BLEU1 (unigram BLEU) and BLEU2 (unigram and bigram BLEU) are\n",
        "also used. We compute BLEU at the sentence level. For machine\n",
        "translation BLEU is most often computed at the corpus level\n",
        "where correlation with human judgment is high; the correlation is **poor at the level of individual sentences**.\n",
        "\n",
        "- **USAGE:** We use BLEU 1-gram, 2-gram, 3-gram, 4-gram\n",
        "\n",
        "\n",
        "## II. METEOR:\n",
        "Based on the paper: *Meteor Universal: Language Specific Translation Evaluation for Any Target Language* \n",
        "-https://www.cs.cmu.edu/~alavie/METEOR/\n",
        "\n",
        "- **RANGE:** [0: low correlation - 1: high correlation]\n",
        "\n",
        "- **ABOUT:** stands for Metric for Evaluation of Translation with Explicit ORdering. It computes the F-measure based on matches, and returns the maximum score over a set of references as its\n",
        "judgment of quality. It resolves word-level correspondences in a more sophisticated manner, using exact\n",
        "matches, stemming and semantic similarity. It optimizes\n",
        "over matches minimizing chunkiness. Minimizing chunkiness implies that matches should be consecutive, wherever\n",
        "possible. It also sets parameters favoring recall over precision in its F-measure computation.\n",
        "\n",
        "- **USAGE:** We use the latest version's (1.5) implementation\n",
        "\n",
        "## III: ROUGE:\n",
        "Based on the paper: *ROUGE: A Package for Automatic Evaluation of Summaries* \n",
        "-https://aclanthology.org/W04-1013.pdf\n",
        "\n",
        "- **RANGE:** [0: low correlation - 1: high correlation]\n",
        "\n",
        "- **ABOUT:** ROUGE stands for Recall Oriented Understudy of Gisting Evaluation. It computes n-gram based recall for the candidate sentence with respect\n",
        "to the references. It is a popular metric for summarization\n",
        "evaluation. Versions of ROUGE can be\n",
        "computed by varying the n-gram count. Two other versions of ROUGE are ROUGE-S and ROUGE-L. These compute an\n",
        "F-measure with a recall bias using skip-bigrams and longest\n",
        "common subsequence respectively, between the candidate\n",
        "and each reference sentence. Skip-bigrams are all pairs of\n",
        "ordered words in a sentence, sampled non-consecutively.\n",
        "Given these scores, they return the maximum score across\n",
        "the set of references as the judgment of quality.\n",
        "\n",
        "- **USAGE:** We use the Rouge-L (Longest Common Subsequence) implementation\n",
        "\n",
        "## IV: CIDEr:\n",
        "Based on the paper: *CIDEr: Consensus-based Image Description Evaluation* \n",
        "-https://arxiv.org/pdf/1411.5726.pdf\n",
        "\n",
        "- **RANGE:** [0: low correlation - 1: high correlation]\n",
        "\n",
        "- **ABOUT:**  CIDEr is an automatic consensus metric of image\n",
        "description quality – CIDEr (Consensus-based Image Description Evaluation). It measures the similarity of\n",
        "a generated sentence against a set of ground truth sentences\n",
        "written by humans. Using sentence similarity, the notions of grammaticality, saliency, importance and\n",
        "accuracy (precision and recall) are inherently captured by\n",
        "this metric.\n",
        "\n",
        "- **USAGE:** We use the default CIDEr implementation\n"
      ],
      "metadata": {
        "id": "Cwg4auyDcdi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_scores(out, gt):\n",
        "    \"\"\"\n",
        "    Performs the evaluation of metrics using pycocoevalcap (https://github.com/salaniz/pycocoevalcap)\n",
        "    :param out: dictionary containing the generated report and the images,\n",
        "    :param gt: dictionary containing the reports ground truth and the images,\n",
        "    \"\"\"\n",
        "    metrics = [\n",
        "        (Bleu(4), [\"BLEU_1\", \"BLEU_2\", \"BLEU_3\", \"BLEU_4\"]),\n",
        "        (Meteor(), \"METEOR\"),\n",
        "        (Rouge(), \"ROUGE_L\"),\n",
        "        (Cider(), \"CIDER\")\n",
        "    ]\n",
        "\n",
        "    result = {}\n",
        "    # Compute score for each metric\n",
        "    for scorer, method in metrics:\n",
        "        try:\n",
        "            score, _ = scorer.compute_score(out, gt, verbose=0)\n",
        "        except TypeError:\n",
        "            score, _ = scorer.compute_score(out, gt)\n",
        "        if type(method) == list:\n",
        "            for score_i, method_i in zip(score, method):\n",
        "                result[method_i] = score_i\n",
        "        else:\n",
        "            result[method] = score\n",
        "    return result"
      ],
      "metadata": {
        "id": "h9UVjprn5ji5"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated = \"FINAL REPORT CHEST ON  HISTORY:  Status post chest pain.  Evaluate for acute process. FINDINGS:  In comparison with prior study from the study.  A nasogastinal silhouette appears grossly clear.  The lungs are stable.\"\n",
        "gen_modif = \"FINAL REPORT CHEST ON  HISTORY:  Status post chest pain.  Evaluate for acute process. FINDINGS:  In comparison with prior study from the study.  A nasogastinal silhouette appears grossly clear.  The lungs are stable.\"\n",
        "\n",
        "ground_truth = \"FINAL REPORT EXAMINATION:  CHEST (PA AND LAT) INDICATION:  History: with shortness of breath TECHNIQUE:  Chest PA and lateral COMPARISON: chest radiograph FINDINGS: Mild to moderate enlargement of the cardiac silhouette is unchanged. The aorta remains tortuous. Mediastinal and hilar contours are otherwise similar. The pulmonary vasculature is not engorged. Lungs are hyperinflated. Apart from minimal atelectasis in the lung bases and biapical scarring in the lung apices, more pronounced on the right, the lungs are clear. No pleural effusion or pneumothorax is demonstrated. There are no acute osseous abnormalities. IMPRESSION: No acute cardiopulmonary abnormality.\""
      ],
      "metadata": {
        "id": "VWf2RZ7bDyvY"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generated = nltk.word_tokenize(generated)\n",
        "# gen_modif = nltk.word_tokenize(gen_modif)\n",
        "# ground_truth = nltk.word_tokenize(ground_truth)"
      ],
      "metadata": {
        "id": "93OBBj3BL0E2"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_id = 5151\n",
        "\n",
        "out_lst = []\n",
        "out_lst.append(generated)\n",
        "\n",
        "gt_lst = []\n",
        "gt_lst.append(gen_modif)\n",
        "\n",
        "#pycocoevalcap uses dictionary where the id of the image is the key and the value is the generated/ground truth report\n",
        "out_dict = {img_id: out_lst}\n",
        "gt_dict = {img_id: gt_lst}"
      ],
      "metadata": {
        "id": "6W_rUCOyKZX9"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = compute_scores(out_dict, gt_dict)"
      ],
      "metadata": {
        "id": "DZ2d0coHII-X"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7arhXoyIU46",
        "outputId": "9288e773-d116-4426-fee7-c8b004921707"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'BLEU_1': 0.9999999999375002,\n",
              " 'BLEU_2': 0.9999999999369961,\n",
              " 'BLEU_3': 0.9999999999364697,\n",
              " 'BLEU_4': 0.999999999935919,\n",
              " 'CIDER': 0.0,\n",
              " 'METEOR': 1.0,\n",
              " 'ROUGE_L': 1.0}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keys = list(result.keys())\n",
        "vals = list(result.values())\n",
        "sns.barplot(x=keys, y=vals)\n",
        "plt.xlabel(\"Metrics\")\n",
        "plt.ylabel('Score')\n",
        "#plt.title('')\n",
        "ax = plt.gca()\n",
        "ax.set_ylim([0, 1])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "eMKL_Z35ZN2A",
        "outputId": "a52ceea4-6839-4a68-b481-18209780b9cb"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "M-bHSVi6ltRS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}