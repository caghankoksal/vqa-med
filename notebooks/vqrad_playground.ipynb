{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import torch\n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "from src.datasets.vqa_rad_dataset import VQRadDataModule\n",
    "from src.models.multimodal.flamingo_module import FlamingoModule\n",
    "\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from pytorch_lightning.loggers import CometLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "from src.utils.utils import load_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load memory False\n",
      "There are 2248 QA pairs in VQA-RAD dataset\n",
      "Training set has 1797 Test set has 451 questions\n",
      "Num unique answers vqa-rad  517\n",
      "BERT Tokenizer is initalized\n",
      "Num unique answers vqa-rad  517\n",
      "BERT Tokenizer is initalized\n",
      "Num unique answers vqa-rad  517\n",
      "BERT Tokenizer is initalized\n",
      "Clip architecture is being loaded\n",
      "Clip pretrained weights are being loaded\n",
      "Flamingo is being initialized with  bert  as language model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert Embeddings are loaded \n",
      "Perceiver Resampler is being initialized\n",
      "Perceiver attention is created with  dim :  768 dim_head:  64\n",
      "Perceiver attention is created with  dim :  768 dim_head:  64\n",
      "Perceiver Resampler is initialized\n",
      "img encoder outdim initialized\n",
      "Layers are initializzed\n",
      "Flamingo is initalized\n",
      "Self classifier  Sequential(\n",
      "  (0): LayerNorm()\n",
      "  (1): Dropout(p=0.5, inplace=False)\n",
      "  (2): Linear(in_features=1536, out_features=517, bias=True)\n",
      "  (3): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    seed_everything(42, workers=True)\n",
    "\n",
    "    args = load_config('/u/home/koksal/mlmi-vqa/configs','config.yaml')\n",
    "     \n",
    "    #img_mean = (0.48,0.48,0.48)\n",
    "    #img_std = (0.265,0.265,0.265)\n",
    "    #img_mean = (0.48,0.48,0.48)\n",
    "    #img_std = (0.265,0.265,0.265)\n",
    "\n",
    "    augmentations = {'train':\n",
    "        T.Compose(\n",
    "        [   \n",
    "            T.Resize((args['train']['augmentation']['resize_size'])),\n",
    "            T.RandomRotation((args['train']['augmentation']['random_rotation'])),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=args['dataset']['mean'], std=args['dataset']['std'])\n",
    "        ]),\n",
    "        'val':\n",
    "        T.Compose(\n",
    "        [\n",
    "            T.Resize((args['test']['augmentation']['resize_size'])),\n",
    "            T.RandomRotation((args['test']['augmentation']['random_rotation'])),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=args['dataset']['mean'], std=args['dataset']['std'])\n",
    "        ]),\n",
    "        'test':\n",
    "        T.Compose(\n",
    "        [\n",
    "            T.Resize((args['test']['augmentation']['resize_size'])),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=args['dataset']['mean'], std=args['dataset']['std'])\n",
    "        ])\n",
    "    }\n",
    "\n",
    "\n",
    "    args = load_config('/u/home/koksal/mlmi-vqa/configs','config.yaml')\n",
    "\n",
    "\n",
    "    mimic_datamodule = VQRadDataModule(args, augmentations= augmentations)\n",
    "\n",
    "\n",
    "    train_loader = mimic_datamodule.train_dataloader()\n",
    "    val_loader = mimic_datamodule.val_dataloader()\n",
    "\n",
    "\n",
    "\n",
    "    model = FlamingoModule(args) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_val = mimic_datamodule.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mimic_datamodule.train_dataset.tokenizer.all_special_ids[mimic_datamodule.train_dataset.tokenizer.all_special_tokens.index('<image>')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_iterator = iter(dataloader_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_data_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['token_type_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['index_eoq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = batch[\"image\"]\n",
    "input_tokens = batch[\"input_ids\"]\n",
    "token_type_ids = batch[\"token_type_ids\"] \n",
    "targets = batch[\"targets\"]\n",
    "batch_size = images.shape[0]\n",
    "\n",
    "class_labels = batch[\"label\"]\n",
    "index_eoq = batch[\"index_eoq\"]\n",
    "flamingo_logits, token_embeds, image_embeddings = model.flamingo_palm(\n",
    "        input_tokens.squeeze(1), images.unsqueeze(1), token_type_ids.unsqueeze(1),\n",
    "        return_image_embeddings = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['index_eoq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_tokens_until_eoc(token_embeds, eoq_indices, agg_func='mean'):\n",
    "    aggregated_tensors = []\n",
    "    for i, row in enumerate(token_embeds):\n",
    "        if agg_func == 'mean':\n",
    "            aggregated_tensors.append(torch.mean(row[:batch['index_eoq'][i],:],dim=0))\n",
    "    return torch.stack(aggregated_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_tokens_until_eoc(token_embeds, batch['index_eoq']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = torch.tensor([[1,2,3], [4,5,6], [7,8,9], [10,11,12]])\n",
    "source, source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = torch.tensor([[0,0],[1,1],[2,2],[0,1]])\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source.gather(dim=1, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source.gather(dim=1, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.concat(torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eoq_embeds = token_embeds[torch.arange(batch_size), index_eoq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eoq_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eoq_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(args['dataset']['vqa_rad_path'],'VQA-RAD_public.json'), 'r') as f:\n",
    "        sample_dicts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sample_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = []\n",
    "train_set = []\n",
    "for sample in sample_dicts:\n",
    "    if sample['phrase_type'].startswith('test'):\n",
    "        test_set.append(sample)\n",
    "    else:\n",
    "        train_set.append(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('len test set', len(test_set), 'len train set', len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create answers dictionary\n",
    "answer_list_concat= []\n",
    "for sample in sample_dicts:\n",
    "    answer_list_concat.append(str(sample['answer']).strip().lower())\n",
    "\n",
    "# Create a dictionary of answers\n",
    "answer_to_label = {}\n",
    "label_to_answer = {}\n",
    "for i,ans in enumerate(set(answer_list_concat)):\n",
    "    answer_to_label[ans] = i\n",
    "    label_to_answer[i] = ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create answers dictionary\n",
    "answer_list_train = []\n",
    "for sample in train_set:\n",
    "    answer_list_train.append(str(sample['answer']).strip().lower())\n",
    "\n",
    "# Create a dictionary of answers\n",
    "answer_to_label_train = {}\n",
    "label_to_answer_train = {}\n",
    "for i,ans in enumerate(set(answer_list_train)):\n",
    "    answer_to_label_train[ans] = i\n",
    "    label_to_answer_train[i] = ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create answers dictionary\n",
    "answer_list_test = []\n",
    "for sample in test_set:\n",
    "    answer_list_test.append(str(sample['answer']).strip().lower())\n",
    "\n",
    "# Create a dictionary of answers\n",
    "answer_to_label_test = {}\n",
    "label_to_answer_test = {}\n",
    "for i,ans in enumerate(set(answer_list_test)):\n",
    "    answer_to_label_test[ans] = i\n",
    "    label_to_answer_test[i] = ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_counter = Counter(answer_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_dict = {k:v for (k,v) in test_counter.most_common(10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(most_common_dict,orient='index', columns=['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(df,title='VQARad Test Answer Counts',labels={'value':'count','index':'x_legend','variable':'legend'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_to_label_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_set_test = set(label_to_answer_test.values()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_set_train = set(label_to_answer.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(answer_set_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(answer_set_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_test_answers = answer_set_test.difference(answer_set_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_test_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_included_tst_answer_cnt = 0\n",
    "for qa in test_set:\n",
    "    if qa['answer'].strip().lower() in answer_set_train:\n",
    "        train_included_tst_answer_cnt += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_included_tst_answer_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "336/451"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_test_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'a bit' in answer_set_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_set_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Num unique answers vqa-rad ', len(answer_to_label.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/u/home/koksal/mlmi-vqa/data/answer2label_vqarad_concat.json', 'w') as f:\n",
    "    json.dump(answer_to_label, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/u/home/koksal/mlmi-vqa/data/label2answer_vqarad_concat.json', 'w') as f:\n",
    "    json.dump(label_to_answer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Lenght of Training Set ',len(train_set), 'length test set', len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dicts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import torch\n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "from src.datasets.vqa_rad_dataset import VQRadDataModule\n",
    "from src.models.multimodal.flamingo_module import FlamingoModule\n",
    "\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from pytorch_lightning.loggers import CometLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "from src.utils.utils import load_config\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    seed_everything(42, workers=True)\n",
    "\n",
    "    img_mean = (0.48,0.48,0.48)\n",
    "    img_std = (0.265,0.265,0.265)\n",
    "\n",
    "    augmentations = {'train':\n",
    "        T.Compose(\n",
    "        [   \n",
    "            T.Resize((224,224)),\n",
    "            T.RandomRotation(10),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=img_mean, std=img_std)\n",
    "        ]),\n",
    "        'val':\n",
    "        T.Compose(\n",
    "        [\n",
    "            T.Resize((224,224)),\n",
    "            T.RandomRotation(10),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=img_mean, std=img_std)\n",
    "        ]),\n",
    "        'test':\n",
    "        T.Compose(\n",
    "        [\n",
    "            T.Resize((224,224)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=img_mean, std=img_std)\n",
    "        ])\n",
    "    }\n",
    "\n",
    "\n",
    "    args = load_config('/u/home/koksal/mlmi-vqa/configs','config.yaml')\n",
    "\n",
    "    args['train']['batch_size']=1\n",
    "    datamodule = VQRadDataModule(args, augmentations= augmentations)\n",
    "\n",
    "    train_loader = datamodule.train_dataloader()\n",
    "    val_loader = datamodule.val_dataloader()\n",
    "\n",
    "\n",
    "\n",
    "    model = FlamingoModule(args) \n",
    "\n",
    "    if args['pretrained']:\n",
    "        print(\"Pretrained Flamingo Model is loaded from checkpoint : \",args['pretrained'])\n",
    "        model.load_state_dict(torch.load(args['pretrained'])[\"state_dict\"])\n",
    "\n",
    "\n",
    "    from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "    from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['pretrained'] = '/u/home/koksal/mlmi-vqa/notebooks/lightning_logs/version_3/checkpoints/epoch=2-val_acc_epoch=0.00-val_total_loss_epoch=11.29-val_loss_generation_epoch=4.15-val_classification_loss_epoch=7.14.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args['pretrained']:\n",
    "    print(\"Pretrained Flamingo Model is loaded from checkpoint : \",args['pretrained'])\n",
    "    model.load_state_dict(torch.load(args['pretrained'])[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from transformers import GPT2Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn as nn\n",
    "import torch.nn.functional as F\n",
    "def generate(image, context, cur_model, batch, ntok=20):\n",
    "    for _ in range(ntok):\n",
    "        out,_ = cur_model({'image': image,'input_ids': context, \"index_eoq\": batch[\"index_eoq\"],\n",
    "        \"targets\": batch[\"targets\"],\"label\": batch[\"label\"]})\n",
    "        logits = out[:, -1, :]\n",
    "        indices_to_remove = logits < torch.topk(logits, 10)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = np.NINF\n",
    "        #next_tok1 = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1).squeeze(1)\n",
    "        #print(next_tok1.shape)\n",
    "        softmax_out = F.softmax(logits, dim=-1)\n",
    "        #print(softmax_out.shape)\n",
    "        next_tok = torch.argmax(softmax_out,dim=-1,keepdim=False)\n",
    "        #print(next_tok.shape)\n",
    "        context = torch.cat([context, next_tok.unsqueeze(-1)], dim=-1)\n",
    "    return context\n",
    "\n",
    "\n",
    "tokenizer = datamodule.train_dataset.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = datamodule.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader_iter = iter(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.ToPILImage()(batch[\"image\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' <EOQ>'+ ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, batch, ntok=20)\n",
    "print(\"Question : \", batch[\"question\"][0])\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule.train_dataset.tokenizer.all_special_ids[datamodule.train_dataset.tokenizer.all_special_tokens.index('<image>')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "image_encoder = EfficientNet.from_name('efficientnet-b0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_encoder._fc = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = nn.Sequential(*list(image_encoder.children())[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_encoder(torch.randn((1,3,224, 224))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer BERT\n",
    "from transformers import AutoTokenizer\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "gpt_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "gpt_tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('caghan4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:31:59) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7396ef363109521e03a1a0d63e78822cb454baa0fd8fa9ea26684fd7d4e3122"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
