{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import torch\n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "from src.datasets.vqa_rad_dataset import VQRadDataModule\n",
    "from src.models.multimodal.flamingo_module import FlamingoModule\n",
    "\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from pytorch_lightning.loggers import CometLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "from src.utils.utils import load_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load memory False\n",
      "Samples length  train :  3064\n",
      "answers length  train :  3064\n",
      "Samples length  test :  451\n",
      "answers length  test :  451\n",
      "Training split length  3064\n",
      "Test split length  451\n",
      "Test split length after removal  408\n",
      "Num unique answers vqa-rad  458\n",
      "Num unique answers vqa-rad  458\n",
      "Num unique answers vqa-rad  458\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    seed_everything(42, workers=True)\n",
    "\n",
    "    args = load_config('/u/home/koksal/mlmi-vqa/configs','config.yaml')\n",
    "     \n",
    "    #img_mean = (0.48,0.48,0.48)\n",
    "    #img_std = (0.265,0.265,0.265)\n",
    "    #img_mean = (0.48,0.48,0.48)\n",
    "    #img_std = (0.265,0.265,0.265)\n",
    "\n",
    "    augmentations = {'train':\n",
    "        T.Compose(\n",
    "        [   \n",
    "            T.Resize((args['train']['augmentation']['resize_size'])),\n",
    "            T.RandomRotation((args['train']['augmentation']['random_rotation'])),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=args['dataset']['mean'], std=args['dataset']['std'])\n",
    "        ]),\n",
    "        'val':\n",
    "        T.Compose(\n",
    "        [\n",
    "            T.Resize((args['test']['augmentation']['resize_size'])),\n",
    "            T.RandomRotation((args['test']['augmentation']['random_rotation'])),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=args['dataset']['mean'], std=args['dataset']['std'])\n",
    "        ]),\n",
    "        'test':\n",
    "        T.Compose(\n",
    "        [\n",
    "            T.Resize((args['test']['augmentation']['resize_size'])),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=args['dataset']['mean'], std=args['dataset']['std'])\n",
    "        ])\n",
    "    }\n",
    "\n",
    "\n",
    "    args = load_config('/u/home/koksal/mlmi-vqa/configs','config.yaml')\n",
    "\n",
    "\n",
    "    mimic_datamodule = VQRadDataModule(args, augmentations= augmentations)\n",
    "\n",
    "\n",
    "    train_loader = mimic_datamodule.train_dataloader()\n",
    "    val_loader = mimic_datamodule.val_dataloader()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yes',\n",
       " 'no',\n",
       " 'ultrasound',\n",
       " 'extremities',\n",
       " 'left temporal lobe',\n",
       " 'cardiovascular',\n",
       " 'vasculature',\n",
       " 'mri diffusion weighted',\n",
       " 'brain',\n",
       " 'chest xray',\n",
       " 'axial',\n",
       " 'asymmetric',\n",
       " 'pulmonary nodules',\n",
       " 'free air',\n",
       " 'hypodense lesion',\n",
       " 'gi',\n",
       " 'ring enhancing lesion',\n",
       " 'diffuse',\n",
       " 'ct',\n",
       " 'pancreas',\n",
       " 'right upper lobe',\n",
       " 'hypodense',\n",
       " 'left hepatic lobe',\n",
       " 'lungs',\n",
       " 'bilateral',\n",
       " 'cirrhosis',\n",
       " 'atherosclerotic calcification',\n",
       " 'right parietal lobe',\n",
       " 'chest',\n",
       " 'metastasis',\n",
       " 'enlarged',\n",
       " 'volume loss',\n",
       " '1',\n",
       " 'extraluminal air and small fluid collection',\n",
       " 'nodular opacities',\n",
       " 'extra axial and at right choroidal fissure',\n",
       " 'prior surgery',\n",
       " 'ring enhancing lesions',\n",
       " 'right temporal lobe',\n",
       " 'gallstones',\n",
       " 'right',\n",
       " 'abscess',\n",
       " 'cystic lesions',\n",
       " 'middle mogul',\n",
       " 'right lung',\n",
       " 'enlarged fluid filled',\n",
       " 'pancreatic body',\n",
       " 'right lobe of liver',\n",
       " 'right hemisphere',\n",
       " 'right pca',\n",
       " 'basal ganglia',\n",
       " 'right superior cavoatrial junction',\n",
       " 'ischemia',\n",
       " 'suprasellar cistern',\n",
       " 'above',\n",
       " 'pa',\n",
       " '4th and 5th',\n",
       " 't5',\n",
       " 'less than half thorax',\n",
       " '0',\n",
       " '5 cm',\n",
       " 'superior',\n",
       " 'high on image',\n",
       " 'right side',\n",
       " 'left side',\n",
       " 'periappendiceal fluid and fat stranding',\n",
       " 'kidneys',\n",
       " 'small bowel',\n",
       " 'heart lungs',\n",
       " 'it is less than half width of thorax',\n",
       " 'left kidney',\n",
       " 'both',\n",
       " 'more dense',\n",
       " 'less dense',\n",
       " 'surrounding tissue',\n",
       " 'above clavicles bilaterally',\n",
       " 'gastrointestinal',\n",
       " 'right lower lobe',\n",
       " 'in thorasic aorta',\n",
       " 'width of aorta',\n",
       " 'posterior to appendix',\n",
       " '7th rib',\n",
       " 'left',\n",
       " 'left lung',\n",
       " 'edematous',\n",
       " 'right lung base',\n",
       " 'anterior mediastinum',\n",
       " 'right lung hilum',\n",
       " 'left hemisphere',\n",
       " 'right lobe',\n",
       " 'on right shoulder',\n",
       " 'pacemaker',\n",
       " 'bullous lesion',\n",
       " 'xray',\n",
       " 'not seen here',\n",
       " 'in right hilum',\n",
       " 'upper right lobe',\n",
       " 'contrast in intestines',\n",
       " 'calcifications',\n",
       " 'left upper lobe',\n",
       " 'lung',\n",
       " 'imaging artifacts',\n",
       " 'contrast',\n",
       " 'posteroanterior',\n",
       " 'right frontal lobe',\n",
       " 'it is shifted to right',\n",
       " 'occipital lobe',\n",
       " 'bit',\n",
       " 'posterior horn of left lateral ventricle',\n",
       " 'liver',\n",
       " 'infarcted areas',\n",
       " 'just 1',\n",
       " 'fat accumulations',\n",
       " 'scoliosis',\n",
       " 'lateral ventricles',\n",
       " 'solid',\n",
       " 'female',\n",
       " '10 20 minutes',\n",
       " 'white matter',\n",
       " 'large bowel',\n",
       " 'lungs bony thoracic cavit y mediastinum and great vessels',\n",
       " 'retroperitoneum retroperitoneal space',\n",
       " '~15 minutes potentially faster with newer imaging systems',\n",
       " 'fat',\n",
       " 'medical process',\n",
       " 'moderate edema',\n",
       " 'anterior to transverse colon',\n",
       " 'ring enhancing lesion in left occipital lobe',\n",
       " 'parietal and occipital lobes',\n",
       " 'pres',\n",
       " 'cardiomegaly with pulmonary edema',\n",
       " 'cns',\n",
       " 'cecum',\n",
       " 'mass',\n",
       " 'cxr',\n",
       " 'coronal',\n",
       " 'left apical pneumothorax',\n",
       " 'vascular',\n",
       " 'cardiomegaly',\n",
       " 'sternotomy wires and surgical clips',\n",
       " 'head of pancreas',\n",
       " 'flair mri',\n",
       " 'left lateral aspect of anterior peritoneum',\n",
       " 'left mid lung',\n",
       " '4th ventricle',\n",
       " 'fat stranding around appendix thickened appendiceal walls dilated appendix and appendicolith is seen as well',\n",
       " 'ring enhancing lesion in right frontal lobe',\n",
       " 'right lenticular nucleus',\n",
       " 'air',\n",
       " '3rd rib',\n",
       " 'xray plain film',\n",
       " 'respiratory system',\n",
       " 'abdomen',\n",
       " 'omental caking',\n",
       " 'diffusion weighted imaging dwi',\n",
       " 'contrast ct with gi and iv contrast',\n",
       " 'left costophrenic angle is blunted',\n",
       " 'non contrast ct',\n",
       " 'left occipital lobe',\n",
       " 'ct with gi and iv contrast',\n",
       " 'aorta',\n",
       " 'aorta and inferior vena cava',\n",
       " 'subarachnoid',\n",
       " 'medial rectus',\n",
       " 'imagine patient is laying down and you are looking from feet',\n",
       " 'bowel contents light up on image',\n",
       " 'ivc',\n",
       " 'loculated',\n",
       " 'medial and lateral rectus',\n",
       " 'pineal gland',\n",
       " 'cardiac region',\n",
       " 'infarct',\n",
       " 'congenital developmental disorder history of surgery and past manipulation',\n",
       " 'mr flair',\n",
       " 'right cerebellum',\n",
       " 'r frontal lobe',\n",
       " 'respiratory cardia c musculoskeletal',\n",
       " 'csf is white',\n",
       " 't2',\n",
       " 'bilateral cerebellum',\n",
       " 'left parietal lobe',\n",
       " 'sternal wires',\n",
       " 'right sided aortic arch',\n",
       " 'cerebellum',\n",
       " 'with contrast',\n",
       " 'right lower lateral lung field',\n",
       " '3.4 cm',\n",
       " 'sella and suprasellar cistern',\n",
       " 't2 weighted',\n",
       " 'left thalamus and basal ganglia',\n",
       " 'oculomotor nerve cn iii and trigeminal nerves cn v',\n",
       " 'pa xray',\n",
       " 'basilar artery',\n",
       " 'heterogeneous',\n",
       " 'right lateral ventricle',\n",
       " 'dwi',\n",
       " 'right cerebellopontine angle',\n",
       " 'upper lobes',\n",
       " 'mri t1 weighted',\n",
       " 'hydrocephalus',\n",
       " 'right posteroinferior cerebellum',\n",
       " 'bleeding in right posteroinferior cerebellum',\n",
       " 'left cerebellum',\n",
       " 'gallbladder',\n",
       " 'ribs',\n",
       " 'choroid plexus',\n",
       " 'cartilage is not well viewed by xrays',\n",
       " 'lung markings present all way laterally to ribs',\n",
       " 'right mainstem bronchus is more in line with trachea than left',\n",
       " 'air fluid level',\n",
       " 'intestine',\n",
       " 'stomach bubble',\n",
       " 'right pica',\n",
       " 'sulcal effacement',\n",
       " 'temporal lobe',\n",
       " 'l2',\n",
       " 'l2 3',\n",
       " 'lateral film as well as pa',\n",
       " 'maxillary sinuses',\n",
       " 'sacroiliac joint',\n",
       " 'varicocele',\n",
       " 'calcification',\n",
       " 'elliptical',\n",
       " 'smooth',\n",
       " 'left temporal horn',\n",
       " 'ring enhancing',\n",
       " 'right side of trachea',\n",
       " 'posterior brain',\n",
       " 'hypoxic ischemic injury',\n",
       " 'loss of normal gray white matter junction',\n",
       " 'abdomen and pelvis',\n",
       " 'lateral and third ventricular hydrocephalus',\n",
       " 'retrocardiac',\n",
       " 'below 7th rib in right lung',\n",
       " 'single lung nodule',\n",
       " 'mri',\n",
       " 'blind loop syndrome',\n",
       " 'blind ending loop of bowel arising from cecum',\n",
       " 'pineal region',\n",
       " 'bilateral frontal lobes',\n",
       " 'mri dwi',\n",
       " 'right colon',\n",
       " 'isointense',\n",
       " 'hyperintense',\n",
       " 'iv contrast',\n",
       " 'diverticulitis',\n",
       " 'micronodular',\n",
       " 'left rectus abdominus',\n",
       " 'nipple location',\n",
       " 'bilateral frontal lobes and body of corpus callosum',\n",
       " 'distal basilar artery',\n",
       " 'suprasellar',\n",
       " 'mri flair',\n",
       " 'left aca and mca',\n",
       " 'caudate putamen left parietal',\n",
       " 'anterior cerebrum',\n",
       " 'anterior surface',\n",
       " 'ascites',\n",
       " 'right kidney',\n",
       " 'hemorrhage',\n",
       " 'kidney cyst',\n",
       " 'pericholecystic fluid',\n",
       " 'mr t2 weighted',\n",
       " 'fatty infiltration',\n",
       " 'jejunum',\n",
       " 'mr adc map',\n",
       " 'nodules',\n",
       " 'descending colon',\n",
       " 'thalami left occipital lobe brainstem and left cerebellum',\n",
       " 'adjacent to vertebrae',\n",
       " 'spleen',\n",
       " 'pleural effusion',\n",
       " 'hip bones',\n",
       " 'multilobulated',\n",
       " 'temporal and lateral occipital lobes',\n",
       " 'mixed intensity',\n",
       " 'on left',\n",
       " '4',\n",
       " 'acute stroke',\n",
       " 'decreased muscle bulk',\n",
       " '6.5 x 6.2 x 8.8cm',\n",
       " 'right vertebral artery sign',\n",
       " '5cm',\n",
       " 'ascending colon',\n",
       " 'r hemidiaphragm',\n",
       " 'almost entire right side',\n",
       " 'fluid',\n",
       " 'in vasculature',\n",
       " 'pons',\n",
       " 'necrotic tissue',\n",
       " 'mediport',\n",
       " '2',\n",
       " 'midline',\n",
       " 'breasts',\n",
       " 'double arch',\n",
       " 'emphysema',\n",
       " 'reduced sulci',\n",
       " 'black',\n",
       " 'man',\n",
       " 'cystic',\n",
       " '5mm',\n",
       " 'bilateral parietal lobes',\n",
       " 'chronic sinusitis vs hemorrhage',\n",
       " 'frontal and occipital',\n",
       " 'basal ganglia cerebellum cerebral cortex',\n",
       " 'white matter plaques',\n",
       " 'in bowels',\n",
       " 'plain film xray',\n",
       " 'short section irregular contour',\n",
       " 'bilateral lungs',\n",
       " 'hydropneumothorax',\n",
       " 'stomach',\n",
       " 'basilar artery thrombosis',\n",
       " 'in bowel',\n",
       " 'gadolinium',\n",
       " 'right of midline superior to right hilum',\n",
       " 'in midline',\n",
       " '2.5cm x 1.7cm x 1.6cm',\n",
       " 'radiolucent',\n",
       " 'hypointense',\n",
       " 'non enhanced',\n",
       " 'ap',\n",
       " 'mid abdomen',\n",
       " 'chest radiograph',\n",
       " 'flair',\n",
       " 'sigmoid flexture of colon',\n",
       " 'underneath right hemidiaphragm',\n",
       " 'posterior fossa',\n",
       " 'right sylvian fissure',\n",
       " 'right subclavian vein',\n",
       " 'normal',\n",
       " 'oral and iv',\n",
       " 'base',\n",
       " 'lower lung fields',\n",
       " 'motion',\n",
       " 'diverticuli',\n",
       " 'psoas major muscle',\n",
       " 'mri t2 weighted',\n",
       " 'posterior to gastric antrum',\n",
       " 'calcified atherosclerosis',\n",
       " 'non contrast',\n",
       " 'right subdural hematoma',\n",
       " 'appendix',\n",
       " 'soft tissue mass in region of terminal ileum with mesenteric lymphadenopathy',\n",
       " 'hepatocellular carcioma',\n",
       " 'punctate',\n",
       " 'right sided pleural effusion',\n",
       " 'cavum vergae',\n",
       " 'central hyperintensity and surrounding hypointensity',\n",
       " '5.6cm focal predominantly hypodense',\n",
       " 'portal vein occlusion',\n",
       " 'portal vein',\n",
       " 'there is massive cerebral hemisphere edema',\n",
       " 'nothing',\n",
       " 'left thalamus',\n",
       " 'small subdural hematoma with cerebral edema',\n",
       " 'proximal aspect of appendix',\n",
       " 'adjacent to appendix',\n",
       " 'left mca',\n",
       " 'ms plaques',\n",
       " 'peritoneum',\n",
       " 'all 3 vascular distributions',\n",
       " 'multiple sclerosis',\n",
       " 'cerebrum and lateral ventricles',\n",
       " 'right convexity',\n",
       " 'abnormal hyperintensity in right occipital lobe',\n",
       " 'pneumonia',\n",
       " 'hyperintensity of left basal ganglia',\n",
       " 'partial silhouetting',\n",
       " 'splenule',\n",
       " 'necrosis',\n",
       " 'posteriorly',\n",
       " 'head neck ct',\n",
       " 'biopsy',\n",
       " 'metastases infection abcess glioblastoma',\n",
       " 'toxoplasma lymphoma abscesses other brain tumors',\n",
       " 'cancer',\n",
       " 'ureteral obstruction',\n",
       " 'lentiform',\n",
       " 'biconvex',\n",
       " 'psoas muscles',\n",
       " 'epidural hematoma',\n",
       " 'sinusitis',\n",
       " 'viral',\n",
       " 'parasitic',\n",
       " 'if heart diameter is greater than half diameter of thoracic cavity',\n",
       " 'c t ratio',\n",
       " 'abcess',\n",
       " 'bronchiectasis',\n",
       " 'thickening of bronchi',\n",
       " 'csf is brightly lit',\n",
       " 'aorta enhancement',\n",
       " 'costophrenic angle blunting',\n",
       " 'fluid in pleural space',\n",
       " 'aorta is bright',\n",
       " 'enhancement of vessels',\n",
       " 'genetic',\n",
       " 'crescent',\n",
       " 'concave',\n",
       " 'right mca',\n",
       " 'catheter',\n",
       " 'quadrantopia aphasia memory deficit etc',\n",
       " 'motor weakness sensory deficits and left neglect',\n",
       " 'viral inflammatory',\n",
       " 'cva',\n",
       " 'in cortex and basal ganglia bilaterally',\n",
       " 'left lobe mass 1.5 x 1.8 cm',\n",
       " 'not sure',\n",
       " 'pituitary fossa',\n",
       " 'nephroblastomatosis',\n",
       " 'location of contrast',\n",
       " 'haustra',\n",
       " 'maybe',\n",
       " 'ruq pain jaundiceweight loss',\n",
       " 'abdominal pain',\n",
       " 'pleural plaques',\n",
       " 'trace gallbladder emptying',\n",
       " 'cystic duct is more tortuous',\n",
       " 'tumors gallstones',\n",
       " 'stones cancer infection anatomic variants',\n",
       " 'gray matter',\n",
       " 'plicae circulares',\n",
       " 'more acute means more inflammation leading to enhancement',\n",
       " 'less enhancement',\n",
       " 'white versus grey matter brightness',\n",
       " 'adenopathy',\n",
       " 'infection',\n",
       " 'paratracheal area',\n",
       " '5%',\n",
       " 'rounded well defined pulmonary nodules varying in size and pattern',\n",
       " 'bilateral pleural effusion',\n",
       " 'basal ganglia caudate and putamen',\n",
       " 'cardiopulmonary',\n",
       " 'semi upright position',\n",
       " 'sharp costophrenic angles',\n",
       " 'right parietal',\n",
       " 'horsehoe kidney',\n",
       " 'posterior lung seen in image section',\n",
       " 'pneumothorax',\n",
       " 'both sides',\n",
       " 'exterior',\n",
       " 'outside',\n",
       " 'blunting of costophrenic angle loss of right hemidiaphragm and right heart border',\n",
       " 'increased opacity in left retrocardiac region',\n",
       " 'right paratracheal mass lesion',\n",
       " 'it is enlarged with prominence of aortic knob',\n",
       " 'well circumscribed',\n",
       " 'exophytic cyst',\n",
       " 'shrunken and nodular',\n",
       " 'infarcts',\n",
       " 'embolus',\n",
       " 'cortical ribbon of right occipital lobe with extension into right posterior temporal lobe',\n",
       " 'irregular',\n",
       " 'infiltrative',\n",
       " '12',\n",
       " 'on top of patient',\n",
       " \"superficial to patient's skin\",\n",
       " 'nucleus pulposus']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mimic_datamodule.label2ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='emilyalsentzer/Bio_ClinicalBERT', vocab_size=28996, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]', 'additional_special_tokens': ['<image>', '<EOC>', '<EOQ>']})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mimic_datamodule.train_dataset.tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_val_dict = {}\n",
    "for question in mimic_datamodule.val_split:\n",
    "    unique_val_dict[question['qid']] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_train_dict = {}\n",
    "for question in mimic_datamodule.train_split:\n",
    "    unique_train_dict[question['qid']] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(unique_train_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(unique_val_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.intersection(set(unique_train_dict.keys()), set(unique_val_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for each in mimic_datamodule.val_split:\n",
    "    if each['answer']['labels'] == None:\n",
    "        cnt +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_datamodule.val_split[122]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = mimic_datamodule.train_dataset[1111]\n",
    "sample2= mimic_datamodule.train_dataset[1465]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in sample1.keys():\n",
    "    print('K : ',k , 'values are : ', sample1[k],'           '  ,sample2[k] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_datamodule.train_dataset[1465].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clip architecture is being loaded\n",
      "Clip pretrained weights are being loaded\n",
      "Flamingo is being initialized with  bert-clinical  as language model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type bert to instantiate a model of type bert-generation. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertGenerationDecoder: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.pooler.dense.bias', 'bert.embeddings.token_type_embeddings.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertGenerationDecoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertGenerationDecoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertGenerationDecoder were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['lm_head.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceiver Resampler is being initialized\n",
      "Perceiver attention is created with  dim :  768 dim_head:  64\n",
      "Perceiver attention is created with  dim :  768 dim_head:  64\n",
      "Perceiver Resampler is initialized\n",
      "img encoder outdim initialized\n",
      "Layers are initializzed\n",
      "Flamingo is initalized\n",
      "Self classifier  Sequential(\n",
      "  (0): LayerNorm()\n",
      "  (1): Dropout(p=0.1, inplace=False)\n",
      "  (2): Linear(in_features=1536, out_features=458, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = FlamingoModule(args) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'is there a pneumothorax',\n",
       " 'label': 1,\n",
       " 'index_eoq': 12,\n",
       " 'input_ids': tensor([[  101, 28996,  2304,   131,  1110,  1175,   170,   185,  1673,  1818,\n",
       "          12858, 25632, 28998,   102,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'targets': tensor([[28996,  2304,   131,  1110,  1175,   170,   185,  1673,  1818, 12858,\n",
       "          25632, 28998,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0]]),\n",
       " 'ID': 'synpic42202.jpg',\n",
       " 'qa_pair': '<image> question: is there a pneumothorax <EOQ>',\n",
       " 'image': tensor([[[-2.1179, -2.1179, -2.1008,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [-2.1179, -2.1179, -2.0665,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [-2.1179, -2.1179, -2.1008,  ..., -2.0494, -2.0837, -2.1179],\n",
       "          ...,\n",
       "          [-2.1008, -2.1008, -2.1008,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
       " \n",
       "         [[-2.0357, -2.0357, -2.0182,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [-2.0357, -2.0357, -1.9832,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [-2.0357, -2.0357, -2.0182,  ..., -1.9657, -2.0007, -2.0357],\n",
       "          ...,\n",
       "          [-2.0182, -2.0182, -2.0182,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
       " \n",
       "         [[-1.8044, -1.8044, -1.7870,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [-1.8044, -1.8044, -1.7522,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [-1.8044, -1.8044, -1.7870,  ..., -1.7347, -1.7696, -1.8044],\n",
       "          ...,\n",
       "          [-1.7870, -1.7870, -1.7870,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mimic_datamodule.train_dataset[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_val = mimic_datamodule.val_dataloader()\n",
    "dataloader_train = mimic_datamodule.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "dict_keys(['question', 'label', 'index_eoq', 'input_ids', 'token_type_ids', 'targets', 'ID', 'qa_pair', 'image'])\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader_val:\n",
    "    print(batch.keys())\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['question', 'label', 'index_eoq', 'input_ids', 'token_type_ids', 'targets', 'ID', 'qa_pair', 'image'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].isnan().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['question', 'label', 'index_eoq', 'input_ids', 'token_type_ids', 'targets', 'ID', 'qa_pair', 'image'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<image> question: is there evidence of an aortic aneurysm <EOQ>',\n",
       " '<image> question: is there airspace consolidation on the left side <EOQ>',\n",
       " '<image> question: is this an axial plane <EOQ>',\n",
       " '<image> question: is there vascular pathology in this image <EOQ>',\n",
       " '<image> question: is there cardiomyopathy <EOQ>',\n",
       " '<image> question: are >12 ribs present in the image <EOQ>',\n",
       " '<image> question: does this represent adequate inspiratory effort <EOQ>',\n",
       " '<image> question: is there a rib fracture <EOQ>',\n",
       " '<image> question: is there evidence of any fractures of the ribs <EOQ>',\n",
       " '<image> question: is there any intraparenchymal abnormalities in the lung fields <EOQ>',\n",
       " '<image> question: is there evidence of any abnormalities of the lung parenchyma <EOQ>',\n",
       " '<image> question: is there evidence of small bowel obstruction on this image <EOQ>',\n",
       " '<image> question: is there small bowel thickening present <EOQ>',\n",
       " '<image> question: was the patient positioned appropriately \\twithout tilting <EOQ>',\n",
       " '<image> question: is the small bowel appropriately identified with gi contrast <EOQ>',\n",
       " '<image> question: is there a small bowel obstruction <EOQ>',\n",
       " '<image> question: are there hyper attenuating material along the aortic walls <EOQ>',\n",
       " '<image> question: are there calcifications present on the abdominal aorta <EOQ>',\n",
       " '<image> question: does this patient have evidence of pneumoperitoneum <EOQ>',\n",
       " '<image> question: is there evidence of air in the peritoneal cavity <EOQ>',\n",
       " '<image> question: is the cardiac silhouette less than half the diameter of the diaphragm <EOQ>',\n",
       " '<image> question: is the cardiac silhouette within normal size limits <EOQ>',\n",
       " '<image> question: is there narrowing of the cardiac contour <EOQ>',\n",
       " '<image> question: is the cardiac silhouette small/stretched <EOQ>',\n",
       " '<image> question: is there evidence of pulmonary edema <EOQ>',\n",
       " '<image> question: is there enlargement of the pulmonary vasculature <EOQ>',\n",
       " '<image> question: is the cardiac contour normal <EOQ>',\n",
       " '<image> question: is the cardiac silhouette less than half the lateral diameter of the chest wall <EOQ>',\n",
       " '<image> question: is there a left apical pneumothorax <EOQ>',\n",
       " '<image> question: is the outline of visceral pleura visible in the upper left lung field <EOQ>',\n",
       " '<image> question: is there slight dilation of the right renal pelvis <EOQ>',\n",
       " '<image> question: is there enlargement of the right renal pelvis <EOQ>',\n",
       " '<image> question: has the midline of the mediastinum shifted <EOQ>',\n",
       " '<image> question: is there a mediastinal shift <EOQ>',\n",
       " '<image> question: is the left lung field more radioopaque than the right side <EOQ>',\n",
       " '<image> question: is the left lung field more white <EOQ>',\n",
       " '<image> question: is the liver normal <EOQ>',\n",
       " '<image> question: is this a typical liver <EOQ>',\n",
       " '<image> question: describe the lung abnormalities <EOQ>',\n",
       " '<image> question: what abnormalities are seen within the lungs <EOQ>',\n",
       " '<image> question: what is under the right hemidiaphragm <EOQ>',\n",
       " '<image> question: what is the abnormality by the right hemidiaphragm <EOQ>',\n",
       " '<image> question: is this in the posterior anterior plane <EOQ>',\n",
       " '<image> question: how would you describe the spleen abnormality <EOQ>',\n",
       " '<image> question: what is abnormal about the spleen <EOQ>',\n",
       " '<image> question: is the image of the gastrointestinal system <EOQ>',\n",
       " '<image> question: which plane is this image taken <EOQ>',\n",
       " '<image> question: are there multiple or just 1 metastatic focus <EOQ>',\n",
       " '<image> question: how many lesions are in the spleen <EOQ>',\n",
       " '<image> question: what is abnormal about the pancreas <EOQ>',\n",
       " '<image> question: why is the pancreas abnormal <EOQ>',\n",
       " '<image> question: what abnormalities are in the lung apices <EOQ>',\n",
       " '<image> question: what is seen in the lung apices <EOQ>',\n",
       " '<image> question: are the lungs normal <EOQ>',\n",
       " '<image> question: is the image study normal <EOQ>',\n",
       " '<image> question: how would you describe the abnormalities <EOQ>',\n",
       " '<image> question: are the lesions ring-enhancing <EOQ>',\n",
       " '<image> question: what lobe is the lesion located in <EOQ>',\n",
       " '<image> question: where is the abnormality located <EOQ>',\n",
       " '<image> question: how would you describe the aortopulmonary window <EOQ>',\n",
       " '<image> question: what characteristic finding do you see in the aortopulmonary window <EOQ>',\n",
       " '<image> question: which sign do you see in the aortopulmonary window in this image <EOQ>',\n",
       " '<image> question: where is the diffuse pleural thickening <EOQ>',\n",
       " '<image> question: which lung is abnormal <EOQ>',\n",
       " '<image> question: is the mass calcified <EOQ>',\n",
       " '<image> question: does this look like a healthy liver <EOQ>',\n",
       " '<image> question: is the liver normal <EOQ>',\n",
       " '<image> question: is this a normal x-ray <EOQ>',\n",
       " '<image> question: is the right hemidiaphragm normal <EOQ>',\n",
       " '<image> question: what kind of image is this <EOQ>',\n",
       " '<image> question: is this in a coronal plane <EOQ>',\n",
       " '<image> question: is the surrounding phlegmon normal <EOQ>',\n",
       " '<image> question: is the fat surrounding the pancreas normal <EOQ>',\n",
       " '<image> question: are lung markings present in the entire thoracic cavity <EOQ>',\n",
       " '<image> question: is there a pneumothorax present <EOQ>',\n",
       " '<image> question: is there more than one organ system present in the image <EOQ>',\n",
       " '<image> question: how many instances of intussusception are in the image <EOQ>',\n",
       " '<image> question: is there more than one intussusception present <EOQ>',\n",
       " '<image> question: which side is more clearly visualized <EOQ>',\n",
       " '<image> question: which lung is clearer <EOQ>',\n",
       " '<image> question: is the diaphragm clearly visualized on both sides of the thorax <EOQ>',\n",
       " '<image> question: is the diaphragm obscured <EOQ>',\n",
       " '<image> question: is the heart wider than more than half the width of the thorax <EOQ>',\n",
       " '<image> question: how wide is the heart compared to the thorax <EOQ>',\n",
       " '<image> question: is there evidence of subdiaphragmatic free air on the left <EOQ>',\n",
       " '<image> question: is the free air under the left hemidiaphragm pathologic <EOQ>',\n",
       " '<image> question: is the gallbladder present <EOQ>',\n",
       " '<image> question: where is free air trapped subcutaneously in this image <EOQ>',\n",
       " '<image> question: is there free air in the supraclavicular fossae <EOQ>',\n",
       " '<image> question: where is the ascending colon <EOQ>',\n",
       " '<image> question: are the borders of the heart clearly visualized <EOQ>',\n",
       " '<image> question: where is the subcutaneous air collection <EOQ>',\n",
       " '<image> question: is there subcutaneous air present in the right neck <EOQ>',\n",
       " '<image> question: is this image taken above the diaphragm <EOQ>',\n",
       " '<image> question: what is in the left apex <EOQ>',\n",
       " '<image> question: is a pneumothorax present in the left apex <EOQ>',\n",
       " '<image> question: where is the lesion located <EOQ>',\n",
       " '<image> question: which side of the heart border is obscured <EOQ>',\n",
       " '<image> question: where are the kidney <EOQ>',\n",
       " '<image> question: are the kidneys present in this image <EOQ>',\n",
       " \"<image> question: is the colon more prominent on the patient 's right or left side <EOQ>\",\n",
       " '<image> question: where is the colon most prominent from this view <EOQ>',\n",
       " '<image> question: is the liver visible in the image <EOQ>',\n",
       " '<image> question: is this an axial image <EOQ>',\n",
       " '<image> question: are the temporal bones fractured <EOQ>',\n",
       " '<image> question: is there consolidation in the lungs <EOQ>',\n",
       " '<image> question: what type of image is this <EOQ>',\n",
       " '<image> question: are the brain gyri atrophied <EOQ>',\n",
       " '<image> question: is the anatomy of the brain gyri affected <EOQ>',\n",
       " '<image> question: is this an ap image <EOQ>',\n",
       " '<image> question: what is the plane of this image <EOQ>',\n",
       " '<image> question: is this a ct image <EOQ>',\n",
       " '<image> question: is there leakage of blood <EOQ>',\n",
       " '<image> question: is there herniation <EOQ>',\n",
       " '<image> question: what bright white structures form an x <EOQ>',\n",
       " '<image> question: what are the bright white structures almost forming an x\"\" <EOQ>',\n",
       " '<image> question: can you see the esophagus in this image <EOQ>',\n",
       " '<image> question: is the esophagus visible <EOQ>',\n",
       " '<image> question: is this modality safe for pregnant women <EOQ>',\n",
       " '<image> question: can fluids be highlighted with this modality <EOQ>',\n",
       " '<image> question: are the kidneys normal <EOQ>',\n",
       " '<image> question: do the kidneys appear to be normal <EOQ>',\n",
       " '<image> question: are the lateral ventricles visible <EOQ>',\n",
       " '<image> question: can the lateral ventricles be seen <EOQ>',\n",
       " '<image> question: what hypoattenuated tissue is between the abdominal wall and skin <EOQ>',\n",
       " '<image> question: what is the dark structure underneath the skin <EOQ>',\n",
       " '<image> question: does this mass likely represent a neoplastic process <EOQ>',\n",
       " '<image> question: is this a neoplastic process <EOQ>']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['qa_pair']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28996"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mimic_datamodule.train_dataset.tokenizer.all_special_ids[mimic_datamodule.train_dataset.tokenizer.all_special_tokens.index('<image>')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28996"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eoc_token_id = mimic_datamodule.train_dataset.tokenizer.all_special_ids[mimic_datamodule.train_dataset.tokenizer.all_special_tokens.index('<image>')]\n",
    "eoc_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='emilyalsentzer/Bio_ClinicalBERT', vocab_size=28996, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]', 'additional_special_tokens': ['<image>', '<EOC>', '<EOQ>']})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mimic_datamodule.train_dataset.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mimic_datamodule.train_dataset.tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "val_data_iterator = iter(dataloader_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_data_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['question', 'label', 'index_eoq', 'input_ids', 'token_type_ids', 'targets', 'ID', 'qa_pair', 'image'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  101, 28996,  2304,  ...,     0,     0,     0]],\n",
       "\n",
       "        [[  101, 28996,  2304,  ...,     0,     0,     0]],\n",
       "\n",
       "        [[  101, 28996,  2304,  ...,     0,     0,     0]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[  101, 28996,  2304,  ...,     0,     0,     0]],\n",
       "\n",
       "        [[  101, 28996,  2304,  ...,     0,     0,     0]],\n",
       "\n",
       "        [[  101, 28996,  2304,  ...,     0,     0,     0]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "batch[\"token_type_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = batch[\"image\"]\n",
    "input_tokens = batch[\"input_ids\"]\n",
    "token_type_ids = batch[\"token_type_ids\"] \n",
    "targets = batch[\"targets\"]\n",
    "batch_size = images.shape[0]\n",
    "\n",
    "class_labels = batch[\"label\"]\n",
    "index_eoq = batch[\"index_eoq\"]\n",
    "flamingo_logits, token_embeds, image_embeddings = model.flamingo_palm(\n",
    "        input_tokens.squeeze(1), images.unsqueeze(1), token_type_ids.unsqueeze(1),\n",
    "        return_image_embeddings = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = batch[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    images = batch[\"image\"]\n",
    "    input_tokens = batch[\"input_ids\"]\n",
    "    if batch[\"input_ids\"].isnan().any() ==True:\n",
    "        print('found nan ')\n",
    "        break\n",
    "    token_type_ids = batch[\"token_type_ids\"] \n",
    "    targets = batch[\"targets\"]\n",
    "    batch_size = images.shape[0]\n",
    "\n",
    "    class_labels = batch[\"label\"]\n",
    "    index_eoq = batch[\"index_eoq\"]\n",
    "    flamingo_logits, token_embeds, image_embeddings = model.flamingo_palm(\n",
    "            input_tokens.squeeze(1), images.unsqueeze(1), token_type_ids.unsqueeze(1),\n",
    "            return_image_embeddings = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['index_eoq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_tokens_until_eoc(token_embeds, eoq_indices, agg_func='mean'):\n",
    "    aggregated_tensors = []\n",
    "    for i, row in enumerate(token_embeds):\n",
    "        if agg_func == 'mean':\n",
    "            aggregated_tensors.append(torch.mean(row[:batch['index_eoq'][i],:],dim=0))\n",
    "    return torch.stack(aggregated_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_tokens_until_eoc(token_embeds, batch['index_eoq']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = torch.tensor([[1,2,3], [4,5,6], [7,8,9], [10,11,12]])\n",
    "source, source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = torch.tensor([[0,0],[1,1],[2,2],[0,1]])\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source.gather(dim=1, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source.gather(dim=1, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.concat(torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eoq_embeds = token_embeds[torch.arange(batch_size), index_eoq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eoq_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eoq_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(args['dataset']['vqa_rad_path'],'VQA-RAD_public.json'), 'r') as f:\n",
    "        sample_dicts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sample_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = []\n",
    "train_set = []\n",
    "for sample in sample_dicts:\n",
    "    if sample['phrase_type'].startswith('test'):\n",
    "        test_set.append(sample)\n",
    "    else:\n",
    "        train_set.append(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('len test set', len(test_set), 'len train set', len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create answers dictionary\n",
    "answer_list_concat= []\n",
    "for sample in sample_dicts:\n",
    "    answer_list_concat.append(str(sample['answer']).strip().lower())\n",
    "\n",
    "# Create a dictionary of answers\n",
    "answer_to_label = {}\n",
    "label_to_answer = {}\n",
    "for i,ans in enumerate(set(answer_list_concat)):\n",
    "    answer_to_label[ans] = i\n",
    "    label_to_answer[i] = ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create answers dictionary\n",
    "answer_list_train = []\n",
    "for sample in train_set:\n",
    "    answer_list_train.append(str(sample['answer']).strip().lower())\n",
    "\n",
    "# Create a dictionary of answers\n",
    "answer_to_label_train = {}\n",
    "label_to_answer_train = {}\n",
    "for i,ans in enumerate(set(answer_list_train)):\n",
    "    answer_to_label_train[ans] = i\n",
    "    label_to_answer_train[i] = ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create answers dictionary\n",
    "answer_list_test = []\n",
    "for sample in test_set:\n",
    "    answer_list_test.append(str(sample['answer']).strip().lower())\n",
    "\n",
    "# Create a dictionary of answers\n",
    "answer_to_label_test = {}\n",
    "label_to_answer_test = {}\n",
    "for i,ans in enumerate(set(answer_list_test)):\n",
    "    answer_to_label_test[ans] = i\n",
    "    label_to_answer_test[i] = ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_counter = Counter(answer_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_dict = {k:v for (k,v) in test_counter.most_common(10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(most_common_dict,orient='index', columns=['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(answer_list_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(answer_list_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "not_included_in_train = []\n",
    "for test_answer in answer_list_test:\n",
    "    if test_answer in set(answer_list_train):\n",
    "        count +=  1\n",
    "    else:\n",
    "        not_included_in_train.append(test_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_train_answers = json.load(open('/u/home/koksal/mlmi-vqa/data/answer2label_vqarad.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "not_included_in_train = []\n",
    "for sample in test_set:\n",
    "    if sample['answer'].lower().strip() in loaded_train_answers.keys():\n",
    "        cnt += 1\n",
    "    else:\n",
    "        not_included_in_train.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(answer_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_included_in_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_included_in_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(df,title='VQARad Test Answer Counts',labels={'value':'count','index':'x_legend','variable':'legend'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_to_label_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_set_test = set(label_to_answer_test.values()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_set_train = set(label_to_answer.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(answer_set_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(answer_set_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_test_answers = answer_set_test.difference(answer_set_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_test_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_included_tst_answer_cnt = 0\n",
    "for qa in test_set:\n",
    "    if qa['answer'].strip().lower() in answer_set_train:\n",
    "        train_included_tst_answer_cnt += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_included_tst_answer_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "336/451"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_test_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'a bit' in answer_set_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_set_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Num unique answers vqa-rad ', len(answer_to_label.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/u/home/koksal/mlmi-vqa/data/answer2label_vqarad_concat.json', 'w') as f:\n",
    "    json.dump(answer_to_label, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/u/home/koksal/mlmi-vqa/data/label2answer_vqarad_concat.json', 'w') as f:\n",
    "    json.dump(label_to_answer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Lenght of Training Set ',len(train_set), 'length test set', len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dicts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import torch\n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "from src.datasets.vqa_rad_dataset import VQRadDataModule\n",
    "from src.models.multimodal.flamingo_module import FlamingoModule\n",
    "\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from pytorch_lightning.loggers import CometLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "from src.utils.utils import load_config\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    seed_everything(42, workers=True)\n",
    "\n",
    "    img_mean = (0.48,0.48,0.48)\n",
    "    img_std = (0.265,0.265,0.265)\n",
    "\n",
    "    augmentations = {'train':\n",
    "        T.Compose(\n",
    "        [   \n",
    "            T.Resize((224,224)),\n",
    "            T.RandomRotation(10),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=img_mean, std=img_std)\n",
    "        ]),\n",
    "        'val':\n",
    "        T.Compose(\n",
    "        [\n",
    "            T.Resize((224,224)),\n",
    "            T.RandomRotation(10),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=img_mean, std=img_std)\n",
    "        ]),\n",
    "        'test':\n",
    "        T.Compose(\n",
    "        [\n",
    "            T.Resize((224,224)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=img_mean, std=img_std)\n",
    "        ])\n",
    "    }\n",
    "\n",
    "\n",
    "    args = load_config('/u/home/koksal/mlmi-vqa/configs','config.yaml')\n",
    "\n",
    "    args['train']['batch_size']=1\n",
    "    datamodule = VQRadDataModule(args, augmentations= augmentations)\n",
    "\n",
    "    train_loader = datamodule.train_dataloader()\n",
    "    val_loader = datamodule.val_dataloader()\n",
    "\n",
    "\n",
    "\n",
    "    model = FlamingoModule(args) \n",
    "\n",
    "    if args['pretrained']:\n",
    "        print(\"Pretrained Flamingo Model is loaded from checkpoint : \",args['pretrained'])\n",
    "        model.load_state_dict(torch.load(args['pretrained'])[\"state_dict\"])\n",
    "\n",
    "\n",
    "    from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "    from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['pretrained'] = '/u/home/koksal/mlmi-vqa/notebooks/lightning_logs/version_3/checkpoints/epoch=2-val_acc_epoch=0.00-val_total_loss_epoch=11.29-val_loss_generation_epoch=4.15-val_classification_loss_epoch=7.14.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args['pretrained']:\n",
    "    print(\"Pretrained Flamingo Model is loaded from checkpoint : \",args['pretrained'])\n",
    "    model.load_state_dict(torch.load(args['pretrained'])[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from transformers import GPT2Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn as nn\n",
    "import torch.nn.functional as F\n",
    "def generate(image, context, cur_model, batch, ntok=20):\n",
    "    for _ in range(ntok):\n",
    "        out,_ = cur_model({'image': image,'input_ids': context, \"index_eoq\": batch[\"index_eoq\"],\n",
    "        \"targets\": batch[\"targets\"],\"label\": batch[\"label\"]})\n",
    "        logits = out[:, -1, :]\n",
    "        indices_to_remove = logits < torch.topk(logits, 10)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = np.NINF\n",
    "        #next_tok1 = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1).squeeze(1)\n",
    "        #print(next_tok1.shape)\n",
    "        softmax_out = F.softmax(logits, dim=-1)\n",
    "        #print(softmax_out.shape)\n",
    "        next_tok = torch.argmax(softmax_out,dim=-1,keepdim=False)\n",
    "        #print(next_tok.shape)\n",
    "        context = torch.cat([context, next_tok.unsqueeze(-1)], dim=-1)\n",
    "    return context\n",
    "\n",
    "\n",
    "tokenizer = datamodule.train_dataset.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = datamodule.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader_iter = iter(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.ToPILImage()(batch[\"image\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' <EOQ>'+ ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, batch, ntok=20)\n",
    "print(\"Question : \", batch[\"question\"][0])\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule.train_dataset.tokenizer.all_special_ids[datamodule.train_dataset.tokenizer.all_special_tokens.index('<image>')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "image_encoder = EfficientNet.from_name('efficientnet-b0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_encoder._fc = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = nn.Sequential(*list(image_encoder.children())[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_encoder(torch.randn((1,3,224, 224))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer BERT\n",
    "from transformers import AutoTokenizer\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "gpt_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "gpt_tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import torch\n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "from src.datasets.vqa_rad_dataset import VQRadDataModule\n",
    "from src.models.multimodal.flamingo_module import FlamingoModule\n",
    "\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from pytorch_lightning.loggers import CometLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "from src.utils.utils import load_config\n",
    "import wandb\n",
    "\n",
    "\n",
    "args = load_config('/u/home/koksal/mlmi-vqa/configs','config.yaml')\n",
    "    \n",
    "#img_mean = (0.48,0.48,0.48)\n",
    "#img_std = (0.265,0.265,0.265)\n",
    "#img_mean = (0.48,0.48,0.48)\n",
    "#img_std = (0.265,0.265,0.265)\n",
    "\n",
    "augmentations = {'train':\n",
    "    T.Compose(\n",
    "    [   \n",
    "        T.Resize((args['train']['augmentation']['resize_size'])),\n",
    "        T.RandomResizedCrop(224,scale=args['train']['augmentation']['random_resize_scale'],\n",
    "                            ratio=args['train']['augmentation']['random_resize_ratio']),\n",
    "        T.RandomRotation((args['train']['augmentation']['random_rotation'])),\n",
    "        T.ColorJitter(brightness=args['train']['augmentation']['color_jitter']['brightness'],\n",
    "                        contrast=args['train']['augmentation']['color_jitter']['contrast'],\n",
    "                        saturation=args['train']['augmentation']['color_jitter']['saturation'],\n",
    "                        hue=args['train']['augmentation']['color_jitter']['hue']),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=args['dataset']['mean'], std=args['dataset']['std']),\n",
    "    ]),\n",
    "    'val':\n",
    "    T.Compose(\n",
    "    [\n",
    "        T.Resize((args['test']['augmentation']['resize_size'])),\n",
    "        T.RandomRotation((args['test']['augmentation']['random_rotation'])),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=args['dataset']['mean'], std=args['dataset']['std'])\n",
    "    ]),\n",
    "    'test':\n",
    "    T.Compose(\n",
    "    [\n",
    "        T.Resize((args['test']['augmentation']['resize_size'])),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=args['dataset']['mean'], std=args['dataset']['std'])\n",
    "    ])\n",
    "}\n",
    "\n",
    "wandb.init(project=\"flamingo-research\", config=args)\n",
    "mimic_datamodule = VQRadDataModule(args, augmentations= augmentations)\n",
    "# Data Loaders\n",
    "train_loader = mimic_datamodule.train_dataloader()\n",
    "val_loader = mimic_datamodule.val_dataloader()\n",
    "\n",
    "model = FlamingoModule(args) \n",
    "wandb.watch(model, log_freq=100)\n",
    "\n",
    "if args['pretrained']:\n",
    "    print(\"Pretrained Flamingo Model is loaded from checkpoint : \",args['pretrained'])\n",
    "    model.load_state_dict(torch.load(args['pretrained'])[\"state_dict\"])\n",
    "\n",
    "\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "\n",
    "if args['model']['classification_mode']:\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "                filename='{epoch}-{val_acc_epoch:.2f}-{val_total_loss_epoch:.2f}-{val_loss_generation_epoch:.2f}-{val_classification_loss_epoch:.2f}',\n",
    "                monitor= 'val_acc_epoch',\n",
    "                save_top_k = 3,\n",
    "                save_last=True,\n",
    "                mode=\"max\")\n",
    "\n",
    "    #early_stopping_callback = EarlyStopping(monitor=\"val_acc_epoch\", mode=\"max\",patience=10)\n",
    "    #early_stopping_callback = EarlyStopping(monitor=\"val_total_loss_epoch\", mode=\"min\",patience=5)\n",
    "else:\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "            filename='{epoch}-{val_loss_generation_epoch:.2f}',\n",
    "            monitor= 'val_loss_generation_epoch',\n",
    "            save_top_k = 3, \n",
    "            save_last=True,\n",
    "            mode=\"min\")\n",
    "    #early_stopping_callback = EarlyStopping(monitor=\"val_loss_generation_epoch\", mode=\"min\",patience=10)\n",
    "\n",
    "#early_stopping_callback = EarlyStopping(monitor=\"val_acc_epoch\", mode=\"max\",patience=10)\n",
    "\n",
    "# All our models are trained using the AdamW optimizer with global norm clipping of 1\n",
    "print(args['train']['devices'])\n",
    "trainer = pl.Trainer(max_epochs=args['train']['num_epochs'],\n",
    "                    accelerator=args['train']['accelerator'], devices=args['train']['devices'],\n",
    "                    callbacks=[lr_monitor, checkpoint_callback, ], #early_stopping_callback],\n",
    "                    gradient_clip_val=args['optimizer']['gradient_clip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_layer = model.flamingo_palm.bert_decoder.bert.encoder.layer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aff75fd314749a29a2dbcc580b95667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type bert to instantiate a model of type bert-generation. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb605df159524a55bcbc555cfcc5f4cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertGenerationDecoder: ['bert.embeddings.token_type_embeddings.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'bert.pooler.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertGenerationDecoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertGenerationDecoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertGenerationDecoder were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (BertGenerationDecoder, BertLayer, BertConfig)\n",
    "bert_decoder = BertGenerationDecoder.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\", add_cross_attention=False, is_decoder=True, bos_token_id=None, eos_token_id=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertGenerationDecoder(\n",
       "  (bert): BertGenerationEncoder(\n",
       "    (embeddings): BertGenerationEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertGenerationLayer(\n",
       "          (attention): BertGenerationAttention(\n",
       "            (self): BertGenerationSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertGenerationSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertGenerationIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertGenerationOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertGenerationLayer(\n",
       "          (attention): BertGenerationAttention(\n",
       "            (self): BertGenerationSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertGenerationSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertGenerationIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertGenerationOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertGenerationLayer(\n",
       "          (attention): BertGenerationAttention(\n",
       "            (self): BertGenerationSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertGenerationSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertGenerationIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertGenerationOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertGenerationLayer(\n",
       "          (attention): BertGenerationAttention(\n",
       "            (self): BertGenerationSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertGenerationSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertGenerationIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertGenerationOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertGenerationLayer(\n",
       "          (attention): BertGenerationAttention(\n",
       "            (self): BertGenerationSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertGenerationSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertGenerationIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertGenerationOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertGenerationLayer(\n",
       "          (attention): BertGenerationAttention(\n",
       "            (self): BertGenerationSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertGenerationSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertGenerationIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertGenerationOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertGenerationLayer(\n",
       "          (attention): BertGenerationAttention(\n",
       "            (self): BertGenerationSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertGenerationSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertGenerationIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertGenerationOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertGenerationLayer(\n",
       "          (attention): BertGenerationAttention(\n",
       "            (self): BertGenerationSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertGenerationSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertGenerationIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertGenerationOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertGenerationLayer(\n",
       "          (attention): BertGenerationAttention(\n",
       "            (self): BertGenerationSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertGenerationSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertGenerationIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertGenerationOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertGenerationLayer(\n",
       "          (attention): BertGenerationAttention(\n",
       "            (self): BertGenerationSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertGenerationSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertGenerationIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertGenerationOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertGenerationLayer(\n",
       "          (attention): BertGenerationAttention(\n",
       "            (self): BertGenerationSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertGenerationSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertGenerationIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertGenerationOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertGenerationLayer(\n",
       "          (attention): BertGenerationAttention(\n",
       "            (self): BertGenerationSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertGenerationSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertGenerationIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertGenerationOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): BertGenerationOnlyLMHead(\n",
       "    (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.multimodal.flamingo_model import Residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertlayer = bert_decoder.base_model.encoder.layer[1]\n",
    "example_layer = Residual(bertlayer)\n",
    "example_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertlayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertlayer(torch.randn(2,64,768),ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bertlayer(torch.randn(2,64,768))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertlayer(torch.randn(2,64,768))[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertlayer(torch.randn(2,64,768))[1][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_decoder = BertGenerationDecoder.from_pretrained('emilyalsentzer/Bio_ClinicalBERT', add_cross_attention=False, is_decoder=True, bos_token_id=None, eos_token_id=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode_plus('New york is the best liver', padding='max_length', truncation=True, max_length=30, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "image_encoder = timm.create_model(' ', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (conv_stem): Conv2dSame(3, 48, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "  (bn1): BatchNormAct2d(\n",
       "    48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "    (drop): Identity()\n",
       "    (act): SiLU(inplace=True)\n",
       "  )\n",
       "  (blocks): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): DepthwiseSeparableConv(\n",
       "        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): DepthwiseSeparableConv(\n",
       "        (conv_dw): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pw): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (2): DepthwiseSeparableConv(\n",
       "        (conv_dw): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pw): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2dSame(144, 144, kernel_size=(3, 3), stride=(2, 2), groups=144, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (4): InvertedResidual(\n",
       "        (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2dSame(240, 240, kernel_size=(5, 5), stride=(2, 2), groups=240, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(384, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(16, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv_pw): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(384, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(16, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (conv_pw): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(384, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(16, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (4): InvertedResidual(\n",
       "        (conv_pw): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(384, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(16, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2dSame(384, 384, kernel_size=(3, 3), stride=(2, 2), groups=384, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(384, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(16, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv_pw): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (conv_pw): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (4): InvertedResidual(\n",
       "        (conv_pw): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (5): InvertedResidual(\n",
       "        (conv_pw): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (6): InvertedResidual(\n",
       "        (conv_pw): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(768, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          176, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(1056, 1056, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1056, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          176, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(1056, 1056, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1056, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          176, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(1056, 1056, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1056, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          176, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (4): InvertedResidual(\n",
       "        (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(1056, 1056, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1056, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          176, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (5): InvertedResidual(\n",
       "        (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(1056, 1056, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1056, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          176, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (6): InvertedResidual(\n",
       "        (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(1056, 1056, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1056, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          176, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2dSame(1056, 1056, kernel_size=(5, 5), stride=(2, 2), groups=1056, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1056, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(1824, 1824, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1824, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(1824, 1824, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1824, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(1824, 1824, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1824, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (4): InvertedResidual(\n",
       "        (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(1824, 1824, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1824, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (5): InvertedResidual(\n",
       "        (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(1824, 1824, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1824, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (6): InvertedResidual(\n",
       "        (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(1824, 1824, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1824, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (7): InvertedResidual(\n",
       "        (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(1824, 1824, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1824, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (8): InvertedResidual(\n",
       "        (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(1824, 1824, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1824, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(1824, 1824, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1824, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1824, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(3072, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(128, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(3072, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv_pw): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(3072, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(128, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(3072, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_head): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (bn2): BatchNormAct2d(\n",
       "    2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "    (drop): Identity()\n",
       "    (act): SiLU(inplace=True)\n",
       "  )\n",
       "  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
       "  (classifier): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.data import resolve_data_config\n",
    "config = resolve_data_config({}, model=image_encoder)\n",
    "from timm.data.transforms_factory import create_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_size': (3, 456, 456),\n",
       " 'interpolation': 'bicubic',\n",
       " 'mean': (0.485, 0.456, 0.406),\n",
       " 'std': (0.229, 0.224, 0.225),\n",
       " 'crop_pct': 0.934}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = create_transform(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=488, interpolation=bicubic, max_size=None, antialias=None)\n",
       "    CenterCrop(size=(456, 456))\n",
       "    ToTensor()\n",
       "    Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlmi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "84dc3abdd12200a074307254a90847c7bebb194f1a3b9d1fe47efe2b88539b79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
