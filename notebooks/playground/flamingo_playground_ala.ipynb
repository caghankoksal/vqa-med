{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f79645d",
   "metadata": {},
   "source": [
    "# FLAMINGO Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bfa1918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from flamingo_pytorch import PerceiverResampler\n",
    "from flamingo_pytorch import GatedCrossAttentionBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b1811a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "perceive = PerceiverResampler(\n",
    "    dim = 1024,\n",
    "    depth = 2,\n",
    "    dim_head = 64,\n",
    "    heads = 8,\n",
    "    num_latents = 64,    # the number of latents to shrink your media sequence to, perceiver style\n",
    "    num_time_embeds = 4  # say you have 4 images maximum in your dialogue\n",
    ")\n",
    "\n",
    "medias = torch.randn(1, 2, 256, 1024) # (batch, time, sequence length, dimension)\n",
    "perceived = perceive(medias) # (1, 2, 64, 1024) - (batch, time, num latents, dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db96deea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_attn = GatedCrossAttentionBlock(\n",
    "    dim = 1024,\n",
    "    dim_head = 64,\n",
    "    heads = 8\n",
    ")\n",
    "\n",
    "text = torch.randn(1, 512, 1024)\n",
    "perceived = torch.randn(1, 2, 64, 1024)\n",
    "\n",
    "media_locations = torch.randint(0, 2, (1, 512)).bool()\n",
    "\n",
    "text = cross_attn(\n",
    "    text,\n",
    "    perceived,\n",
    "    media_locations = media_locations\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b5f093",
   "metadata": {},
   "source": [
    "## WITH PALM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c6dea06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_pytorch.vit import ViT\n",
    "from vit_pytorch.extractor import Extractor\n",
    "\n",
    "import torch\n",
    "from flamingo_pytorch import FlamingoPaLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcd002bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = ViT(\n",
    "    image_size = 256,\n",
    "    patch_size = 32,\n",
    "    num_classes = 1000,\n",
    "    dim = 1024,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 2048,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ")\n",
    "\n",
    "vit = Extractor(vit, return_embeddings_only = True)\n",
    "\n",
    "# first take your trained image encoder and wrap it in an adapter that returns the image embeddings\n",
    "# here we use the ViT from the vit-pytorch library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75e6456",
   "metadata": {},
   "source": [
    "- image_size: int.\n",
    "Image size. If you have rectangular images, make sure your image size is the maximum of the width and height\n",
    "\n",
    "- patch_size: int.\n",
    "Number of patches. image_size must be divisible by patch_size.\n",
    "The number of patches is: n = (image_size // patch_size) ** 2 and n must be greater than 16.\n",
    "\n",
    "- num_classes: int.\n",
    "Number of classes to classify\n",
    "\n",
    "- dim: int.\n",
    "Last dimension of output tensor after linear transformation nn.Linear(..., dim).\n",
    "\n",
    "- depth: int.\n",
    "Number of Transformer blocks.\n",
    "\n",
    "- heads: int.\n",
    "Number of heads in Multi-head Attention layer.\n",
    "\n",
    "- mlp_dim: int.\n",
    "Dimension of the MLP (FeedForward) layer.\n",
    "\n",
    "- channels: int, default 3.\n",
    "Number of image's channels.\n",
    "\n",
    "- dropout: float between [0, 1], default 0..\n",
    "Dropout rate.\n",
    "\n",
    "- emb_dropout: float between [0, 1], default 0.\n",
    "Embedding dropout rate.\n",
    "\n",
    "- pool: string, either cls token pooling or mean pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ea74379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a PaLM language model, the 540 billion parameter model from google that shows signs of general intelligence\n",
    "\n",
    "flamingo_palm = FlamingoPaLM(\n",
    "    num_tokens = 20000,          # number of tokens\n",
    "    dim = 1024,                  # dimensions\n",
    "    depth = 12,                  # depth\n",
    "    heads = 8,                   # attention heads\n",
    "    dim_head = 64,               # dimension per attention head\n",
    "    img_encoder = vit,           # plugin your image encoder (this can be optional if you pass in the image embeddings separately, but probably want to train end to end given the perceiver resampler)\n",
    "    media_token_id = 3,          # the token id representing the [media] or [image]\n",
    "    cross_attn_every = 3,        # how often to cross attend\n",
    "    perceiver_num_latents = 64,  # perceiver number of latents, should be smaller than the sequence length of the image tokens\n",
    "    perceiver_depth = 2          # perceiver resampler depth\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87b26c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train your PaLM as usual\n",
    "\n",
    "text = torch.randint(0, 20000, (2, 512))\n",
    "\n",
    "palm_logits = flamingo_palm(text)\n",
    "\n",
    "# after much training off the regular PaLM logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2219cdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now you are ready to train Flamingo + PaLM\n",
    "# by passing in images, it automatically freezes everything but the perceiver and cross attention blocks, as in the paper\n",
    "\n",
    "dialogue = torch.randint(0, 20000, (4, 512))\n",
    "images = torch.randn(4, 2, 3, 256, 256)\n",
    "\n",
    "flamingo_logits = flamingo_palm(dialogue, images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff6da31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do your usual cross entropy loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
