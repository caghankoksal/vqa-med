{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import torchxrayvision as xrv\n",
    "from torchvision import transforms\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from typing import Optional\n",
    "from pathlib import Path, PurePath\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VqaRadDataset(Dataset):\n",
    "    def __init__(self, root, split:str = 'train',\n",
    "                 transform=None, tokenizer=None):\n",
    "        \n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        \n",
    "        assert split in ['train', 'val', 'test']\n",
    "        self.split = split\n",
    "        if split == \"train\":\n",
    "            self.annotations = pd.read_csv(os.path.join(root,\"vqa_rad_train.csv\"))\n",
    "        elif split == \"val\":\n",
    "            self.annotations = pd.read_csv(os.path.join(root,\"vqa_rad_valid.csv\"))\n",
    "        elif split == \"test\":\n",
    "            self.annotations = pd.read_csv(os.path.join(root,\"vqa_rad_test.csv\"))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        cur_ann = self.annotations.iloc[idx]\n",
    "        image_name = cur_ann[\"img_id\"]\n",
    "        answer = cur_ann[\"answer\"]\n",
    "        mode = cur_ann[\"mode\"]\n",
    "        question = cur_ann[\"question\"]\n",
    "        \n",
    "        #####\n",
    "        ## Image Processing\n",
    "        #####\n",
    "        # Img path of the given question\n",
    "        cur_image_path = PurePath(self.root, \"imgs\", image_name).as_posix()\n",
    "        \n",
    "        img = Image.open(cur_image_path).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = transforms.ToTensor()(img)\n",
    "            \n",
    "        #####\n",
    "        ## Text Processing\n",
    "        #####\n",
    "        question_tokens = word_tokenize(question)\n",
    "        answer_tokens = word_tokenize(answer)\n",
    "        tokens = np.concatenate([[\"<BOS>\"],[\"<image>\"],[\"Question:\"],question_tokens,[\"Answer:\"],answer_tokens,[\"<EOC>\"]]).ravel()\n",
    "        \n",
    "        #text = f\"<BOS> <image> Question: {question} Answer: {answer} <EOC>\"\n",
    "        #tokens = word_tokenize(text)\n",
    "        \n",
    "        sample = {'image': img, 'text': tokens}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from flamingo_pytorch import FlamingoPaLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class FlamingoModule(pl.LightningModule):\n",
    "    def __init__(self, image_encoder, target):\n",
    "        super().__init__()\n",
    "        self.image_encoder = image_encoder\n",
    "        self.model = FlamingoPaLM(\n",
    "                        num_tokens = 15185,          # number of tokens\n",
    "                        dim = 18,                  # dimensions\n",
    "                        depth = 12,                  # depth\n",
    "                        heads = 8,                   # attention heads\n",
    "                        dim_head = 64,               # dimension per attention head\n",
    "                        img_encoder = image_encoder, # plugin your image encoder (this can be optional if you pass in the image embeddings separately, but probably want to train end to end given the perceiver resampler)\n",
    "                        media_token_id = 3,          # the token id representing the [media] or [image]\n",
    "                        cross_attn_every = 3,        # how often to cross attend\n",
    "                        perceiver_num_latents = 16,  # perceiver number of latents, should be smaller than the sequence length of the image tokens\n",
    "                        perceiver_depth = 2          # perceiver resampler depth\n",
    "                    )\n",
    "\n",
    "        # TODO DEFINE LOSS cross entropy?\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "        # elif:\n",
    "            # self.loss =\n",
    "        \n",
    "        self.target = target\n",
    "\n",
    "        self.train_preds = []\n",
    "        self.train_gts = []\n",
    "        self.val_preds = []\n",
    "        self.val_gts = []\n",
    "        self.test_preds = []\n",
    "        self.test_gts = []\n",
    "        self.reset_metrics()\n",
    "\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        self.test_loss = []\n",
    "\n",
    "        self.writer = SummaryWriter()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        if self.target == 'palm':\n",
    "            text, y = batch\n",
    "            logits = self.model(\n",
    "                text = text,\n",
    "            )\n",
    "        elif self.target == 'flamingo':\n",
    "            text, image, y = batch\n",
    "            logits = self.model(\n",
    "                text = text,\n",
    "                images = image\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        loss = self.loss(logits.float(), y)\n",
    "        self.log('train/loss', loss, on_step=False, on_epoch=True)\n",
    "        return {'loss': loss}\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # validation_step defines the validation loop.\n",
    "\n",
    "        if self.target == 'palm':\n",
    "            text, y = batch\n",
    "            logits = self.model(\n",
    "                text = text,\n",
    "            )\n",
    "        elif self.target == 'flamingo':\n",
    "            text, image, y = batch\n",
    "            logits = self.model(\n",
    "                text = text,\n",
    "                images = image\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        loss = self.loss(logits.squeeze(), y)\n",
    "        self.update_metrics(text, logits, split='val')\n",
    "        self.val_loss.append(loss.item())\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # test_step defines the test loop.\n",
    "        if self.target == 'palm':\n",
    "            text, y = batch\n",
    "            logits = self.model(\n",
    "                text = text,\n",
    "            )\n",
    "        elif self.target == 'flamingo':\n",
    "            text, image, y = batch\n",
    "            logits = self.model(\n",
    "                text = text,\n",
    "                images = image\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        loss = self.loss(logits.squeeze(), y)\n",
    "        self.update_metrics(text, logits, split='test')\n",
    "        self.test_loss.append(loss.item())\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "    def reset_metrics(self, split=None):\n",
    "        if split == 'train':\n",
    "            self.train_preds = []\n",
    "            self.train_gts = []\n",
    "        elif split == 'val':\n",
    "            self.val_preds = []\n",
    "            self.val_gts = []\n",
    "        elif split == 'test':\n",
    "            self.test_preds = []\n",
    "            self.test_gts = []\n",
    "        else:\n",
    "            self.train_preds = []\n",
    "            self.train_gts = []\n",
    "            self.val_preds = []\n",
    "            self.val_gts = []\n",
    "            self.test_preds = []\n",
    "            self.test_gts = []\n",
    "\n",
    "    def update_metrics(self, gt, pred, split='train'):\n",
    "        if split == 'train':\n",
    "            self.train_preds.extend(pred.detach().cpu().numpy().argmax(1))\n",
    "            self.train_gts.extend(gt.detach().cpu().numpy())\n",
    "        elif split == 'val':\n",
    "            self.val_preds.extend(pred.detach().cpu().numpy().argmax(1))\n",
    "            self.val_gts.extend(gt.detach().cpu().numpy())\n",
    "        elif split == 'test':\n",
    "            self.test_preds.extend(pred.detach().cpu().numpy().argmax(1))\n",
    "            self.test_gts.extend(gt.detach().cpu().numpy())\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    def training_epoch_end(self, output):\n",
    "        loss = 0\n",
    "        for o in output:\n",
    "            loss +=  o['loss'].item()\n",
    "        loss = loss / len(output)\n",
    "        self.writer.add_scalar('Epoch_loss/train', loss, self.current_epoch)\n",
    "        self.reset_metrics(split='train')\n",
    "\n",
    "    def validation_epoch_end(self, output):\n",
    "        loss = 0\n",
    "        for o in output:\n",
    "            loss +=  o['loss'].item()\n",
    "        loss = loss / len(output)\n",
    "        self.log('val_loss', loss)\n",
    "        self.writer.add_scalar('Epoch_loss/validation', loss, self.current_epoch)\n",
    "        self.reset_metrics(split='val')\n",
    "    \n",
    "    def test_epoch_end(self, output):\n",
    "        loss = 0\n",
    "        for o in output:\n",
    "            loss +=  o['loss'].item()\n",
    "        loss = loss / len(output)\n",
    "        self.writer.add_scalar('Epoch_loss/test', loss, self.current_epoch)\n",
    "        self.reset_metrics(split='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PureWindowsPath('D:/Dev/PythonProjects/Project/emic-vqa/data/external/vqa_rad')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = PurePath(r\"D:\\Dev\\PythonProjects\\Project\\emic-vqa\\data\\external\\vqa_rad\")\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/Dev/PythonProjects/Project/emic-vqa/data/external/vqa_rad\n"
     ]
    }
   ],
   "source": [
    "root = path.as_posix()\n",
    "print(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(img):\n",
    "    img = xrv.datasets.normalize(img, 255) \n",
    "\n",
    "    # Check that images are 2D arrays\n",
    "    if len(img.shape) > 2:\n",
    "        img = img[:, :, 0]\n",
    "    if len(img.shape) < 2:\n",
    "        print(\"error, dimension lower than 2 for image\")\n",
    "\n",
    "    # Add color channel\n",
    "    img = img[None, :, :]\n",
    "\n",
    "    transform = torchvision.transforms.Compose([xrv.datasets.XRayCenterCrop(),\n",
    "                                                xrv.datasets.XRayResizer(224)])\n",
    "\n",
    "    img = transform(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_transform = transforms.Compose([xrv.datasets.XRayCenterCrop(),\n",
    "                                     xrv.datasets.XRayResizer(224),])\n",
    "\n",
    "# train_dataset = VqaRadDataset(root, split='train',transform=pil_transform)\n",
    "# val_dataset = VqaRadDataset(root, split='val',transform=pil_transform)\n",
    "# test_dataset = VqaRadDataset(root, split='test',transform=pil_transform)\n",
    "\n",
    "train_dataset = VqaRadDataset(root, split='train')\n",
    "val_dataset = VqaRadDataset(root, split='val')\n",
    "test_dataset = VqaRadDataset(root, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset Size: 1797\n",
      "Validation Dataset Size: 226\n",
      "Test Dataset Size: 225\n"
     ]
    }
   ],
   "source": [
    "print('Train Dataset Size:', len(train_dataset))\n",
    "print('Validation Dataset Size:', len(val_dataset))\n",
    "print('Test Dataset Size:', len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
      "         [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
      "         [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
      "         [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
      "         [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
      "         [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
      "         [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039]]]), 'text': array(['<BOS>', '<image>', 'Question:', 'what', 'type', 'of', 'imaging',\n",
      "       'does', 'this', 'not', 'represent', 'Answer:', 'ultrasound',\n",
      "       '<EOC>'], dtype='<U10')}\n"
     ]
    }
   ],
   "source": [
    "sample = train_dataset[3]\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading weights...\n",
      "If this fails you can run `wget https://github.com/mlmed/torchxrayvision/releases/download/v1/nih-pc-chex-mimic_ch-google-openi-kaggle-densenet121-d121-tw-lr001-rot45-tr15-sc15-seed0-best.pt -O C:\\Users\\alaed\\.torchxrayvision\\models_data\\nih-pc-chex-mimic_ch-google-openi-kaggle-densenet121-d121-tw-lr001-rot45-tr15-sc15-seed0-best.pt`\n",
      "[██████████████████████████████████████████████████]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Can't instantiate abstract class FlamingoModule with abstract methods forward",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [44]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m      6\u001b[0m image_encoder \u001b[38;5;241m=\u001b[39m xrv\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mDenseNet(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdensenet121-res224-all\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mFlamingoModule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mflamingo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: Can't instantiate abstract class FlamingoModule with abstract methods forward"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, pin_memory=True, num_workers=4)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False, pin_memory=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, pin_memory=True, num_workers=4)\n",
    "\n",
    "\n",
    "image_encoder = xrv.models.DenseNet(weights=\"densenet121-res224-all\")\n",
    "model = FlamingoModule(image_encoder, target='flamingo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train PaLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | image_encoder | DenseNet         | 7.0 M \n",
      "1 | model         | FlamingoPaLM     | 7.8 M \n",
      "2 | loss          | CrossEntropyLoss | 0     \n",
      "---------------------------------------------------\n",
      "806 K     Trainable params\n",
      "7.0 M     Non-trainable params\n",
      "7.8 M     Total params\n",
      "31.091    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:   1%|          | 12/1147 [00:56<1:28:21,  4.67s/it, loss=0.186, v_num=3]  "
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(gpus=[1], max_epochs=100, num_sanity_val_steps=0) \n",
    "trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values = model.val_loss\n",
    "plt.plot(loss_values)\n",
    "plt.title(\"Validation loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save language model\n",
    "torch.save(model.state_dict(), 'PaLM_mimic_BPETokenizer_bs32_Adam.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Training Setup For Flamingo + PaLM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_transform = transforms.Compose([xrv.datasets.XRayCenterCrop(),\n",
    "                                    xrv.datasets.XRayResizer(224),])\n",
    "                                    \n",
    "train_dataset = MimicDatset(target='flamingo', split='train', pil_transform=pil_transform)\n",
    "val_dataset = MimicDatset(target='flamingo', split='val', pil_transform=pil_transform)\n",
    "test_dataset = MimicDatset(target='flamingo', split='test', pil_transform=pil_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, pin_memory=True, num_workers=4)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False, pin_memory=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False, pin_memory=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model, dataloaders=test_loader)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "30ec110bc924a9e139919a87e1ff85100b6c769b1dd45b8d281b6aff673f8e03"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
