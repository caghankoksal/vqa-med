{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "import torchxrayvision as xrv\n",
    "import numpy as np\n",
    "import pydicom as dicom\n",
    "import csv\n",
    "import pickle as pkl\n",
    "\n",
    "from torch import nn\n",
    "from typing import Optional\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of slices: 6\n",
      "[slice(0, 62851, None), slice(62851, 125702, None), slice(125702, 188553, None), slice(188553, 251404, None), slice(251404, 314255, None), slice(314255, 377110, None)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 87/62855 [00:02<34:52, 30.00it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/andrei/TUM/SoSe2022/MLMI/emic-vqa/notebooks/playground/train_flamingo.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 60>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrei/TUM/SoSe2022/MLMI/emic-vqa/notebooks/playground/train_flamingo.ipynb#ch0000002?line=58'>59</a>\u001b[0m         processes\u001b[39m.\u001b[39mappend(p)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrei/TUM/SoSe2022/MLMI/emic-vqa/notebooks/playground/train_flamingo.ipynb#ch0000002?line=59'>60</a>\u001b[0m \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m processes:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/andrei/TUM/SoSe2022/MLMI/emic-vqa/notebooks/playground/train_flamingo.ipynb#ch0000002?line=60'>61</a>\u001b[0m         p\u001b[39m.\u001b[39;49mjoin()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrei/TUM/SoSe2022/MLMI/emic-vqa/notebooks/playground/train_flamingo.ipynb#ch0000002?line=63'>64</a>\u001b[0m data_list \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrei/TUM/SoSe2022/MLMI/emic-vqa/notebooks/playground/train_flamingo.ipynb#ch0000002?line=64'>65</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(data_list))\n",
      "File \u001b[0;32m~/anaconda3/envs/mlmi/lib/python3.10/multiprocessing/process.py:149\u001b[0m, in \u001b[0;36mBaseProcess.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/multiprocessing/process.py?line=146'>147</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parent_pid \u001b[39m==\u001b[39m os\u001b[39m.\u001b[39mgetpid(), \u001b[39m'\u001b[39m\u001b[39mcan only join a child process\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/multiprocessing/process.py?line=147'>148</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m'\u001b[39m\u001b[39mcan only join a started process\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/multiprocessing/process.py?line=148'>149</a>\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_popen\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/multiprocessing/process.py?line=149'>150</a>\u001b[0m \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/multiprocessing/process.py?line=150'>151</a>\u001b[0m     _children\u001b[39m.\u001b[39mdiscard(\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlmi/lib/python3.10/multiprocessing/popen_fork.py:43\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/multiprocessing/popen_fork.py?line=40'>41</a>\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/multiprocessing/popen_fork.py?line=41'>42</a>\u001b[0m     \u001b[39m# This shouldn't block if wait() returned successfully.\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/multiprocessing/popen_fork.py?line=42'>43</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpoll(os\u001b[39m.\u001b[39;49mWNOHANG \u001b[39mif\u001b[39;49;00m timeout \u001b[39m==\u001b[39;49m \u001b[39m0.0\u001b[39;49m \u001b[39melse\u001b[39;49;00m \u001b[39m0\u001b[39;49m)\n\u001b[1;32m     <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/multiprocessing/popen_fork.py?line=43'>44</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturncode\n",
      "File \u001b[0;32m~/anaconda3/envs/mlmi/lib/python3.10/multiprocessing/popen_fork.py:27\u001b[0m, in \u001b[0;36mPopen.poll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/multiprocessing/popen_fork.py?line=24'>25</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturncode \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/multiprocessing/popen_fork.py?line=25'>26</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/multiprocessing/popen_fork.py?line=26'>27</a>\u001b[0m         pid, sts \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mwaitpid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpid, flag)\n\u001b[1;32m     <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/multiprocessing/popen_fork.py?line=27'>28</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/multiprocessing/popen_fork.py?line=28'>29</a>\u001b[0m         \u001b[39m# Child process not yet created. See #1731717\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/multiprocessing/popen_fork.py?line=29'>30</a>\u001b[0m         \u001b[39m# e.errno == errno.ECHILD == 10\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/multiprocessing/popen_fork.py?line=30'>31</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Process Process-3:\n",
      "  0%|          | 93/62851 [00:02<32:35, 32.10it/s]Process Process-6:\n",
      "Process Process-4:\n",
      "Process Process-2:\n",
      "Traceback (most recent call last):\n",
      "\n",
      "Process Process-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/andrei/anaconda3/envs/mlmi/lib/python3.10/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/andrei/anaconda3/envs/mlmi/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/andrei/anaconda3/envs/mlmi/lib/python3.10/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/andrei/anaconda3/envs/mlmi/lib/python3.10/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Process Process-5:\n",
      "  File \"/tmp/ipykernel_10782/3684903847.py\", line 37, in process\n",
      "    if id in report['study_id']:\n",
      "  File \"/home/andrei/anaconda3/envs/mlmi/lib/python3.10/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/andrei/anaconda3/envs/mlmi/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/andrei/anaconda3/envs/mlmi/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_10782/3684903847.py\", line 37, in process\n",
      "    if id in report['study_id']:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/andrei/anaconda3/envs/mlmi/lib/python3.10/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/tmp/ipykernel_10782/3684903847.py\", line 51, in process\n",
      "    data.append(entry)\n",
      "  File \"/home/andrei/anaconda3/envs/mlmi/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<string>\", line 2, in append\n",
      "  File \"/home/andrei/anaconda3/envs/mlmi/lib/python3.10/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/tmp/ipykernel_10782/3684903847.py\", line 37, in process\n",
      "    if id in report['study_id']:\n",
      "  File \"/home/andrei/anaconda3/envs/mlmi/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_10782/3684903847.py\", line 37, in process\n",
      "    if id in report['study_id']:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/andrei/anaconda3/envs/mlmi/lib/python3.10/multiprocessing/managers.py\", line 818, in _callmethod\n",
      "    kind, result = conn.recv()\n",
      "  File \"/home/andrei/anaconda3/envs/mlmi/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/andrei/anaconda3/envs/mlmi/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/andrei/anaconda3/envs/mlmi/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/andrei/anaconda3/envs/mlmi/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/tmp/ipykernel_10782/3684903847.py\", line 37, in process\n",
      "    if id in report['study_id']:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# Run this only once to create the data dictionaries, DATA SET SPECIFIC\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Process, Manager\n",
    "\n",
    "dataset_path = Path('/mnt/209C31C29C3192F0/Datasets/Mimic-CXR/physionet.org/files/mimic-cxr/2.0.0/')\n",
    "\n",
    "# be careful here, loading both amounts to ~10GB of memory\n",
    "image_list = list(csv.DictReader(open(dataset_path / 'cxr-record-list.csv', 'r'), delimiter=','))         # list of dictionaries\n",
    "report_list = list(csv.DictReader(open(dataset_path / 'cxr-study-list.csv', 'r'), delimiter=','))           # list of dictionaries\n",
    "\n",
    "dataset_length = len(image_list)\n",
    "\n",
    "n_proc = 6\n",
    "offset = 0\n",
    "\n",
    "chunksize = dataset_length // n_proc\n",
    "proc_slices = []\n",
    "\n",
    "for i_proc in range(n_proc):\n",
    "        chunkstart = int(offset + (i_proc * chunksize))\n",
    "        # make sure to include the division remainder for the last process\n",
    "        chunkend = int(offset + (i_proc + 1) * chunksize) if i_proc < n_proc - 1 else int(offset + dataset_length)\n",
    "        proc_slices.append(np.s_[chunkstart:chunkend])\n",
    "\n",
    "print(f'Number of slices: {len(proc_slices)}\\n{proc_slices}')\n",
    "\n",
    "def process(data, slice, rank):                    # split it up into slices\n",
    "    # preprocess reports and images\n",
    "    # iterate through images and find corresponding report\n",
    "    for image in tqdm(image_list[slice]):\n",
    "        image_path = image['path']\n",
    "        id = image['study_id']\n",
    "        # find corresponding report\n",
    "        report_path = None\n",
    "        for report in report_list:\n",
    "            if id in report['study_id']:\n",
    "                report_path = report['path']\n",
    "        \n",
    "        if report is None:\n",
    "                print(f'Not found report for image with path {image_path}, study id: {id} and dicom')\n",
    "                continue\n",
    "\n",
    "        entry = {'subject_id': report['subject_id'],\n",
    "                'study_id': id,\n",
    "                'dicom_id': image['dicom_id'],\n",
    "                'report_path': report_path,\n",
    "                'image_path': image_path,\n",
    "                }\n",
    "\n",
    "        data.append(entry)\n",
    "\n",
    "\n",
    "data = Manager().list()\n",
    "processes = []\n",
    "for i in range(n_proc):\n",
    "        p = Process(target=process, args=(data, proc_slices[i], i))  # Passing the list\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "for p in processes:\n",
    "        p.join()\n",
    "\n",
    "\n",
    "data_list = list(data)\n",
    "print(len(data_list))\n",
    "with open(dataset_path / 'images2reports.pkl', 'wb') as f:\n",
    "        pkl.dump(data_list, f, pkl.DEFAULT_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subject_id': '10000032', 'study_id': '50414267', 'dicom_id': '02aa804e-bde0afdd-112c0b34-7bc16630-4e384014', 'path': 'files/p10/p10000032/s50414267/02aa804e-bde0afdd-112c0b34-7bc16630-4e384014.dcm'}\n",
      "{'subject_id': '10000032', 'study_id': '50414267', 'path': 'files/p10/p10000032/s50414267.txt'}\n"
     ]
    }
   ],
   "source": [
    "# visualize data\n",
    "dataset_path = Path('/mnt/209C31C29C3192F0/Datasets/Mimic-CXR/physionet.org/files/mimic-cxr/2.0.0/')\n",
    "image_list = list(csv.DictReader(open(dataset_path / 'cxr-record-list.csv', 'r'), delimiter=','))         # list of dictionaries\n",
    "report_list = list(csv.DictReader(open(dataset_path / 'cxr-study-list.csv', 'r'), delimiter=','))           # list of dictionaries\n",
    "\n",
    "print(image_list[0])\n",
    "print(report_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast, AutoTokenizer\n",
    "import pydicom as dicom\n",
    "\n",
    "class MimicDatset(Dataset):\n",
    "    def __init__(self, target:str = 'palm', split:str = 'train', pil_transform: Optional[transforms.Compose] = None, tensor_transform: Optional[transforms.Compose] = None):\n",
    "        self.target = target\n",
    "        assert target in ['palm', 'flamingo']\n",
    "\n",
    "        self.split = split\n",
    "        assert split in ['train', 'val', 'test']\n",
    "\n",
    "        self.pil_transform = pil_transform\n",
    "        self.tensor_transform = tensor_transform\n",
    "        \n",
    "        self.dataset_path = Path('/mnt/209C31C29C3192F0/Datasets/Mimic-CXR/physionet.org/files/mimic-cxr/2.0.0/')\n",
    "\n",
    "        with open(dataset_path / 'images2reports.pkl', 'rb') as f:\n",
    "            data_list_pkl = pkl.load(f)\n",
    "\n",
    "        self.data = data_list_pkl\n",
    "        del data_list_pkl\n",
    "        self.dataset_length = len(self.data)\n",
    "\n",
    "        # self.tokenizer = PreTrainedTokenizerFast(tokenizer_file=str(self.data_dir / 'mimic_tokenizer.json'), pad_token='<pad>')\n",
    "        \n",
    "    def __len__(self):                  # TODO figure this out, for now only limited data, TODO dynamic loading\n",
    "        if self.target == 'palm':\n",
    "            # return self.dataset_length\n",
    "            return 10\n",
    "        else:\n",
    "            # return self.dataset_length\n",
    "            return 2000\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # don't forget about the target\n",
    "\n",
    "        # get item and convert text into tokens\n",
    "        data_sample = self.data[index]\n",
    "\n",
    "        with open(self.dataset_path / data_sample['report_path'], 'r') as f:\n",
    "            unfiltered_report = f.readlines()\n",
    "\n",
    "        rep = ''.join(unfiltered_report)\n",
    "        report = rep[(rep.find('FINDINGS:') + 9):rep.find('IMPRESSION:')].replace('\\n','')\n",
    "\n",
    "        # tokenize report ? \n",
    "        # report_tokenized = np.array(self.tokenizer(report))\n",
    "\n",
    "        if self.target == 'palm':\n",
    "            return torch.from_numpy(report).to(self.device)\n",
    "        \n",
    "        else:\n",
    "            # get corresponding images\n",
    "            image = np.array(dicom.dcmread(self.dataset_path / data_sample['image_path']).pixel_array[None, :, :])\n",
    "            image = self.pil_transform(image)\n",
    "\n",
    "            # maybe normalize?\n",
    "            # xrv.datasets.normalize(image,maxval=np.max(image))\n",
    "\n",
    "            return torch.from_numpy(report).to(self.device), torch.from_numpy(image).to(self.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from flamingo_pytorch import FlamingoPaLM\n",
    "from transformers import BatchFeature, PreTrainedTokenizerFast, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlamingoModule(pl.LightningModule):\n",
    "    def __init__(self, image_encoder, target):\n",
    "        super().__init__()\n",
    "        self.image_encoder = image_encoder\n",
    "        self.model = FlamingoPaLM(\n",
    "                        num_tokens = 20000,          # number of tokens\n",
    "                        dim = 18,                    # dimensions\n",
    "                        depth = 12,                  # depth\n",
    "                        heads = 8,                   # attention heads\n",
    "                        dim_head = 64,               # dimension per attention head\n",
    "                        img_encoder = self.image_encoder,           # plugin your image encoder (this can be optional if you pass in the image embeddings separately, but probably want to train end to end given the perceiver resampler)\n",
    "                        media_token_id = 3,          # the token id representing the [media] or [image]\n",
    "                        cross_attn_every = 3,        # how often to cross attend\n",
    "                        perceiver_num_latents = 16,  # perceiver number of latents, should be smaller than the sequence length of the image tokens\n",
    "                        perceiver_depth = 2          # perceiver resampler depth\n",
    "                    )\n",
    "\n",
    "        # TODO DEFINE LOSS cross entropy? \n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "        self.target = target\n",
    "\n",
    "        self.train_preds = []\n",
    "        self.train_gts = []\n",
    "        self.val_preds = []\n",
    "        self.val_gts = []\n",
    "        self.test_preds = []\n",
    "        self.test_gts = []\n",
    "        self.reset_metrics()\n",
    "\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        self.test_loss = []\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        print(f'batch: {batch.shape}')\n",
    "\n",
    "        model_output = None\n",
    "        if self.target == 'palm':\n",
    "            logits = self.model(\n",
    "                text = batch.tokens.input_ids,\n",
    "            )\n",
    "        elif self.target == 'flamingo':\n",
    "            logits = self.model(\n",
    "                text = batch.tokens.input_ids,\n",
    "                images = batch.images\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        print(f'model output: {logits.shape}')\n",
    "        loss = self.loss(logits.squeeze(), y)\n",
    "        self.log('train/loss', loss, on_step=False, on_epoch=True)\n",
    "        return {'loss': loss, 'ACC': self.calculate_accuracy(logits).detach().cpu()}\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # validation_step defines the validation loop.\n",
    "\n",
    "        if self.target == 'palm':\n",
    "            model_output = self.model(\n",
    "                text = batch.tokens.input_ids,\n",
    "            )\n",
    "        elif self.target == 'flamingo':\n",
    "            model_output = self.model(\n",
    "                text = batch.tokens.input_ids,\n",
    "                images = batch.images\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        loss = self.loss(y_hat.squeeze(), y)\n",
    "        self.update_metrics(y, y_hat, split='val')\n",
    "        self.val_loss.append(loss.item())\n",
    "        return {'val_loss': loss}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # test_step defines the test loop.\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = self.loss(y_hat.squeeze(), y)\n",
    "        self.update_metrics(y, y_hat, split='test')\n",
    "        self.test_loss.append(loss.item())\n",
    "        return {'test_loss': loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "    def reset_metrics(self, split=None):\n",
    "        if split == 'train':\n",
    "            self.train_preds = []\n",
    "            self.train_gts = []\n",
    "        elif split == 'val':\n",
    "            self.val_preds = []\n",
    "            self.val_gts = []\n",
    "        elif split == 'test':\n",
    "            self.test_preds = []\n",
    "            self.test_gts = []\n",
    "        else:\n",
    "            self.train_preds = []\n",
    "            self.train_gts = []\n",
    "            self.val_preds = []\n",
    "            self.val_gts = []\n",
    "            self.test_preds = []\n",
    "            self.test_gts = []\n",
    "\n",
    "    def update_metrics(self, gt, pred, split='train'):\n",
    "        if split == 'train':\n",
    "            self.train_preds.extend(pred.detach().cpu().numpy().argmax(1))\n",
    "            self.train_gts.extend(gt.detach().cpu().numpy())\n",
    "        elif split == 'val':\n",
    "            self.val_preds.extend(pred.detach().cpu().numpy().argmax(1))\n",
    "            self.val_gts.extend(gt.detach().cpu().numpy())\n",
    "        elif split == 'test':\n",
    "            self.test_preds.extend(pred.detach().cpu().numpy().argmax(1))\n",
    "            self.test_gts.extend(gt.detach().cpu().numpy())\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        self.evaluate_predictions(split='train')\n",
    "        self.reset_metrics(split='train')\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        self.evaluate_predictions(split='val')\n",
    "        self.reset_metrics(split='val')\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        self.evaluate_predictions(split='test')\n",
    "        self.reset_metrics(split='test')\n",
    "\n",
    "    def evaluate_predictions(self, split):\n",
    "        if split == 'train':\n",
    "            preds = self.train_preds\n",
    "            gts = self.train_gts\n",
    "        elif split == 'val':\n",
    "            preds = self.val_preds\n",
    "            gts = self.val_gts\n",
    "        elif split == 'test':\n",
    "            preds = self.test_preds\n",
    "            gts = self.test_gts\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        cls_report = classification_report(gts, preds)\n",
    "        print(split)\n",
    "        print(cls_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Training Setup For PaLM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_transform = transforms.Compose([xrv.datasets.XRayCenterCrop(),\n",
    "                                    xrv.datasets.XRayResizer(224),])\n",
    "train_dataset = MimicDatset(target='palm', split='train', pil_transform=pil_transform)\n",
    "val_dataset = MimicDatset(target='palm', split='val', pil_transform=pil_transform)\n",
    "test_dataset = MimicDatset(target='palm', split='test', pil_transform=pil_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, pin_memory=True, num_workers=4)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False, pin_memory=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False, pin_memory=True, num_workers=4)\n",
    "\n",
    "image_encoder = xrv.models.DenseNet(weights=\"densenet121-res224-mimic_nb\")\n",
    "model = FlamingoModule(image_encoder, target='palm')                            # maybe change target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train PaLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /home/andrei/TUM/SoSe2022/MLMI/emic-vqa/notebooks/playground/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | image_encoder | DenseNet         | 7.0 M \n",
      "1 | model         | FlamingoPaLM     | 7.9 M \n",
      "2 | loss          | CrossEntropyLoss | 0     \n",
      "---------------------------------------------------\n",
      "893 K     Trainable params\n",
      "7.0 M     Non-trainable params\n",
      "7.9 M     Total params\n",
      "31.438    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/32 [00:00<?, ?it/s] "
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_10782/2541399743.py\", line 41, in __getitem__\n    with open(self.dataset_path / data_sample['report_path'], 'r') as f:\nFileNotFoundError: [Errno 2] No such file or directory: '/mnt/209C31C29C3192F0/Datasets/Mimic-CXR/physionet.org/files/mimic-cxr/2.0.0/files/p16/p16669959/s57012984.txt'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/andrei/TUM/SoSe2022/MLMI/emic-vqa/notebooks/playground/train_flamingo.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/andrei/TUM/SoSe2022/MLMI/emic-vqa/notebooks/playground/train_flamingo.ipynb#ch0000011?line=0'>1</a>\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(gpus\u001b[39m=\u001b[39m[\u001b[39m1\u001b[39m], max_epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, num_sanity_val_steps\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)                    \n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/andrei/TUM/SoSe2022/MLMI/emic-vqa/notebooks/playground/train_flamingo.ipynb#ch0000011?line=1'>2</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model\u001b[39m=\u001b[39;49mmodel, train_dataloaders\u001b[39m=\u001b[39;49mtrain_loader, val_dataloaders\u001b[39m=\u001b[39;49mval_loader)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:770\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=750'>751</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=751'>752</a>\u001b[0m \u001b[39mRuns the full optimization routine.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=752'>753</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=766'>767</a>\u001b[0m \u001b[39m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=767'>768</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=768'>769</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n\u001b[0;32m--> <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=769'>770</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=770'>771</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=771'>772</a>\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:723\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=720'>721</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=721'>722</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=722'>723</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=723'>724</a>\u001b[0m \u001b[39m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=724'>725</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m~/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:811\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=806'>807</a>\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=807'>808</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_ckpt_path(\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=808'>809</a>\u001b[0m     ckpt_path, model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=809'>810</a>\u001b[0m )\n\u001b[0;32m--> <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=810'>811</a>\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=812'>813</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=813'>814</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1236\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1231'>1232</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1233'>1234</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1235'>1236</a>\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1237'>1238</a>\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1238'>1239</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m~/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1323\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1320'>1321</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1321'>1322</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1322'>1323</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m~/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1353\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1350'>1351</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[1;32m   <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1351'>1352</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1352'>1353</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/loops/base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/loops/base.py?line=201'>202</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/loops/base.py?line=202'>203</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/loops/base.py?line=203'>204</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/loops/base.py?line=204'>205</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/loops/base.py?line=205'>206</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:269\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py?line=264'>265</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher\u001b[39m.\u001b[39msetup(\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py?line=265'>266</a>\u001b[0m     dataloader, batch_to_device\u001b[39m=\u001b[39mpartial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_call_strategy_hook, \u001b[39m\"\u001b[39m\u001b[39mbatch_to_device\u001b[39m\u001b[39m\"\u001b[39m, dataloader_idx\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py?line=266'>267</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py?line=267'>268</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py?line=268'>269</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/loops/base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/loops/base.py?line=201'>202</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/loops/base.py?line=202'>203</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/loops/base.py?line=203'>204</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/loops/base.py?line=204'>205</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/loops/base.py?line=205'>206</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:171\u001b[0m, in \u001b[0;36mTrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=168'>169</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data_fetcher, DataLoaderIterDataFetcher):\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=169'>170</a>\u001b[0m     batch_idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_idx \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=170'>171</a>\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(data_fetcher)\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=171'>172</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=172'>173</a>\u001b[0m     batch_idx, batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(data_fetcher)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/utilities/fetching.py:184\u001b[0m, in \u001b[0;36mAbstractDataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/utilities/fetching.py?line=182'>183</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m--> <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/utilities/fetching.py?line=183'>184</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfetching_function()\n",
      "File \u001b[0;32m~/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/utilities/fetching.py:259\u001b[0m, in \u001b[0;36mDataFetcher.fetching_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/utilities/fetching.py?line=255'>256</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone:\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/utilities/fetching.py?line=256'>257</a>\u001b[0m     \u001b[39m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/utilities/fetching.py?line=257'>258</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/utilities/fetching.py?line=258'>259</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fetch_next_batch(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataloader_iter)\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/utilities/fetching.py?line=259'>260</a>\u001b[0m         \u001b[39m# consume the batch we just fetched\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/utilities/fetching.py?line=260'>261</a>\u001b[0m         batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatches\u001b[39m.\u001b[39mpop(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/utilities/fetching.py:273\u001b[0m, in \u001b[0;36mDataFetcher._fetch_next_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/utilities/fetching.py?line=270'>271</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fetch_next_batch\u001b[39m(\u001b[39mself\u001b[39m, iterator: Iterator) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/utilities/fetching.py?line=271'>272</a>\u001b[0m     start_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_fetch_start()\n\u001b[0;32m--> <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/utilities/fetching.py?line=272'>273</a>\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(iterator)\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/utilities/fetching.py?line=273'>274</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfetched \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/utilities/fetching.py?line=274'>275</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprefetch_batches \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_len:\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/utilities/fetching.py?line=275'>276</a>\u001b[0m         \u001b[39m# when we don't prefetch but the dataloader is sized, we use the length for `done`\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/supporters.py:553\u001b[0m, in \u001b[0;36mCombinedLoaderIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/supporters.py?line=546'>547</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/supporters.py?line=547'>548</a>\u001b[0m     \u001b[39m\"\"\"Fetches the next batch from multiple data loaders.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/supporters.py?line=548'>549</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/supporters.py?line=549'>550</a>\u001b[0m \u001b[39m    Returns:\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/supporters.py?line=550'>551</a>\u001b[0m \u001b[39m        a collections of batch data\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/supporters.py?line=551'>552</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/supporters.py?line=552'>553</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_next_batch(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloader_iters)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/supporters.py:565\u001b[0m, in \u001b[0;36mCombinedLoaderIterator.request_next_batch\u001b[0;34m(loader_iters)\u001b[0m\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/supporters.py?line=554'>555</a>\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/supporters.py?line=555'>556</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest_next_batch\u001b[39m(loader_iters: Union[Iterator, Sequence, Mapping]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/supporters.py?line=556'>557</a>\u001b[0m     \u001b[39m\"\"\"Return the batch of data from multiple iterators.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/supporters.py?line=557'>558</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/supporters.py?line=558'>559</a>\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/supporters.py?line=562'>563</a>\u001b[0m \u001b[39m        Any: a collections of batch data\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/supporters.py?line=563'>564</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/trainer/supporters.py?line=564'>565</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m apply_to_collection(loader_iters, Iterator, \u001b[39mnext\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/utilities/apply_func.py:99\u001b[0m, in \u001b[0;36mapply_to_collection\u001b[0;34m(data, dtype, function, wrong_dtype, include_none, *args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/utilities/apply_func.py?line=96'>97</a>\u001b[0m \u001b[39m# Breaking condition\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/utilities/apply_func.py?line=97'>98</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, dtype) \u001b[39mand\u001b[39;00m (wrong_dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data, wrong_dtype)):\n\u001b[0;32m---> <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/utilities/apply_func.py?line=98'>99</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m function(data, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/utilities/apply_func.py?line=100'>101</a>\u001b[0m elem_type \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(data)\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/pytorch_lightning/utilities/apply_func.py?line=102'>103</a>\u001b[0m \u001b[39m# Recursively apply to collection items\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlmi/lib/python3.10/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=527'>528</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=528'>529</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=529'>530</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=530'>531</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=531'>532</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=532'>533</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=533'>534</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/mlmi/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1224\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=1221'>1222</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=1222'>1223</a>\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=1223'>1224</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlmi/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1250\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=1247'>1248</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=1248'>1249</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=1249'>1250</a>\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=1250'>1251</a>\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/mlmi/lib/python3.10/site-packages/torch/_utils.py:457\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/torch/_utils.py?line=452'>453</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/torch/_utils.py?line=453'>454</a>\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/torch/_utils.py?line=454'>455</a>\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/torch/_utils.py?line=455'>456</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> <a href='file:///home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/torch/_utils.py?line=456'>457</a>\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_10782/2541399743.py\", line 41, in __getitem__\n    with open(self.dataset_path / data_sample['report_path'], 'r') as f:\nFileNotFoundError: [Errno 2] No such file or directory: '/mnt/209C31C29C3192F0/Datasets/Mimic-CXR/physionet.org/files/mimic-cxr/2.0.0/files/p16/p16669959/s57012984.txt'\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(gpus=[1], max_epochs=10, num_sanity_val_steps=0)                    \n",
    "trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SegSmall_betterDataSet_r3000_ConvPoint_SearchQuantized_betterDataset_Adam_StepLR_CrossEntropy_Classification'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = '/wholebrain/scratch/amancu/mergeError/Nodes/Trainings/SegSmall_betterDataSet_r3000_ConvPoint_SearchQuantized_betterDataset_Adam_StepLR_CrossEntropy_Classification/state_dict.pth'\n",
    "import os\n",
    "os.path.basename(os.path.dirname(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Training Setup For Flamingo + PaLM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_transform = transforms.Compose([xrv.datasets.XRayCenterCrop(),\n",
    "                                    xrv.datasets.XRayResizer(224),])\n",
    "                                    \n",
    "train_dataset = MimicDatset(target='flamingo', split='train', pil_transform=pil_transform)\n",
    "val_dataset = MimicDatset(target='flamingo', split='val', pil_transform=pil_transform)\n",
    "test_dataset = MimicDatset(target='flamingo', split='test', pil_transform=pil_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, pin_memory=True, num_workers=4)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False, pin_memory=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False, pin_memory=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model, dataloaders=test_loader)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "30ec110bc924a9e139919a87e1ff85100b6c769b1dd45b8d281b6aff673f8e03"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('mlmi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
