{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import torchxrayvision as xrv\n",
    "import numpy as np\n",
    "import pydicom as dicom\n",
    "import pickle as pkl\n",
    "\n",
    "from torch import nn\n",
    "from typing import Optional\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom as dicom\n",
    "from transformers import BatchFeature, PreTrainedTokenizerFast\n",
    "import mpu.ml\n",
    "\n",
    "class MimicDatset(Dataset):\n",
    "    def __init__(self, target:str = 'palm', split:str = 'train', pil_transform: Optional[transforms.Compose] = None, tensor_transform: Optional[transforms.Compose] = None):\n",
    "        self.target = target\n",
    "        assert target in ['palm', 'flamingo']\n",
    "\n",
    "        self.split = split\n",
    "        assert split in ['train', 'val', 'test']\n",
    "\n",
    "        self.pil_transform = pil_transform\n",
    "        self.tensor_transform = tensor_transform\n",
    "        \n",
    "        self.dataset_path = Path('/mnt/209C31C29C3192F0/Datasets/Mimic-CXR/physionet.org/files/mimic-cxr/2.0.0/')\n",
    "\n",
    "        with open(self.dataset_path / 'images2reports.pkl', 'rb') as f:\n",
    "            data_list_pkl = pkl.load(f)\n",
    "\n",
    "        self.data = data_list_pkl\n",
    "        del data_list_pkl\n",
    "        self.dataset_length = len(self.data)\n",
    "\n",
    "        self.tokenizer = PreTrainedTokenizerFast(tokenizer_file=str(self.dataset_path / 'tokenizer_mimic.json'), pad_token='[PAD]')\n",
    "\n",
    "        # max length of text tokens\n",
    "        self.max_length = 512\n",
    "        \n",
    "    def __len__(self):                  # TODO figure this out, for now only limited data, TODO dynamic loading\n",
    "        \n",
    "        if self.split == \"palm\":\n",
    "            match self.split:\n",
    "                case 'train':\n",
    "                    return int(0.9 * self.dataset_length)\n",
    "                case 'val':\n",
    "                    return int(0.1 * self.dataset_length)\n",
    "                case 'test':\n",
    "                    return int(0.2 * self.dataset_length)\n",
    "        else:\n",
    "            match self.split:\n",
    "                case 'train':\n",
    "                    return int(0.8 * self.dataset_length)\n",
    "                case 'val':\n",
    "                    return int(0.2 * self.dataset_length)\n",
    "                case 'test':\n",
    "                    return int(0.2 * self.dataset_length)\n",
    "        # return 10\n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # don't forget about the target\n",
    "\n",
    "        # get item and convert text into tokens\n",
    "        # data_sample = self.data[index]\n",
    "\n",
    "        # because I don't have the entire dataset on my computer, find a sample to load without read error \n",
    "        while True:\n",
    "            try:\n",
    "                data_sample = self.data[index]\n",
    "                with open(self.dataset_path / data_sample['report_path'], 'r') as f:\n",
    "                    report_text = f.readlines()\n",
    "                    \n",
    "                # TODO make this faster with batch_encode_plus()\n",
    "                report_tokenized = self.tokenizer.encode_plus(report_text[0], padding=True, truncation=True, max_length=self.max_length)\n",
    "                batch_ids = report_tokenized['input_ids']\n",
    "\n",
    "                # manually pad for batch stack\n",
    "                batch_ids = [batch_ids + [self.tokenizer.pad_token_id] * (self.max_length - len(batch_ids))]\n",
    "                # print(f'{batch_ids}')\n",
    "                batch_one_hot = mpu.ml.indices2one_hot(batch_ids[0], nb_classes=15185)\n",
    "                image = np.array(dicom.dcmread(self.dataset_path / data_sample['image_path']).pixel_array[None, :, :])\n",
    "                break\n",
    "            except:\n",
    "                index = np.random.randint(0,self.dataset_length)\n",
    "\n",
    "\n",
    "        batch_mask = report_tokenized['attention_mask']\n",
    "        batch_type_ids = report_tokenized['token_type_ids']\n",
    "        # print(f'report: {report_tokenized}')\n",
    "\n",
    "        if self.target == 'palm':\n",
    "            return torch.LongTensor(np.array(batch_ids)).squeeze(), torch.FloatTensor(np.array(batch_one_hot)).squeeze()\n",
    "        \n",
    "        else:\n",
    "            # get corresponding images\n",
    "            image = self.pil_transform(image)\n",
    "\n",
    "            # maybe normalize?\n",
    "            xrv.datasets.normalize(image, maxval=np.max(image))\n",
    "\n",
    "            # add temporal channel to images\n",
    "            return torch.LongTensor(np.array(batch_ids)).squeeze(), torch.from_numpy(image[None,:,:,:]), torch.FloatTensor(np.array(batch_one_hot)).squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from flamingo_pytorch import FlamingoPaLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class FlamingoModule(pl.LightningModule):\n",
    "    def __init__(self, image_encoder, target):\n",
    "        super().__init__()\n",
    "        self.image_encoder = image_encoder\n",
    "        self.model = FlamingoPaLM(\n",
    "                        num_tokens = 15185,          # number of tokens\n",
    "                        dim = 18,                  # dimensions\n",
    "                        depth = 12,                  # depth\n",
    "                        heads = 8,                   # attention heads\n",
    "                        dim_head = 64,               # dimension per attention head\n",
    "                        img_encoder = image_encoder, # plugin your image encoder (this can be optional if you pass in the image embeddings separately, but probably want to train end to end given the perceiver resampler)\n",
    "                        media_token_id = 3,          # the token id representing the [media] or [image]\n",
    "                        cross_attn_every = 3,        # how often to cross attend\n",
    "                        perceiver_num_latents = 16,  # perceiver number of latents, should be smaller than the sequence length of the image tokens\n",
    "                        perceiver_depth = 2          # perceiver resampler depth\n",
    "                    )\n",
    "\n",
    "        # TODO DEFINE LOSS cross entropy?\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "        # elif:\n",
    "            # self.loss =\n",
    "        \n",
    "        self.target = target\n",
    "\n",
    "        self.train_preds = []\n",
    "        self.train_gts = []\n",
    "        self.val_preds = []\n",
    "        self.val_gts = []\n",
    "        self.test_preds = []\n",
    "        self.test_gts = []\n",
    "        self.reset_metrics()\n",
    "\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        self.test_loss = []\n",
    "\n",
    "        self.writer = SummaryWriter()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        if self.target == 'palm':\n",
    "            text, y = batch\n",
    "            logits = self.model(\n",
    "                text = text,\n",
    "            )\n",
    "        elif self.target == 'flamingo':\n",
    "            text, image, y = batch\n",
    "            logits = self.model(\n",
    "                text = text,\n",
    "                images = image\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        loss = self.loss(logits.float(), y)\n",
    "        self.log('train/loss', loss, on_step=False, on_epoch=True)\n",
    "        return {'loss': loss}\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # validation_step defines the validation loop.\n",
    "\n",
    "        if self.target == 'palm':\n",
    "            text, y = batch\n",
    "            logits = self.model(\n",
    "                text = text,\n",
    "            )\n",
    "        elif self.target == 'flamingo':\n",
    "            text, image, y = batch\n",
    "            logits = self.model(\n",
    "                text = text,\n",
    "                images = image\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        loss = self.loss(logits.squeeze(), y)\n",
    "        self.update_metrics(text, logits, split='val')\n",
    "        self.val_loss.append(loss.item())\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # test_step defines the test loop.\n",
    "        if self.target == 'palm':\n",
    "            text, y = batch\n",
    "            logits = self.model(\n",
    "                text = text,\n",
    "            )\n",
    "        elif self.target == 'flamingo':\n",
    "            text, image, y = batch\n",
    "            logits = self.model(\n",
    "                text = text,\n",
    "                images = image\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        loss = self.loss(logits.squeeze(), y)\n",
    "        self.update_metrics(text, logits, split='test')\n",
    "        self.test_loss.append(loss.item())\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "    def reset_metrics(self, split=None):\n",
    "        if split == 'train':\n",
    "            self.train_preds = []\n",
    "            self.train_gts = []\n",
    "        elif split == 'val':\n",
    "            self.val_preds = []\n",
    "            self.val_gts = []\n",
    "        elif split == 'test':\n",
    "            self.test_preds = []\n",
    "            self.test_gts = []\n",
    "        else:\n",
    "            self.train_preds = []\n",
    "            self.train_gts = []\n",
    "            self.val_preds = []\n",
    "            self.val_gts = []\n",
    "            self.test_preds = []\n",
    "            self.test_gts = []\n",
    "\n",
    "    def update_metrics(self, gt, pred, split='train'):\n",
    "        if split == 'train':\n",
    "            self.train_preds.extend(pred.detach().cpu().numpy().argmax(1))\n",
    "            self.train_gts.extend(gt.detach().cpu().numpy())\n",
    "        elif split == 'val':\n",
    "            self.val_preds.extend(pred.detach().cpu().numpy().argmax(1))\n",
    "            self.val_gts.extend(gt.detach().cpu().numpy())\n",
    "        elif split == 'test':\n",
    "            self.test_preds.extend(pred.detach().cpu().numpy().argmax(1))\n",
    "            self.test_gts.extend(gt.detach().cpu().numpy())\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    def training_epoch_end(self, output):\n",
    "        loss = 0\n",
    "        for o in output:\n",
    "            loss +=  o['loss'].item()\n",
    "        loss = loss / len(output)\n",
    "        self.writer.add_scalar('Epoch_loss/train', loss, self.current_epoch)\n",
    "        self.reset_metrics(split='train')\n",
    "\n",
    "    def validation_epoch_end(self, output):\n",
    "        loss = 0\n",
    "        for o in output:\n",
    "            loss +=  o['loss'].item()\n",
    "        loss = loss / len(output)\n",
    "        self.log('val_loss', loss)\n",
    "        self.writer.add_scalar('Epoch_loss/validation', loss, self.current_epoch)\n",
    "        self.reset_metrics(split='val')\n",
    "    \n",
    "    def test_epoch_end(self, output):\n",
    "        loss = 0\n",
    "        for o in output:\n",
    "            loss +=  o['loss'].item()\n",
    "        loss = loss / len(output)\n",
    "        self.writer.add_scalar('Epoch_loss/test', loss, self.current_epoch)\n",
    "        self.reset_metrics(split='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Training Setup For PaLM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_transform = transforms.Compose([xrv.datasets.XRayCenterCrop(),\n",
    "                                    xrv.datasets.XRayResizer(224),])\n",
    "train_dataset = MimicDatset(target='palm', split='train', pil_transform=pil_transform)\n",
    "val_dataset = MimicDatset(target='palm', split='val', pil_transform=pil_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, pin_memory=True, num_workers=4)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False, pin_memory=True, num_workers=4)\n",
    "\n",
    "image_encoder = xrv.models.DenseNet(weights=\"densenet121-res224-mimic_nb\")\n",
    "model = FlamingoModule(image_encoder, target='palm')                            # maybe change target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train PaLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | image_encoder | DenseNet         | 7.0 M \n",
      "1 | model         | FlamingoPaLM     | 7.8 M \n",
      "2 | loss          | CrossEntropyLoss | 0     \n",
      "---------------------------------------------------\n",
      "806 K     Trainable params\n",
      "7.0 M     Non-trainable params\n",
      "7.8 M     Total params\n",
      "31.091    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:  45%|████▌     | 520/1147 [30:30<36:47,  3.52s/it, loss=0.186, v_num=4]   "
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(gpus=[1], max_epochs=100, num_sanity_val_steps=0) \n",
    "trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values = model.val_loss\n",
    "plt.plot(loss_values)\n",
    "plt.title(\"Validation loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save language model\n",
    "torch.save(model.state_dict(), 'PaLM_mimic_BPETokenizer_bs32_Adam.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Training Setup For Flamingo + PaLM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_transform = transforms.Compose([xrv.datasets.XRayCenterCrop(),\n",
    "                                    xrv.datasets.XRayResizer(224),])\n",
    "                                    \n",
    "train_dataset = MimicDatset(target='flamingo', split='train', pil_transform=pil_transform)\n",
    "val_dataset = MimicDatset(target='flamingo', split='val', pil_transform=pil_transform)\n",
    "test_dataset = MimicDatset(target='flamingo', split='test', pil_transform=pil_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, pin_memory=True, num_workers=4)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False, pin_memory=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, pin_memory=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_encoder = xrv.models.DenseNet(weights=\"densenet121-res224-mimic_nb\")\n",
    "model = FlamingoModule(image_encoder, target='flamingo')                            # maybe change target\n",
    "model.load_state_dict(torch.load('/home/andrei/mlmi/home/mlmi-matthias/Andrei/mlmi-vqa/models/PaLM_mimic_BPETokenizer_unfiltered_bs32_Adam.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /home/andrei/mlmi/home/mlmi-matthias/Andrei/mlmi-vqa/src/models/multimodal/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | image_encoder | DenseNet         | 7.0 M \n",
      "1 | model         | FlamingoPaLM     | 7.8 M \n",
      "2 | loss          | CrossEntropyLoss | 0     \n",
      "---------------------------------------------------\n",
      "806 K     Trainable params\n",
      "7.0 M     Non-trainable params\n",
      "7.8 M     Total params\n",
      "31.091    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/1147 [00:00<?, ?it/s] Warning: Input image does not appear to be normalized correctly. The input image has the range [0.00,8857.02] which doesn't seem to be in the [-1024,1024] range. This warning may be wrong though. Only the first image is tested and we are only using a heuristic in an attempt to save a user from using the wrong normalization.\n",
      "Epoch 0:   4%|▍         | 48/1147 [06:20<2:25:13,  7.93s/it, loss=0.186, v_num=0]"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(gpus=[1], max_epochs=100, num_sanity_val_steps=0) \n",
    "trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model, dataloaders=test_loader)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "30ec110bc924a9e139919a87e1ff85100b6c769b1dd45b8d281b6aff673f8e03"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
