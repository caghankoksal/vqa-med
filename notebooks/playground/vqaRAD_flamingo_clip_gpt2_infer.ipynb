{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrei/anaconda3/envs/mlmi/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "import torch\n",
    "sys.path.append('../..')\n",
    "\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "from src.datasets.vqa_rad_dataset import VQRadDataModule\n",
    "from src.models.multimodal.flamingo_module import FlamingoModule\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from pytorch_lightning.loggers import CometLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "seed_everything(42, workers=True)\n",
    "\n",
    "img_mean = (0.48,0.48,0.48)\n",
    "img_std = (0.265,0.265,0.265)\n",
    "\n",
    "transforms = {'train':\n",
    "    T.Compose(\n",
    "    [\n",
    "        # T.RandomRotation(10),\n",
    "        T.ToTensor(),\n",
    "        # T.Normalize(mean=img_mean, std=img_std)\n",
    "    ]),\n",
    "    'val':\n",
    "    T.Compose(\n",
    "    [\n",
    "        # T.RandomRotation(10),\n",
    "        T.ToTensor(),\n",
    "        # T.Normalize(mean=img_mean, std=img_std)\n",
    "    ]),\n",
    "    'test':\n",
    "    T.Compose(\n",
    "    [\n",
    "        T.ToTensor(),\n",
    "        # T.Normalize(mean=img_mean, std=img_std)\n",
    "    ])\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 794 QA pairs in VQA-RAD dataset\n",
      "Dataset root: /home/andrei/mlmi/home/mlmi-matthias/Data/VQA_RAD_preprocessed/\n",
      "Loading all images into memory... for train\n",
      "Found 107 images\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'answers_list_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/andrei/mlmi/home/mlmi-matthias/Andrei/mlmi-vqa/notebooks/playground/vqaRAD_flamingo_clip_gpt2_infer.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrei/mlmi/home/mlmi-matthias/Andrei/mlmi-vqa/notebooks/playground/vqaRAD_flamingo_clip_gpt2_infer.ipynb#ch0000001?line=18'>19</a>\u001b[0m LOAD_IN_MEM \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrei/mlmi/home/mlmi-matthias/Andrei/mlmi-vqa/notebooks/playground/vqaRAD_flamingo_clip_gpt2_infer.ipynb#ch0000001?line=19'>20</a>\u001b[0m PREPROCESSED \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/andrei/mlmi/home/mlmi-matthias/Andrei/mlmi-vqa/notebooks/playground/vqaRAD_flamingo_clip_gpt2_infer.ipynb#ch0000001?line=21'>22</a>\u001b[0m vqarad_datamodule \u001b[39m=\u001b[39m VQRadDataModule(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrei/mlmi/home/mlmi-matthias/Andrei/mlmi-vqa/notebooks/playground/vqaRAD_flamingo_clip_gpt2_infer.ipynb#ch0000001?line=22'>23</a>\u001b[0m                                 batch_size\u001b[39m=\u001b[39;49mBATCH_SIZE, transforms\u001b[39m=\u001b[39;49mtransforms, root\u001b[39m=\u001b[39;49mDATASET_ROOT,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrei/mlmi/home/mlmi-matthias/Andrei/mlmi-vqa/notebooks/playground/vqaRAD_flamingo_clip_gpt2_infer.ipynb#ch0000001?line=23'>24</a>\u001b[0m                                 limit_num_samples\u001b[39m=\u001b[39;49mLIMIT_NUM_SAMPLES, num_workers\u001b[39m=\u001b[39;49mNUM_DATA_WORKERS, shuffle\u001b[39m=\u001b[39;49mSHUFFLE,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrei/mlmi/home/mlmi-matthias/Andrei/mlmi-vqa/notebooks/playground/vqaRAD_flamingo_clip_gpt2_infer.ipynb#ch0000001?line=24'>25</a>\u001b[0m                                 tokenizer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt2\u001b[39;49m\u001b[39m\"\u001b[39;49m, preprocessed\u001b[39m=\u001b[39;49mPREPROCESSED, load_in_memory\u001b[39m=\u001b[39;49mLOAD_IN_MEM\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrei/mlmi/home/mlmi-matthias/Andrei/mlmi-vqa/notebooks/playground/vqaRAD_flamingo_clip_gpt2_infer.ipynb#ch0000001?line=25'>26</a>\u001b[0m )\n",
      "File \u001b[0;32m~/mlmi/home/mlmi-matthias/Andrei/mlmi-vqa/notebooks/playground/../../src/datasets/vqa_rad_dataset.py:202\u001b[0m, in \u001b[0;36mVQRadDataModule.__init__\u001b[0;34m(self, batch_size, transforms, root, limit_num_samples, num_workers, shuffle, tokenizer, preprocessed, load_in_memory)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_split \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_split[:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlimit_num_samples]\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_split \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_split[:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlimit_num_samples]\n\u001b[0;32m--> 202\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_dataset \u001b[39m=\u001b[39m VQARadDataset(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot, \u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_split, transform\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransforms[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    203\u001b[0m                             tokenizer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer, preprocessed \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpreprocessed, load_in_memory \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_in_memory)\n\u001b[1;32m    205\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidation_dataset \u001b[39m=\u001b[39m VQARadDataset(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot, \u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_split, transform\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms[\u001b[39m\"\u001b[39m\u001b[39mval\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    206\u001b[0m                             tokenizer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer, preprocessed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocessed, load_in_memory \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_in_memory)\n\u001b[1;32m    208\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_dataset \u001b[39m=\u001b[39m VQARadDataset(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot, \u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_split, transform\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms[\u001b[39m\"\u001b[39m\u001b[39mval\u001b[39m\u001b[39m\"\u001b[39m], \n\u001b[1;32m    209\u001b[0m                             tokenizer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer, preprocessed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocessed, load_in_memory \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_in_memory)\n",
      "File \u001b[0;32m~/mlmi/home/mlmi-matthias/Andrei/mlmi-vqa/notebooks/playground/../../src/datasets/vqa_rad_dataset.py:81\u001b[0m, in \u001b[0;36mVQARadDataset.__init__\u001b[0;34m(self, root, mode, samples, transform, tokenizer, question_tokenize, answer_tokenize, tokenizer_add_special_tokens, token_max_length, return_pil, preprocessed, load_in_memory)\u001b[0m\n\u001b[1;32m     78\u001b[0m     num_added_toks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39madd_special_tokens(special_tokens_dict)\n\u001b[1;32m     80\u001b[0m \u001b[39m#Â Create answers dictionary\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(answers_list_path, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     82\u001b[0m     lines \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mreadlines()\n\u001b[1;32m     83\u001b[0m     lines \u001b[39m=\u001b[39m [each\u001b[39m.\u001b[39mstrip() \u001b[39mfor\u001b[39;00m each \u001b[39min\u001b[39;00m lines]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'answers_list_path' is not defined"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "NUM_DATA_WORKERS  = 8\n",
    "ONLY_IMAGES = False\n",
    "BATCH_SIZE = 1\n",
    "NUM_EPOCHS = 10\n",
    "LIMIT_NUM_SAMPLES = None\n",
    "\n",
    "ACCELERATOR = \"gpu\"\n",
    "DEVICES = [4]\n",
    "# ACCELERATOR = \"cpu\"\n",
    "# DEVICES = 1\n",
    "DATASET_ROOT = '/home/andrei/mlmi/home/mlmi-matthias/Data/VQA_RAD_preprocessed/'\n",
    "PRETRAINED_CLIP_PATH = '/home/andrei/mlmi/home/mlmi-matthias/Caghan/pretrained_models/PubMedCLIP_ViT32.pth'\n",
    "PRETRAINED_GPT2_PATH = \"/home/andrei/mlmi/home/mlmi-matthias/Caghan/pretrained_models/gpt2-pytorch_model.bin\"\n",
    "ANSWERS_LIST_PATH = '/home/andrei/mlmi/home/mlmi-matthias/VQA-RAD/unique_answers.pkl'\n",
    "\n",
    "IMAGE_TYPE = \"jpg\"\n",
    "SHUFFLE = True\n",
    "TOKENIZER  = \"gpt2\"\n",
    "LOAD_IN_MEM = True\n",
    "PREPROCESSED = True\n",
    "\n",
    "vqarad_datamodule = VQRadDataModule(\n",
    "                                batch_size=BATCH_SIZE, transforms=transforms, root=DATASET_ROOT,\n",
    "                                limit_num_samples=LIMIT_NUM_SAMPLES, num_workers=NUM_DATA_WORKERS, shuffle=SHUFFLE,\n",
    "                                tokenizer=\"gpt2\", preprocessed=PREPROCESSED, load_in_memory=LOAD_IN_MEM, answers_list_path=ANSWERS_LIST_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = vqarad_datamodule.train_dataloader()\n",
    "val_loader = vqarad_datamodule.val_dataloader()\n",
    "test_loader = vqarad_datamodule.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â MODEL HPRAMS\n",
    "VOCAB_SIZE_OF_TOKENIZER = 50257 #Â mimic_datamodule.train_dataset.tokenizer.vocab_size\n",
    "LANGUAGE_MODEL = 'gpt2'\n",
    "NUM_TOKENS = VOCAB_SIZE_OF_TOKENIZER +3 if LANGUAGE_MODEL==\"gpt2\" else 31092\n",
    "FLAMINGO_EMBED_DIM = 768\n",
    "DEPTH = 12\n",
    "NUM_HEADS = 8\n",
    "ATT_HEAD_DIM = 64\n",
    "CROOS_ATT_EVERY=3\n",
    "MEDIA_TOKEN_ID = vqarad_datamodule.train_dataset.tokenizer.\\\n",
    "    all_special_ids[vqarad_datamodule.train_dataset.tokenizer.all_special_tokens.index('<image>')]\n",
    "PERCEIVER_NUM_LATENTS = 64\n",
    "PERCEIVER_DEPTH = 2\n",
    "IMAGE_ENCODER = \"clip\"\n",
    "\n",
    "\n",
    "\n",
    "print(\"LANGUAGE_MODEL : \",LANGUAGE_MODEL, \"\\n\"\n",
    "        \"NUM_TOKENS : \",NUM_TOKENS, \"\\n\"\n",
    "        \"FLAMINGO_EMBED_DIM : \",FLAMINGO_EMBED_DIM, \"\\n\"\n",
    "        \"DEPTH : \",DEPTH, \"\\n\"\n",
    "        \"NUM_HEADS : \",NUM_HEADS, \"\\n\"\n",
    "        \"ATT_HEAD_DIM : \",ATT_HEAD_DIM, \"\\n\"\n",
    "        \"CROOS_ATT_EVERY : \",CROOS_ATT_EVERY, \"\\n\"\n",
    "        \"MEDIA_TOKEN_ID : \",MEDIA_TOKEN_ID, \"\\n\"\n",
    "        \"PERCEIVER_NUM_LATENTS : \",PERCEIVER_NUM_LATENTS, \"\\n\"\n",
    "        \"PERCEIVER_DEPTH : \",PERCEIVER_DEPTH, \"\\n\"\n",
    "        \"IMAGE_ENCODER : \",IMAGE_ENCODER, \"\\n\"\n",
    "        \"PRETRAINED_CLIP_PATH : \",PRETRAINED_CLIP_PATH, \"\\n\"\n",
    "        \"PRETRAINED_GPT2_PATH : \",PRETRAINED_GPT2_PATH, \"\\n\")\n",
    "\n",
    "\n",
    "hyperparams = {\n",
    "    'pretrained_clip_path': PRETRAINED_CLIP_PATH,\n",
    "    'warmup_steps': 0,\n",
    "    'num_tokens': NUM_TOKENS,\n",
    "    'dim': FLAMINGO_EMBED_DIM,\n",
    "    'depth': DEPTH,\n",
    "    'num_heads': NUM_HEADS,\n",
    "    'dim_head': ATT_HEAD_DIM,\n",
    "    'cross_attn_every': CROOS_ATT_EVERY,\n",
    "    'media_token_id': MEDIA_TOKEN_ID,\n",
    "    'perceiver_num_latents': PERCEIVER_NUM_LATENTS,\n",
    "    'perceiver_depth': PERCEIVER_DEPTH,\n",
    "    'image_encoder': IMAGE_ENCODER,\n",
    "    'language_model': LANGUAGE_MODEL,\n",
    "    'pretrained_gpt2_path': PRETRAINED_GPT2_PATH,\n",
    "}\n",
    "\n",
    "\n",
    "model = FlamingoModule(**hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKPOINT_PATH = \"/home/andrei/mlmi/home/mlmi-matthias/Andrei/mlmi-vqa/notebooks/lightning_logs/flamingo_clip_GPT2_FT_vqaRAD_MIMICback_lr1e-4_Xray/checkpoints/epoch=5-val_loss=1.35-other_metric=0.00.ckpt\"\n",
    "# CHECKPOINT_PATH = \"/home/andrei/mlmi/home/mlmi-matthias/Andrei/mlmi-vqa/notebooks/lightning_logs/flamingo_clip_GPT2_FT_vqaRAD_MIMICback_all/checkpoints/epoch=59-val_loss=1.48-other_metric=0.00.ckpt\"\n",
    "CHECKPOINT_PATH = \"/home/andrei/mlmi/home/mlmi-matthias/Andrei/mlmi-vqa/notebooks/lightning_logs/flamingo_clip_GPT2_FT_vqaRAD_ROCOback_all/checkpoints/epoch=78-val_loss=1.53-other_metric=0.00.ckpt\"\n",
    "\n",
    "START_FROM_CHECKPOINT = True\n",
    "\n",
    "if START_FROM_CHECKPOINT:\n",
    "    print(\"Pretrained Flamingo Model is loaded from checkpoint : \",CHECKPOINT_PATH)\n",
    "    model.load_state_dict(torch.load(CHECKPOINT_PATH)[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from transformers import GPT2Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn as nn\n",
    "import torch.nn.functional as F\n",
    "def generate(image, context, cur_model, ntok=20):\n",
    "    for _ in range(ntok):\n",
    "        out = cur_model({'image': image,'input_ids': context })\n",
    "        logits = out[:, -1, :]\n",
    "        indices_to_remove = logits < torch.topk(logits, 10)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = np.NINF\n",
    "        next_tok = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1).squeeze(1)\n",
    "        context = torch.cat([context, next_tok.unsqueeze(-1)], dim=-1)\n",
    "    return context\n",
    "\n",
    "\n",
    "tokenizer = vqarad_datamodule.train_dataset.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader_iter = iter(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = next(val_loader_iter)\n",
    "val_img = batch[\"image\"]\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' answer:')]) \n",
    "out = generate(val_img, context, model, ntok=20)\n",
    "print(\"Model out : \",tokenizer.decode(out[0]))\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])\n",
    "plt.imshow(torch.swapaxes(val_img.squeeze(0),0,2), cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_img = batch[\"image\"]\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' answer:')])\n",
    "out = generate(val_img, context, model, ntok=20)\n",
    "print(\"Model out : \",tokenizer.decode(out[0]))\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])\n",
    "plt.imshow(torch.swapaxes(val_img.squeeze(0),0,2), cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader_iter = iter(val_loader)\n",
    "cos_similarity = torch.nn.CosineSimilarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import no_grad\n",
    "\n",
    "\n",
    "def generate_logits(image, pred_context, GT_context, GT_answer, model, ntok):\n",
    "\n",
    "    # get a prediction (whole answer)\n",
    "    pred_out_logits = None\n",
    "    pred_answer = torch.tensor([[]])\n",
    "    for i in range(ntok):\n",
    "        out = model({'image': image,'input_ids': pred_context})\n",
    "        logits = out[:, -1, :]\n",
    "        indices_to_remove = logits < torch.topk(logits, 10)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = np.NINF\n",
    "        next_tok = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1).squeeze(1)\n",
    "        pred_answer = torch.cat([pred_answer, next_tok.unsqueeze(-1)], dim=-1)\n",
    "        pred_context = torch.cat([pred_context, next_tok.unsqueeze(-1)], dim=-1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        # only get embeddings for answer and GT from the model\n",
    "        answer = tokenizer.decode(pred_answer[pred_answer!=50257].int())                # remove padding tokens, <EOC> token remains\n",
    "        # print(f'Ans: {answer}')\n",
    "        pred_answer = torch.tensor([tokenizer.encode(\"<|endoftext|> \" + answer)])\n",
    "        pred_out_logits = model.forward({'image': image,'input_ids': pred_answer}, return_embeds=True)\n",
    "        real_out_logits  = model.forward({'image': image,'input_ids': GT_answer}, return_embeds=True)\n",
    "\n",
    "    return pred_context, pred_out_logits, real_out_logits, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "val_img = batch[\"image\"]\n",
    "pred_context  = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+ batch[\"question\"][0] + ' answer:')]) \n",
    "GT_context  = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+ batch[\"question\"][0] + ' answer: ' + batch[\"answer\"][0] + '<EOC>')])\n",
    "GT_answer =  torch.tensor([tokenizer.encode(\"<|endoftext|> \" + batch[\"answer\"][0] + ' <EOC>')])\n",
    "out, pred_out_logits, real_out_logits, answer = generate_logits(val_img, pred_context, GT_context, GT_answer, model, ntok=20, batch=batch)\n",
    "\n",
    "print(\"Model out : \",tokenizer.decode(out[out != 50257]))\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])\n",
    "\n",
    "# print(f'Answer: {tokenizer.encode(answer[:-1])}')\n",
    "\n",
    "pred = torch.mean(pred_out_logits, dim=1)\n",
    "real = torch.mean(real_out_logits, dim=1)\n",
    "\n",
    "torch.nn.functional.cosine_similarity(pred, real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import umap.plot\n",
    "\n",
    "reducer = umap.UMAP()\n",
    "data = torch.cat([preds, reals], dim=0).detach().numpy()\n",
    "labels = torch.cat([torch.zeros(preds.shape[0]), torch.ones(reals.shape[0])])\n",
    "# data = torch.tensor(preds)\n",
    "print(f'data: {data.shape}')\n",
    "print(f'labels: {labels.shape}')\n",
    "mapper = reducer.fit(data)\n",
    "umap.plot.points(mapper, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do validation dataset eval\n",
    "val_loader_iter = iter(val_loader)\n",
    "n = len(val_loader)\n",
    "print(f'validation dataset has {n} samples')\n",
    "\n",
    "preds = torch.tensor([])\n",
    "reals = torch.tensor([])\n",
    "\n",
    "similarity_num = 0\n",
    "identical_num = 0\n",
    "\n",
    "for sample in val_loader_iter:\n",
    "    val_img = sample[\"image\"]\n",
    "    pred_context  = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+ sample[\"question\"][0] + ' answer:')]) \n",
    "    GT_context  = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+ sample[\"question\"][0] + ' answer: ' + sample[\"answer\"][0] + '<EOC>')])\n",
    "    GT_answer =  torch.tensor([tokenizer.encode(\"<|endoftext|> \" + sample[\"answer\"][0] + ' <EOC>')])\n",
    "    out, pred_out_logits, real_out_logits, answer = generate_logits(val_img, pred_context, GT_context, GT_answer, model, ntok=20)\n",
    "\n",
    "    pred = torch.mean(pred_out_logits, dim=1)\n",
    "    real = torch.mean(real_out_logits, dim=1)\n",
    "\n",
    "    ans = answer.replace('<EOC>','')\n",
    "    # print(f'gt {sample[\"answer\"][0].strip()}\\npred: {ans.strip()}')\n",
    "\n",
    "    if sample[\"answer\"][0].strip().lower() == ans.strip().lower():\n",
    "        identical_num += 1\n",
    "\n",
    "    similarity = cos_similarity(pred, real)\n",
    "    if similarity.detach().numpy()[0] > 0.95:\n",
    "        print(\"Model out : \",tokenizer.decode(out[out != 50257]))\n",
    "        print(\"Correct Answer: \" + sample[\"answer\"][0])\n",
    "        similarity_num += 1\n",
    "        if similarity_num % 10 == 0:\n",
    "            print(f'Another 10..')\n",
    "        preds = torch.cat([preds,pred], dim=1)\n",
    "        reals = torch.cat([reals,real], dim=1)\n",
    "\n",
    "print(f'There were {similarity_num} TPs out of {n}')\n",
    "print(f'There were {identical_num} identi cal answers out of {n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do validation dataset eval\n",
    "test_loader_iter = iter(test_loader)\n",
    "n = len(test_loader)\n",
    "print(f'validation dataset has {n} samples')\n",
    "\n",
    "preds = torch.tensor([])\n",
    "reals = torch.tensor([])\n",
    "\n",
    "similarity_num = 0\n",
    "identical_num = 0\n",
    "\n",
    "for sample in test_loader_iter:\n",
    "    val_img = sample[\"image\"]\n",
    "    pred_context  = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+ sample[\"question\"][0] + ' answer:')]) \n",
    "    GT_context  = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+ sample[\"question\"][0] + ' answer: ' + sample[\"answer\"][0] + '<EOC>')])\n",
    "    GT_answer =  torch.tensor([tokenizer.encode(\"<|endoftext|> \" + sample[\"answer\"][0] + ' <EOC>')])\n",
    "    out, pred_out_logits, real_out_logits, answer = generate_logits(val_img, pred_context, GT_context, GT_answer, model, ntok=20)\n",
    "\n",
    "    pred = torch.mean(pred_out_logits, dim=1)\n",
    "    real = torch.mean(real_out_logits, dim=1)\n",
    "\n",
    "    ans = answer.replace('<EOC>','')\n",
    "    # print(f'gt {sample[\"answer\"][0].strip()}\\npred: {ans.strip()}')\n",
    "\n",
    "    if sample[\"answer\"][0].strip().lower() == ans.strip().lower():\n",
    "        identical_num += 1\n",
    "\n",
    "    similarity = cos_similarity(pred, real)\n",
    "    if similarity.detach().numpy()[0] > 0.95:\n",
    "        # print(\"Model out : \",tokenizer.decode(out[out != 50257]))\n",
    "        # print(\"Correct Answer: \" + sample[\"answer\"][0])\n",
    "        similarity_num += 1\n",
    "        if similarity_num % 10 == 0:\n",
    "            print(f'Another 10..')\n",
    "        preds = torch.cat([preds,pred], dim=1)\n",
    "        reals = torch.cat([reals,real], dim=1)\n",
    "\n",
    "print(f'There were {similarity_num} TPs out of {n}')\n",
    "print(f'There were {identical_num} identi cal answers out of {n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do it for the flamingo off ROCO model with classification head and EOQ token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ROOT = '/home/andrei/mlmi/home/mlmi-matthias/VQA-RAD'\n",
    "PRETRAINED_CLIP_PATH = '/home/andrei/mlmi/home/mlmi-matthias/Caghan/pretrained_models/PubMedCLIP_ViT32.pth'\n",
    "PRETRAINED_GPT2_PATH = \"/home/andrei/mlmi/home/mlmi-matthias/Caghan/pretrained_models/gpt2-pytorch_model.bin\"\n",
    "ANSWERS_LIST_PATH = '/home/andrei/mlmi/home/mlmi-matthias/VQA-RAD/unique_answers.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2248 QA pairs in VQA-RAD dataset\n",
      "Dataset root: /home/andrei/mlmi/home/mlmi-matthias/VQA-RAD\n",
      "Loading all images into memory... for train\n",
      "Found 314 images\n",
      "Dataset root: /home/andrei/mlmi/home/mlmi-matthias/VQA-RAD\n",
      "Loading all images into memory... for val\n",
      "Found 314 images\n",
      "Dataset root: /home/andrei/mlmi/home/mlmi-matthias/VQA-RAD\n",
      "Loading all images into memory... for test\n",
      "Found 314 images\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "NUM_DATA_WORKERS  = 8\n",
    "ONLY_IMAGES = False\n",
    "BATCH_SIZE = 1\n",
    "NUM_EPOCHS = 80\n",
    "LIMIT_NUM_SAMPLES = None\n",
    "\n",
    "IMAGE_TYPE = \"jpg\"\n",
    "SHUFFLE = True\n",
    "TOKENIZER  = \"gpt2\"\n",
    "LOAD_IN_MEM = True\n",
    "PREPROCESSED = False\n",
    "RETURN_IDX_EOC = True\n",
    "\n",
    "RAD_datamodule = VQRadDataModule(\n",
    "                                batch_size=BATCH_SIZE, transforms=transforms, root=DATASET_ROOT,\n",
    "                                limit_num_samples=LIMIT_NUM_SAMPLES, num_workers=NUM_DATA_WORKERS, shuffle=SHUFFLE,\n",
    "                                tokenizer=\"gpt2\", preprocessed=PREPROCESSED, load_in_memory=LOAD_IN_MEM, answers_list_path=ANSWERS_LIST_PATH,\n",
    "                                return_idx_answer_eoc=RETURN_IDX_EOC\n",
    ")\n",
    "\n",
    "train_loader = RAD_datamodule.train_dataloader()\n",
    "val_loader = RAD_datamodule.val_dataloader()\n",
    "test_loader = RAD_datamodule.test_dataloader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmpcoqtq78h\n",
      "INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmpcoqtq78h/_remote_module_non_scriptable.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANGUAGE_MODEL :  gpt2 \n",
      "NUM_TOKENS :  50261 \n",
      "FLAMINGO_EMBED_DIM :  768 \n",
      "DEPTH :  12 \n",
      "NUM_HEADS :  8 \n",
      "ATT_HEAD_DIM :  64 \n",
      "CROOS_ATT_EVERY :  3 \n",
      "MEDIA_TOKEN_ID :  50258 \n",
      "PERCEIVER_NUM_LATENTS :  64 \n",
      "PERCEIVER_DEPTH :  2 \n",
      "IMAGE_ENCODER :  clip \n",
      "PRETRAINED_CLIP_PATH :  /home/andrei/mlmi/home/mlmi-matthias/Caghan/pretrained_models/PubMedCLIP_ViT32.pth \n",
      "PRETRAINED_GPT2_PATH :  /home/andrei/mlmi/home/mlmi-matthias/Caghan/pretrained_models/gpt2-pytorch_model.bin \n",
      "\n",
      "Clip architecture is being loaded\n",
      "Clip pretrained weights are being loaded\n",
      "Flamingo is being initialized with  gpt2  as language model\n",
      "GPT 2 Weights are loading...\n",
      "Loaded GPT2 weights and Embeddings num_weights loaded :  156\n"
     ]
    }
   ],
   "source": [
    "#Â MODEL HPRAMS\n",
    "VOCAB_SIZE_OF_TOKENIZER = 50257 #Â mimic_datamodule.train_dataset.tokenizer.vocab_size\n",
    "LANGUAGE_MODEL = 'gpt2'\n",
    "NUM_TOKENS = VOCAB_SIZE_OF_TOKENIZER +4 if LANGUAGE_MODEL==\"gpt2\" else 31092\n",
    "FLAMINGO_EMBED_DIM = 768\n",
    "DEPTH = 12\n",
    "NUM_HEADS = 8\n",
    "ATT_HEAD_DIM = 64\n",
    "CROOS_ATT_EVERY=3\n",
    "MEDIA_TOKEN_ID = RAD_datamodule.train_dataset.tokenizer.\\\n",
    "    all_special_ids[RAD_datamodule.train_dataset.tokenizer.all_special_tokens.index('<image>')]\n",
    "PERCEIVER_NUM_LATENTS = 64\n",
    "PERCEIVER_DEPTH = 2\n",
    "IMAGE_ENCODER = \"clip\"\n",
    "CLASSIFICATION_MODE = True\n",
    "NUM_CLASSES = 25\n",
    "FLAMINGO_MODE = False\n",
    "LABEL_SMOOTHING = 0.2\n",
    "TOKEN_LABEL_SMOOTHING = 0.0\n",
    "GRADIENT_CLIP_VAL = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "USE_IMAGE_EMBEDDINGS = True\n",
    "TRAIN_EMBEDDING_LAYER = True\n",
    "CLASSIFIER_DROPOUT = 0.5\n",
    "\n",
    "\n",
    "print(\"LANGUAGE_MODEL : \",LANGUAGE_MODEL, \"\\n\"\n",
    "        \"NUM_TOKENS : \",NUM_TOKENS, \"\\n\"\n",
    "        \"FLAMINGO_EMBED_DIM : \",FLAMINGO_EMBED_DIM, \"\\n\"\n",
    "        \"DEPTH : \",DEPTH, \"\\n\"\n",
    "        \"NUM_HEADS : \",NUM_HEADS, \"\\n\"\n",
    "        \"ATT_HEAD_DIM : \",ATT_HEAD_DIM, \"\\n\"\n",
    "        \"CROOS_ATT_EVERY : \",CROOS_ATT_EVERY, \"\\n\"\n",
    "        \"MEDIA_TOKEN_ID : \",MEDIA_TOKEN_ID, \"\\n\"\n",
    "        \"PERCEIVER_NUM_LATENTS : \",PERCEIVER_NUM_LATENTS, \"\\n\"\n",
    "        \"PERCEIVER_DEPTH : \",PERCEIVER_DEPTH, \"\\n\"\n",
    "        \"IMAGE_ENCODER : \",IMAGE_ENCODER, \"\\n\"\n",
    "        \"PRETRAINED_CLIP_PATH : \",PRETRAINED_CLIP_PATH, \"\\n\"\n",
    "        \"PRETRAINED_GPT2_PATH : \",PRETRAINED_GPT2_PATH, \"\\n\")\n",
    "\n",
    "\n",
    "hyperparams = {\n",
    "    'pretrained_clip_path': PRETRAINED_CLIP_PATH,\n",
    "    'warmup_steps': 30,\n",
    "    'num_tokens': NUM_TOKENS,\n",
    "    'dim': FLAMINGO_EMBED_DIM,\n",
    "    'depth': DEPTH,\n",
    "    'num_heads': NUM_HEADS,\n",
    "    'dim_head': ATT_HEAD_DIM,\n",
    "    'cross_attn_every': CROOS_ATT_EVERY,\n",
    "    'media_token_id': MEDIA_TOKEN_ID,\n",
    "    'perceiver_num_latents': PERCEIVER_NUM_LATENTS,\n",
    "    'perceiver_depth': PERCEIVER_DEPTH,\n",
    "    'image_encoder': IMAGE_ENCODER,\n",
    "    'language_model': LANGUAGE_MODEL,\n",
    "    'pretrained_gpt2_path': PRETRAINED_GPT2_PATH,\n",
    "    'classification_mode': CLASSIFICATION_MODE,\n",
    "    'classification_num_classes': NUM_CLASSES,  # 332 if DATASET==\"IMAGECLEF\"\n",
    "    'flamingo_mode': FLAMINGO_MODE,\n",
    "    \"label_smoothing\": LABEL_SMOOTHING,\n",
    "    \"token_label_smoothing\": TOKEN_LABEL_SMOOTHING,\n",
    "    \"learning_rate\":LEARNING_RATE,\n",
    "    \"use_image_embeddings\": USE_IMAGE_EMBEDDINGS,\n",
    "    \"train_embedding_layer\": TRAIN_EMBEDDING_LAYER,\n",
    "    \"classifier_dropout\": CLASSIFIER_DROPOUT\n",
    "}\n",
    "\n",
    "\n",
    "model = FlamingoModule(**hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained Flamingo Model is loaded from checkpoint :  /home/andrei/mlmi/home/mlmi-matthias/Andrei/mlmi-vqa/notebooks/lightning_logs/flamingo_clip_GPT2_FT_vqaRad_ROCOback_all_flamingoON_classification/checkpoints/epoch=36-val_acc_epoch=0.75-val_total_loss_epoch=3.03-val_loss_generation_epoch=1.27-val_classification_loss_epoch=1.76.ckpt\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT_PATH = \"/home/andrei/mlmi/home/mlmi-matthias/Andrei/mlmi-vqa/notebooks/lightning_logs/\\\n",
    "flamingo_clip_GPT2_FT_vqaRad_ROCOback_all_flamingoON_classification/checkpoints/\\\n",
    "epoch=36-val_acc_epoch=0.75-val_total_loss_epoch=3.03-val_loss_generation_epoch=1.27-val_classification_loss_epoch=1.76.ckpt\"\n",
    "START_FROM_CHECKPOINT = True\n",
    "\n",
    "if START_FROM_CHECKPOINT:\n",
    "    print(\"Pretrained Flamingo Model is loaded from checkpoint : \",CHECKPOINT_PATH)\n",
    "    model.load_state_dict(torch.load(CHECKPOINT_PATH, map_location=torch.device('cuda:1'))[\"state_dict\"],strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from transformers import GPT2Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn as nn\n",
    "import torch.nn.functional as F\n",
    "def generate(image, context, cur_model, batch, ntok=20):\n",
    "    classification_logits = None\n",
    "    for _ in range(ntok):\n",
    "        out, classification_logits = cur_model({'image': image,'input_ids': context, \"index_eoq\": batch[\"index_eoq\"],\n",
    "        \"targets\": batch[\"targets\"],\"label\": batch[\"label\"]})\n",
    "        logits = out[:, -1, :]\n",
    "        indices_to_remove = logits < torch.topk(logits, 10)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = np.NINF\n",
    "        #next_tok1 = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1).squeeze(1)\n",
    "        #print(next_tok1.shape)\n",
    "        softmax_out = F.softmax(logits, dim=-1)\n",
    "        #print(softmax_out.shape)\n",
    "        next_tok = torch.argmax(softmax_out,dim=-1,keepdim=False)\n",
    "        #print(next_tok.shape)\n",
    "        context = torch.cat([context, next_tok.unsqueeze(-1)], dim=-1)\n",
    "    return context, classification_logits\n",
    "\n",
    "\n",
    "tokenizer = RAD_datamodule.train_dataset.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/405 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 405/405 [16:49<00:00,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 405/405 [16:49<00:00,  2.49s/it]\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "val_dataloader = RAD_datamodule.val_dataloader()\n",
    "tokenizer = RAD_datamodule.train_dataset.tokenizer\n",
    "true_predictions = []\n",
    "false_predictions = []\n",
    "classification = []\n",
    "for batch in tqdm(val_dataloader):\n",
    "    context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' <EOQ>'+ ' answer:')]) \n",
    "    out, classification_logits = generate(batch[\"image\"], context, model, batch, ntok=20)\n",
    "    # print(f'class shape: {classification_logits.shape}')\n",
    "    models_answer = tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0].rstrip().strip()\n",
    "    correct_answer = batch[\"answer\"][0].rstrip().strip()\n",
    "    if models_answer == correct_answer:\n",
    "        correct += 1\n",
    "        true_predictions.append((models_answer, correct_answer))\n",
    "    else:\n",
    "        false_predictions.append((models_answer, correct_answer))\n",
    "    classification.append((batch['label'], torch.argmax(classification_logits)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30864197530864196"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct/len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 0\n",
    "for tup in classification:\n",
    "    if tup[0] == tup[1]:\n",
    "        num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8740740740740741"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num/len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/225 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 225/225 [10:01<00:00,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 225/225 [10:01<00:00,  2.67s/it]\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "test_dataloader = RAD_datamodule.test_dataloader()\n",
    "tokenizer = RAD_datamodule.train_dataset.tokenizer\n",
    "true_predictions = []\n",
    "false_predictions = []\n",
    "classification = []\n",
    "for batch in tqdm(test_dataloader):\n",
    "    context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' <EOQ>'+ ' answer:')]) \n",
    "    out, classification_logits = generate(batch[\"image\"], context, model, batch, ntok=20)\n",
    "    # print(f'class shape: {classification_logits.shape}')\n",
    "    models_answer = tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0].rstrip().strip()\n",
    "    correct_answer = batch[\"answer\"][0].rstrip().strip()\n",
    "    if models_answer == correct_answer:\n",
    "        correct += 1\n",
    "        true_predictions.append((models_answer, correct_answer))\n",
    "    else:\n",
    "        false_predictions.append((models_answer, correct_answer))\n",
    "    classification.append((batch['label'], torch.argmax(classification_logits)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = 0\n",
    "for tup in classification:\n",
    "    if tup[0] == tup[1]:\n",
    "        num += 1\n",
    "\n",
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num/len(test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 405/405 [00:55<00:00,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 405/405 [00:56<00:00,  7.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# generate answer embeds\n",
    "val_loader_iter = iter(val_loader)\n",
    "\n",
    "answer_embeds = dict()\n",
    "\n",
    "for batch in tqdm(val_loader_iter):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        ans = batch['answer'][0]\n",
    "        if ans not in answer_embeds.keys():\n",
    "            answer_embeds[ans] = torch.Tensor([])\n",
    "        GT_answer =  torch.tensor([tokenizer.encode(\"<|endoftext|> \" + batch[\"answer\"][0] + ' <EOC>')])\n",
    "        pred_out_logits, _ = model.forward({'image': batch['image'], 'input_ids': GT_answer}, return_embeds=True)\n",
    "        answer_embeds[ans] = torch.cat([answer_embeds[ans], torch.mean(pred_out_logits, dim=1)])\n",
    "\n",
    "# mean all possible answers\n",
    "for key in tqdm(answer_embeds.keys()):\n",
    "    if answer_embeds[key].shape[0] == 1:\n",
    "        continue\n",
    "    answer_embeds[key] = torch.mean(answer_embeds[key], dim=0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_llogits(image, pred_context, model, ntok, batch):\n",
    "\n",
    "    # get a prediction (whole answer)\n",
    "    pred_out_logits = None\n",
    "    pred_answer = torch.tensor([[]])\n",
    "    for i in range(ntok):\n",
    "        out, classification = model.forward({'image': image,'input_ids': pred_context, 'index_eoq': batch['index_eoq'],\n",
    "        'targets': batch['targets'],'label': batch['label']}, return_embeds=True)\n",
    "        logits = out[:, -1, :]\n",
    "        indices_to_remove = logits < torch.topk(logits, 10)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = np.NINF\n",
    "        next_tok = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1).squeeze(1)\n",
    "        pred_answer = torch.cat([pred_answer, next_tok.unsqueeze(-1)], dim=-1)\n",
    "        pred_context = torch.cat([pred_context, next_tok.unsqueeze(-1)], dim=-1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        # only get embeddings for answer and GT from the model\n",
    "        answer = tokenizer.decode(pred_answer[pred_answer!=50257].int())                # remove padding tokens, <EOC> token remains\n",
    "        # print(f'Ans: {answer}')\n",
    "        pred_answer = torch.tensor([tokenizer.encode(\"<|endoftext|> \" + answer)])\n",
    "        pred_out_logits, _ = model.forward({'image': image,'input_ids': pred_answer}, return_embeds=True)\n",
    "\n",
    "    return pred_context, pred_out_logits, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "validation dataset has 405 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|âââ       | 104/405 [04:37<14:48,  2.95s/it]"
     ]
    }
   ],
   "source": [
    "# do validation dataset eval\n",
    "val_loader_iter = iter(val_loader)\n",
    "n = len(val_loader)\n",
    "print(f'validation dataset has {n} samples')\n",
    "\n",
    "preds = torch.tensor([])\n",
    "reals = torch.tensor([])\n",
    "\n",
    "similarity_num = 0\n",
    "identical_num = 0\n",
    "\n",
    "for sample in tqdm(val_loader_iter):\n",
    "    val_img = sample[\"image\"]\n",
    "    pred_context  = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+ sample[\"question\"][0] + ' answer:')]) \n",
    "    GT_context  = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+ sample[\"question\"][0] + ' answer: ' + sample[\"answer\"][0] + '<EOC>')])\n",
    "    out, pred_out_logits, answer = generate_llogits(val_img, pred_context, model, ntok=20, batch=sample)\n",
    "\n",
    "    # print(f'pred: {pred_out_logits.shape}\\nreal: {real_out_logits.shape}')\n",
    "    pred = torch.mean(pred_out_logits, dim=1)\n",
    "\n",
    "    ans = answer.replace('<EOC>','')\n",
    "\n",
    "    if sample[\"answer\"][0].strip().lower() == ans.strip().lower():\n",
    "        identical_num += 1\n",
    "\n",
    "    multiples = [len(answer_embeds.keys()), 1]\n",
    "\n",
    "    b = pred.tile(multiples)\n",
    "\n",
    "    similarity_max = np.NINF\n",
    "    answer = None\n",
    "    for i, key in enumerate(answer_embeds.keys()):\n",
    "        similarity = torch.nn.CosineSimilarity()(b[i], answer_embeds[key])\n",
    "        if similarity_max < similarity:\n",
    "            similarity_max = similarity\n",
    "            answer = key\n",
    "\n",
    "    if answer == sample['answer'][0]:\n",
    "        similarity_num += 1\n",
    "    \n",
    "    # print(\"Model out : \", answer)\n",
    "    # print(\"Correct Answer: \" + sample[\"answer\"][0])\n",
    "    # print(f'Embed closest answer: {answer}')\n",
    "\n",
    "print(f'There were {similarity_num} TPs out of {n}')\n",
    "print(f'There were {identical_num} identi cal answers out of {n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "validation dataset has 225 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/225 [00:02<09:05,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model out :  Gallstones\n",
      "Correct Answer: Splenule\n",
      "Embed closest answer: Gallstones\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/225 [00:04<09:03,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model out :  Pacemaker\n",
      "Correct Answer: no\n",
      "Embed closest answer: Pacemaker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â         | 3/225 [00:07<09:10,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model out :  Pacemaker\n",
      "Correct Answer: no\n",
      "Embed closest answer: Pacemaker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â         | 4/225 [00:10<09:35,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model out :  Pacemaker\n",
      "Correct Answer: No\n",
      "Embed closest answer: Pacemaker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â         | 5/225 [00:12<09:30,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model out :  Bilateral frontal lobes\n",
      "Correct Answer: No\n",
      "Embed closest answer: Bilateral frontal lobes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â         | 6/225 [00:15<09:26,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model out :  Gallstones\n",
      "Correct Answer: necrosis\n",
      "Embed closest answer: Gallstones\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â         | 7/225 [00:17<09:16,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model out :  Gallstones\n",
      "Correct Answer: necrosis\n",
      "Embed closest answer: Gallstones\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â         | 8/225 [00:20<09:08,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model out :  Gallstones\n",
      "Correct Answer: posteriorly\n",
      "Embed closest answer: Gallstones\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â         | 9/225 [00:22<09:00,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model out :  ~15 minutes\tpotentially faster with newer imaging systems\n",
      "Correct Answer: posteriorly\n",
      "Embed closest answer: ~15 minutes\tpotentially faster with newer imaging systems\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â         | 10/225 [00:25<09:00,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model out :  Pacemaker\n",
      "Correct Answer: Right\n",
      "Embed closest answer: Pacemaker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|â         | 11/225 [00:27<08:57,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model out :  Pacemaker\n",
      "Correct Answer: Right\n",
      "Embed closest answer: Pacemaker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|â         | 12/225 [00:30<08:55,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model out :  Gallstones\n",
      "Correct Answer: Yes\n",
      "Embed closest answer: Gallstones\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â         | 13/225 [00:32<08:46,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model out :  Gallstones\n",
      "Correct Answer: Yes\n",
      "Embed closest answer: Gallstones\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â         | 14/225 [00:35<08:41,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model out :  Gallstones\n",
      "Correct Answer: YES\n",
      "Embed closest answer: Gallstones\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â         | 15/225 [00:37<08:35,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model out :  Gallstones\n",
      "Correct Answer: The lungs\n",
      "Embed closest answer: Gallstones\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â         | 16/225 [00:40<08:41,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model out :  parietal and occipital lobes\n",
      "Correct Answer: Right lung\n",
      "Embed closest answer: parietal and occipital lobes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â         | 17/225 [00:43<09:01,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model out :  ~15 minutes\tpotentially faster with newer imaging systems\n",
      "Correct Answer: Right lung\n",
      "Embed closest answer: ~15 minutes\tpotentially faster with newer imaging systems\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â         | 18/225 [00:45<09:04,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model out :  Gallstones\n",
      "Correct Answer: Yes\n",
      "Embed closest answer: Gallstones\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â         | 19/225 [00:48<08:52,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model out :  Gallstones\n",
      "Correct Answer: Yes\n",
      "Embed closest answer: Gallstones\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|â         | 20/225 [00:50<08:48,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model out :  Calcified atherosclerosis\n",
      "Correct Answer: No\n",
      "Embed closest answer: Calcified atherosclerosis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|â         | 21/225 [00:53<08:45,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model out :  Gallstones\n",
      "Correct Answer: No\n",
      "Embed closest answer: Gallstones\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â         | 22/225 [00:56<08:54,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model out :  Pacemaker\n",
      "Correct Answer: head/neck CT\n",
      "Embed closest answer: Pacemaker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â         | 23/225 [00:58<08:48,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model out :  Pacemaker\n",
      "Correct Answer: Biopsy\n",
      "Embed closest answer: Pacemaker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|â         | 24/225 [01:01<08:33,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model out :  Gallstones\n",
      "Correct Answer: metastases, infection/abcess, glioblastoma\n",
      "Embed closest answer: Gallstones\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|â         | 25/225 [01:03<08:29,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model out :  Gallstones\n",
      "Correct Answer: Toxoplasma, lymphoma, abscesses, other brain tumors\n",
      "Embed closest answer: Gallstones\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|ââ        | 26/225 [01:06<08:28,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model out :  Gallstones\n",
      "Correct Answer: Hemorrhage\n",
      "Embed closest answer: Gallstones\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|ââ        | 27/225 [01:08<08:25,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model out :  Gallstones\n",
      "Correct Answer: Cancer\n",
      "Embed closest answer: Gallstones\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|ââ        | 28/225 [01:11<08:38,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model out :  Gallstones\n",
      "Correct Answer: ureteral obstruction\n",
      "Embed closest answer: Gallstones\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|ââ        | 29/225 [01:14<08:44,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model out :  Gallstones\n",
      "Correct Answer: ureteral obstruction\n",
      "Embed closest answer: Gallstones\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|ââ        | 30/225 [01:17<08:42,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model out :  Pacemaker\n",
      "Correct Answer: lentiform\n",
      "Embed closest answer: Pacemaker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|ââ        | 31/225 [01:19<08:42,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model out :  Pacemaker\n",
      "Correct Answer: Biconvex\n",
      "Embed closest answer: Pacemaker\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|ââ        | 31/225 [01:21<08:29,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/andrei/mlmi/home/mlmi-matthias/Andrei/mlmi-vqa/notebooks/playground/vqaRAD_flamingo_clip_gpt2_infer.ipynb Cell 35\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrei/mlmi/home/mlmi-matthias/Andrei/mlmi-vqa/notebooks/playground/vqaRAD_flamingo_clip_gpt2_infer.ipynb#ch0000038?line=13'>14</a>\u001b[0m pred_context  \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([tokenizer\u001b[39m.\u001b[39mencode(\u001b[39m\"\u001b[39m\u001b[39m<|endoftext|> <image> question: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m sample[\u001b[39m\"\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m answer:\u001b[39m\u001b[39m'\u001b[39m)]) \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrei/mlmi/home/mlmi-matthias/Andrei/mlmi-vqa/notebooks/playground/vqaRAD_flamingo_clip_gpt2_infer.ipynb#ch0000038?line=14'>15</a>\u001b[0m GT_context  \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([tokenizer\u001b[39m.\u001b[39mencode(\u001b[39m\"\u001b[39m\u001b[39m<|endoftext|> <image> question: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m sample[\u001b[39m\"\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m answer: \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m sample[\u001b[39m\"\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m<EOC>\u001b[39m\u001b[39m'\u001b[39m)])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/andrei/mlmi/home/mlmi-matthias/Andrei/mlmi-vqa/notebooks/playground/vqaRAD_flamingo_clip_gpt2_infer.ipynb#ch0000038?line=15'>16</a>\u001b[0m out, pred_out_logits, answer \u001b[39m=\u001b[39m generate_llogits(img, pred_context, model, ntok\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m, batch\u001b[39m=\u001b[39;49msample)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrei/mlmi/home/mlmi-matthias/Andrei/mlmi-vqa/notebooks/playground/vqaRAD_flamingo_clip_gpt2_infer.ipynb#ch0000038?line=17'>18</a>\u001b[0m \u001b[39m# print(f'pred: {pred_out_logits.shape}\\nreal: {real_out_logits.shape}')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrei/mlmi/home/mlmi-matthias/Andrei/mlmi-vqa/notebooks/playground/vqaRAD_flamingo_clip_gpt2_infer.ipynb#ch0000038?line=18'>19</a>\u001b[0m pred \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(pred_out_logits, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;32m/home/andrei/mlmi/home/mlmi-matthias/Andrei/mlmi-vqa/notebooks/playground/vqaRAD_flamingo_clip_gpt2_infer.ipynb Cell 35\u001b[0m in \u001b[0;36mgenerate_llogits\u001b[0;34m(image, pred_context, model, ntok, batch)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/andrei/mlmi/home/mlmi-matthias/Andrei/mlmi-vqa/notebooks/playground/vqaRAD_flamingo_clip_gpt2_infer.ipynb#ch0000038?line=5'>6</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/andrei/mlmi/home/mlmi-matthias/Andrei/mlmi-vqa/notebooks/playground/vqaRAD_flamingo_clip_gpt2_infer.ipynb#ch0000038?line=6'>7</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(ntok):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/andrei/mlmi/home/mlmi-matthias/Andrei/mlmi-vqa/notebooks/playground/vqaRAD_flamingo_clip_gpt2_infer.ipynb#ch0000038?line=7'>8</a>\u001b[0m         out, classification \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward({\u001b[39m'\u001b[39;49m\u001b[39mimage\u001b[39;49m\u001b[39m'\u001b[39;49m: image,\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m: pred_context, \u001b[39m'\u001b[39;49m\u001b[39mindex_eoq\u001b[39;49m\u001b[39m'\u001b[39;49m: batch[\u001b[39m'\u001b[39;49m\u001b[39mindex_eoq\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/andrei/mlmi/home/mlmi-matthias/Andrei/mlmi-vqa/notebooks/playground/vqaRAD_flamingo_clip_gpt2_infer.ipynb#ch0000038?line=8'>9</a>\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mtargets\u001b[39;49m\u001b[39m'\u001b[39;49m: batch[\u001b[39m'\u001b[39;49m\u001b[39mtargets\u001b[39;49m\u001b[39m'\u001b[39;49m],\u001b[39m'\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m'\u001b[39;49m: batch[\u001b[39m'\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m'\u001b[39;49m]}, return_embeds\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrei/mlmi/home/mlmi-matthias/Andrei/mlmi-vqa/notebooks/playground/vqaRAD_flamingo_clip_gpt2_infer.ipynb#ch0000038?line=9'>10</a>\u001b[0m         logits \u001b[39m=\u001b[39m out[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrei/mlmi/home/mlmi-matthias/Andrei/mlmi-vqa/notebooks/playground/vqaRAD_flamingo_clip_gpt2_infer.ipynb#ch0000038?line=10'>11</a>\u001b[0m         indices_to_remove \u001b[39m=\u001b[39m logits \u001b[39m<\u001b[39m torch\u001b[39m.\u001b[39mtopk(logits, \u001b[39m10\u001b[39m)[\u001b[39m0\u001b[39m][\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mNone\u001b[39;00m]\n",
      "File \u001b[0;32m~/mlmi/home/mlmi-matthias/Andrei/mlmi-vqa/notebooks/playground/../../src/models/multimodal/flamingo_module.py:160\u001b[0m, in \u001b[0;36mFlamingoModule.forward\u001b[0;34m(self, x, return_attn, return_embeds)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39mif\u001b[39;00m return_embeds:\n\u001b[1;32m    158\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassification_mode:\n\u001b[1;32m    159\u001b[0m         \u001b[39m# index_eoq = x[\"index_eoq\"]\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m         flamingo_logits, token_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflamingo_palm(\n\u001b[1;32m    161\u001b[0m             input_tokens\u001b[39m.\u001b[39;49msqueeze(\u001b[39m1\u001b[39;49m), images\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m1\u001b[39;49m), return_attn\u001b[39m=\u001b[39;49mreturn_attn, return_embeds\u001b[39m=\u001b[39;49mreturn_embeds\n\u001b[1;32m    162\u001b[0m         )\n\u001b[1;32m    164\u001b[0m         \u001b[39m# classification_logits = self.classifier(token_embeds[torch.arange(batch_size), index_eoq])\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39m# classification_logits = torch.softmax(classification_logits, dim=1)\u001b[39;00m\n\u001b[1;32m    167\u001b[0m         \u001b[39mreturn\u001b[39;00m token_embeds, \u001b[39mNone\u001b[39;00m \u001b[39m#classification_logits\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlmi/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mlmi/home/mlmi-matthias/Andrei/mlmi-vqa/notebooks/playground/../../src/models/multimodal/flamingo_model.py:393\u001b[0m, in \u001b[0;36mFlamingoModel.forward\u001b[0;34m(self, text, images, image_embeds, return_attn, return_image_embeddings, return_embeds)\u001b[0m\n\u001b[1;32m    389\u001b[0m         image_embeds, attns \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_encoder(\n\u001b[1;32m    390\u001b[0m             images, return_attn\u001b[39m=\u001b[39mreturn_attn\n\u001b[1;32m    391\u001b[0m         )\n\u001b[1;32m    392\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 393\u001b[0m         image_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimg_encoder(images)\n\u001b[1;32m    394\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_encoder_outdim_layer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    395\u001b[0m     image_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_encoder_outdim_layer(image_embeds)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlmi/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mlmi/home/mlmi-matthias/Andrei/mlmi-vqa/notebooks/playground/../../src/models/multimodal/clip_model.py:327\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x, return_before_ln, return_attn)\u001b[0m\n\u001b[1;32m    325\u001b[0m     x, attns \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer(x, return_attn\u001b[39m=\u001b[39mreturn_attn)\n\u001b[1;32m    326\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(x)\n\u001b[1;32m    328\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# LND -> NLD\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[39mif\u001b[39;00m return_before_ln \u001b[39mand\u001b[39;00m return_attn:\n",
      "File \u001b[0;32m~/anaconda3/envs/mlmi/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mlmi/home/mlmi-matthias/Andrei/mlmi-vqa/notebooks/playground/../../src/models/multimodal/clip_model.py:263\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, return_attn)\u001b[0m\n\u001b[1;32m    261\u001b[0m         attns\u001b[39m.\u001b[39mappend(attn)\n\u001b[1;32m    262\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 263\u001b[0m         x \u001b[39m=\u001b[39m layer(x)\n\u001b[1;32m    265\u001b[0m \u001b[39mif\u001b[39;00m return_attn:\n\u001b[1;32m    266\u001b[0m     \u001b[39mreturn\u001b[39;00m x, attns\n",
      "File \u001b[0;32m~/anaconda3/envs/mlmi/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mlmi/home/mlmi-matthias/Andrei/mlmi-vqa/notebooks/playground/../../src/models/multimodal/clip_model.py:239\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[0;34m(self, x, return_attn)\u001b[0m\n\u001b[1;32m    235\u001b[0m attn, attn_output_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention(\n\u001b[1;32m    236\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(x), need_weights\u001b[39m=\u001b[39mreturn_attn\n\u001b[1;32m    237\u001b[0m )\n\u001b[1;32m    238\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m attn\n\u001b[0;32m--> 239\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln_2(x))\n\u001b[1;32m    240\u001b[0m \u001b[39mif\u001b[39;00m return_attn:\n\u001b[1;32m    241\u001b[0m     \u001b[39mreturn\u001b[39;00m x, attn_output_weights\n",
      "File \u001b[0;32m~/anaconda3/envs/mlmi/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/mlmi/lib/python3.10/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlmi/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/mlmi/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# do validation dataset eval\n",
    "test_loader_iter = iter(test_loader)\n",
    "n = len(test_loader)\n",
    "print(f'validation dataset has {n} samples')\n",
    "\n",
    "preds = torch.tensor([])\n",
    "reals = torch.tensor([])\n",
    "\n",
    "similarity_num = 0\n",
    "identical_num = 0\n",
    "\n",
    "for sample in tqdm(test_loader_iter):\n",
    "    img = sample[\"image\"]\n",
    "    pred_context  = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+ sample[\"question\"][0] + ' answer:')]) \n",
    "    GT_context  = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+ sample[\"question\"][0] + ' answer: ' + sample[\"answer\"][0] + '<EOC>')])\n",
    "    out, pred_out_logits, answer = generate_llogits(img, pred_context, model, ntok=20, batch=sample)\n",
    "\n",
    "    # print(f'pred: {pred_out_logits.shape}\\nreal: {real_out_logits.shape}')\n",
    "    pred = torch.mean(pred_out_logits, dim=1)\n",
    "\n",
    "    ans = answer.replace('<EOC>','')\n",
    "\n",
    "    if sample[\"answer\"][0].strip().lower() == ans.strip().lower():\n",
    "        identical_num += 1\n",
    "\n",
    "    multiples = [len(answer_embeds.keys()), 1]\n",
    "\n",
    "    b = pred.tile(multiples)\n",
    "\n",
    "    similarity_max = np.NINF\n",
    "    answer = None\n",
    "    for i, key in enumerate(answer_embeds.keys()):\n",
    "        similarity = torch.nn.CosineSimilarity()(b[i], answer_embeds[key])\n",
    "        if similarity_max < similarity:\n",
    "            similarity_max = similarity\n",
    "            answer = key\n",
    "\n",
    "    if answer == sample['answer'][0]:\n",
    "        similarity_num += 1\n",
    "    \n",
    "    # print(\"Model out : \", answer)\n",
    "    # print(\"Correct Answer: \" + sample[\"answer\"][0])\n",
    "    # print(f'Embed closest answer: {answer}')\n",
    "\n",
    "print(f'There were {similarity_num} TPs out of {n}')\n",
    "print(f'There were {identical_num} identi cal answers out of {n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('mlmi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "30ec110bc924a9e139919a87e1ff85100b6c769b1dd45b8d281b6aff673f8e03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
