{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import comet_ml at the top of your file\n",
    "import comet_ml\n",
    "from pytorch_lightning.loggers import CometLogger\n",
    "import sys \n",
    "sys.path.append('..')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from src.datasets.mimic_cxr_dataset import MIMICCXRDataModule\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "# sets seeds for numpy, torch, python.random and PYTHONHASHSEED.\n",
    "seed_everything(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "augmentations = {'train':\n",
    "    T.Compose(\n",
    "    [\n",
    "        T.Resize((224, 224)),\n",
    "        #T.RandomApply([T.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1)], p=0.8),\n",
    "        #T.RandomGrayscale(p=0.2),\n",
    "        #T.GaussianBlur(kernel_size=9),\n",
    "        T.ToTensor(),\n",
    "    ]),\n",
    "    'val':\n",
    "    T.Compose(\n",
    "    [\n",
    "        T.Resize((224, 224)),\n",
    "        #T.RandomApply([T.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1)], p=0.8),\n",
    "        #T.RandomGrayscale(p=0.2),\n",
    "        #T.GaussianBlur(kernel_size=9),\n",
    "        T.ToTensor(),\n",
    "    ])\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 200\n",
    "LIMIT_NUM_SAMPLES = 16\n",
    "NUM_DATA_WORKERS  = 0\n",
    "ONLY_IMAGES = False\n",
    "\n",
    "MIMIC_CXR_DCM_PATH = '/Users/caghankoksal/Desktop/development/Flamingo-playground/physionet.org/files/mimic-cxr/2.0.0/files/'\n",
    "MIMIC_CXR_JPG_PATH = '/Users/caghankoksal/Desktop/development/physionet.org/files/mimic-cxr-jpg/2.0.0/files/'\n",
    "SPLIT_PATH = '/Users/caghankoksal/Desktop/SS2022/mlmi-vqa/data/external/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mimic_datamodule = MIMICCXRDataModule(MIMIC_CXR_DCM_PATH, MIMIC_CXR_JPG_PATH, \n",
    "                                      transforms=augmentations, only_images=False, batch_size=BATCH_SIZE,\n",
    "                                      limit_num_samples=LIMIT_NUM_SAMPLES, num_data_workers=NUM_DATA_WORKERS,\n",
    "                                      tokenizer=\"gpt2\",image_type=\"jpg\", split_path=SPLIT_PATH, preprocessed=True)\n",
    "train_loader = mimic_datamodule.train_dataloader()\n",
    "val_loader = mimic_datamodule.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0431, 0.0471, 0.0510],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0706, 0.0745, 0.0784],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0627, 0.0667, 0.0667],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.4902, 0.4784, 0.4549],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.4980, 0.4745, 0.4510],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.5137, 0.4902, 0.4627]],\n",
       " \n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0431, 0.0471, 0.0510],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0706, 0.0745, 0.0784],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0627, 0.0667, 0.0667],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.4902, 0.4784, 0.4549],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.4980, 0.4745, 0.4510],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.5137, 0.4902, 0.4627]],\n",
       " \n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0431, 0.0471, 0.0510],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0706, 0.0745, 0.0784],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0627, 0.0667, 0.0667],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.4902, 0.4784, 0.4549],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.4980, 0.4745, 0.4510],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.5137, 0.4902, 0.4627]]]),\n",
       " 'text': '<|endoftext|> <image> Output: FINAL REPORT EXAMINATION:  CHEST (PORTABLE AP) INDICATION:   year old man with concern for pulmonary renal syndrome/pna  // acute process, worsening TECHNIQUE:  CHEST (PORTABLE AP) COMPARISON:   IMPRESSION: Cardiomediastinal silhouette demonstrate mild dilatation as compared to the previous study, diffuse. There is also vascular enlargement, consistent with pulmonary edema.  New right basal opacity concerning for interval development of atelectasis or aspiration is demonstrated. <EOC>',\n",
       " 'input_ids': tensor([[50256,   220, 50258, 25235,    25, 25261, 39099,  7788,  2390,  1268,\n",
       "           6234,    25,   220,  5870,  6465,   357, 44680, 38148,  3486,     8,\n",
       "          24413,  2149,  6234,    25,   220,   220,   614,  1468,   582,   351,\n",
       "           2328,   329, 45105, 44375, 14027,    14,    79,  2616,   220,  3373,\n",
       "          14352,  1429,    11, 42373, 44999, 22125,    48,  8924,    25,   220,\n",
       "           5870,  6465,   357, 44680, 38148,  3486,     8, 24301,  1503, 39960,\n",
       "             25,   220,   220,  8959, 32761,  2849,    25,  5172,    72, 12657,\n",
       "             72,   459,  1292, 41834, 10176, 11607, 11844,   265,   341,   355,\n",
       "           3688,   284,   262,  2180,  2050,    11, 42864,    13,  1318,   318,\n",
       "            635, 39495, 26537,   972,    11,  6414,   351, 45105,  1225, 19687,\n",
       "             13,   220,   968,   826, 41202, 45912,  9305,   329, 16654,  2478,\n",
       "            286,   379,  9509, 17765,   393, 48217,   318,  9555,    13,   220,\n",
       "          50259, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "          50257, 50257, 50257, 50257, 50257, 50257]]),\n",
       " 'token_type_ids': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'targets': tensor([[  220, 50258, 25235,    25, 25261, 39099,  7788,  2390,  1268,  6234,\n",
       "             25,   220,  5870,  6465,   357, 44680, 38148,  3486,     8, 24413,\n",
       "           2149,  6234,    25,   220,   220,   614,  1468,   582,   351,  2328,\n",
       "            329, 45105, 44375, 14027,    14,    79,  2616,   220,  3373, 14352,\n",
       "           1429,    11, 42373, 44999, 22125,    48,  8924,    25,   220,  5870,\n",
       "           6465,   357, 44680, 38148,  3486,     8, 24301,  1503, 39960,    25,\n",
       "            220,   220,  8959, 32761,  2849,    25,  5172,    72, 12657,    72,\n",
       "            459,  1292, 41834, 10176, 11607, 11844,   265,   341,   355,  3688,\n",
       "            284,   262,  2180,  2050,    11, 42864,    13,  1318,   318,   635,\n",
       "          39495, 26537,   972,    11,  6414,   351, 45105,  1225, 19687,    13,\n",
       "            220,   968,   826, 41202, 45912,  9305,   329, 16654,  2478,   286,\n",
       "            379,  9509, 17765,   393, 48217,   318,  9555,    13,   220, 50259,\n",
       "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "          50257, 50257, 50257, 50257, 50257, 50257]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mimic_datamodule.train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mimic_datamodule.train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mimic_datamodule.validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mimic_datamodule.test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape :  torch.Size([8, 3, 224, 224])\n",
      "<|endoftext|> <image> Output: FINAL REPORT EXAMINATION:  CHEST (PORTABLE AP) INDICATION:   year old man with concern for pulmonary renal syndrome/pna  // acute process, worsening TECHNIQUE:  CHEST (PORTABLE AP) COMPARISON:   IMPRESSION: Cardiomediastinal silhouette demonstrate mild dilatation as compared to the previous study, diffuse. There is also vascular enlargement, consistent with pulmonary edema.  New right basal opacity concerning for interval development of atelectasis or aspiration is demonstrated. <EOC>\n",
      "Input Ids shape :  torch.Size([8, 1, 256])\n",
      "Targets shape :  torch.Size([8, 1, 256])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(\"Image shape : \",batch[\"image\"].shape)\n",
    "    print(batch[\"text\"][0]) # First sample in the batch\n",
    "    print(\"Input Ids shape : \",batch[\"input_ids\"].shape)\n",
    "    print(\"Targets shape : \",batch[\"targets\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.multimodal.flamingo_module import FlamingoModule\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB_SIZE_OF_TOKENIZER :  50257 \n",
      "LANGUAGE_MODEL :  gpt2 \n",
      "NUM_TOKENS :  50260 \n",
      "FLAMINGO_EMBED_DIM :  768 \n",
      "DEPTH :  12 \n",
      "NUM_HEADS :  8 \n",
      "ATT_HEAD_DIM :  64 \n",
      "CROOS_ATT_EVERY :  3 \n",
      "MEDIA_TOKEN_ID :  50258 \n",
      "PERCEIVER_NUM_LATENTS :  64 \n",
      "PERCEIVER_DEPTH :  2 \n",
      "IMAGE_ENCODER :  clip \n",
      "PRETRAINED_CLIP_PATH :  /Users/caghankoksal/Desktop/development/PubMedCLIP_ViT32.pth \n",
      "PRETRAINED_GPT2_PATH :  /Users/caghankoksal/Desktop/development/TransformerPlay/gpt2-pytorch_model.bin \n",
      "\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE_OF_TOKENIZER = 50257 # mimic_datamodule.train_dataset.tokenizer.vocab_size\n",
    "LANGUAGE_MODEL = 'gpt2'\n",
    "NUM_TOKENS = VOCAB_SIZE_OF_TOKENIZER +3 if LANGUAGE_MODEL==\"gpt2\" else 31092\n",
    "FLAMINGO_EMBED_DIM = 768\n",
    "DEPTH = 12\n",
    "NUM_HEADS = 8\n",
    "ATT_HEAD_DIM = 64\n",
    "CROOS_ATT_EVERY=3\n",
    "MEDIA_TOKEN_ID = mimic_datamodule.train_dataset.tokenizer.all_special_ids[mimic_datamodule.train_dataset.tokenizer.all_special_tokens.index('<image>')]\n",
    "PERCEIVER_NUM_LATENTS = 64\n",
    "PERCEIVER_DEPTH = 2\n",
    "IMAGE_ENCODER = \"clip\"\n",
    "PRETRAINED_CLIP_PATH = '/Users/caghankoksal/Desktop/development/PubMedCLIP_ViT32.pth'\n",
    "PRETRAINED_GPT2_PATH = \"/Users/caghankoksal/Desktop/development/TransformerPlay/gpt2-pytorch_model.bin\"\n",
    "\n",
    "print(  \"VOCAB_SIZE_OF_TOKENIZER : \",VOCAB_SIZE_OF_TOKENIZER, \"\\n\"\n",
    "        \"LANGUAGE_MODEL : \",LANGUAGE_MODEL, \"\\n\"\n",
    "        \"NUM_TOKENS : \",NUM_TOKENS, \"\\n\"\n",
    "        \"FLAMINGO_EMBED_DIM : \",FLAMINGO_EMBED_DIM, \"\\n\"\n",
    "        \"DEPTH : \",DEPTH, \"\\n\"\n",
    "        \"NUM_HEADS : \",NUM_HEADS, \"\\n\"\n",
    "        \"ATT_HEAD_DIM : \",ATT_HEAD_DIM, \"\\n\"\n",
    "        \"CROOS_ATT_EVERY : \",CROOS_ATT_EVERY, \"\\n\"\n",
    "        \"MEDIA_TOKEN_ID : \",MEDIA_TOKEN_ID, \"\\n\"\n",
    "        \"PERCEIVER_NUM_LATENTS : \",PERCEIVER_NUM_LATENTS, \"\\n\"\n",
    "        \"PERCEIVER_DEPTH : \",PERCEIVER_DEPTH, \"\\n\"\n",
    "        \"IMAGE_ENCODER : \",IMAGE_ENCODER, \"\\n\"\n",
    "        \"PRETRAINED_CLIP_PATH : \",PRETRAINED_CLIP_PATH, \"\\n\"\n",
    "        \"PRETRAINED_GPT2_PATH : \",PRETRAINED_GPT2_PATH, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'pretrained_clip_path': PRETRAINED_CLIP_PATH,\n",
    "    'warmup_steps': 569,\n",
    "    'num_tokens': NUM_TOKENS,\n",
    "    'dim': FLAMINGO_EMBED_DIM,\n",
    "    'depth': DEPTH,\n",
    "    'num_heads': NUM_HEADS,\n",
    "    'dim_head': ATT_HEAD_DIM,\n",
    "    'cross_attn_every': CROOS_ATT_EVERY,\n",
    "    'media_token_id': MEDIA_TOKEN_ID,\n",
    "    'perceiver_num_latents': PERCEIVER_NUM_LATENTS,\n",
    "    'perceiver_depth': PERCEIVER_DEPTH,\n",
    "    'image_encoder': IMAGE_ENCODER,\n",
    "    'language_model': LANGUAGE_MODEL,\n",
    "    'pretrained_gpt2_path': PRETRAINED_GPT2_PATH,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /var/folders/61/9c2llh9n2pjb81c4dmhmb67w0000gn/T/tmp5mh91vu6\n",
      "INFO:torch.distributed.nn.jit.instantiator:Writing /var/folders/61/9c2llh9n2pjb81c4dmhmb67w0000gn/T/tmp5mh91vu6/_remote_module_non_sriptable.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained clip is being loaded\n",
      "Flamingo is being initialized with  gpt2  as language model\n",
      "GPT 2 Weights are loading...\n",
      "Loaded GPT2 weights and Embeddings num_weights loaded :  156\n"
     ]
    }
   ],
   "source": [
    "model = FlamingoModule(**hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iterator)\n",
    "out, attns = model(batch, return_attn = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1584, -0.3243,  0.0264,  ..., -0.2829, -0.5910, -0.0099],\n",
       "         [-0.3828, -0.0156, -0.4264,  ..., -0.8193, -0.1046, -0.1572],\n",
       "         [-0.4204,  0.7016,  0.0097,  ..., -0.2313,  0.3466,  0.1772],\n",
       "         ...,\n",
       "         [ 0.2086,  0.0754, -0.4519,  ...,  0.1039,  0.0590,  0.1050],\n",
       "         [-0.0104,  0.1238, -0.1079,  ..., -0.0618,  0.2761, -0.0398],\n",
       "         [ 0.3627,  0.2320, -0.0376,  ...,  0.2112,  0.1849,  0.1083]],\n",
       "\n",
       "        [[-0.4783,  0.7416,  0.2245,  ..., -0.3303,  0.0104,  0.4521],\n",
       "         [-0.5691, -0.1834, -0.1711,  ..., -1.0878,  0.3459, -0.4896],\n",
       "         [-0.5099,  0.6013,  0.1814,  ..., -0.2899,  0.3962,  0.0820],\n",
       "         ...,\n",
       "         [-0.1608,  0.3808, -0.1850,  ..., -0.5502, -0.4207,  0.1371],\n",
       "         [ 0.0264,  0.4373, -0.0424,  ..., -0.4474, -0.2347,  0.0716],\n",
       "         [ 0.2198,  0.3883,  0.2171,  ..., -0.6124, -0.4146,  0.1145]],\n",
       "\n",
       "        [[-0.3992,  0.6790,  0.1200,  ..., -0.4232, -0.1586,  0.4711],\n",
       "         [-0.7167, -0.4431, -0.1101,  ..., -0.5379,  0.2052,  0.0061],\n",
       "         [-0.5764,  0.6094, -0.0781,  ..., -0.2397,  0.5878,  0.2847],\n",
       "         ...,\n",
       "         [-0.0587,  0.0336, -0.3725,  ...,  0.1498,  0.1145, -0.1466],\n",
       "         [-0.2629, -0.2487, -0.3770,  ...,  0.0414,  0.0688,  0.1737],\n",
       "         [-0.2866,  0.1280, -0.4113,  ...,  0.1083,  0.3683,  0.1667]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.4748,  0.6576,  0.1719,  ..., -0.3339, -0.0845,  0.4431],\n",
       "         [-0.3836, -0.0477, -0.2475,  ..., -0.8630,  0.2166, -0.1168],\n",
       "         [-0.5936,  0.5470,  0.0225,  ..., -0.3542,  0.4633,  0.2736],\n",
       "         ...,\n",
       "         [-0.0117,  0.2208, -0.1400,  ..., -0.3955, -0.2231, -0.0870],\n",
       "         [-0.0898,  0.2084, -0.2031,  ..., -0.3554, -0.0907, -0.1742],\n",
       "         [ 0.3910,  0.0707,  0.0171,  ..., -0.1047, -0.0964, -0.1419]],\n",
       "\n",
       "        [[-0.5659,  0.4574,  0.0585,  ..., -0.4322, -0.1156,  0.4604],\n",
       "         [-0.4802,  0.4748, -0.2742,  ..., -1.0874,  0.5897,  0.4790],\n",
       "         [-0.6635,  0.8656,  0.2876,  ..., -0.3756,  0.1227,  0.4599],\n",
       "         ...,\n",
       "         [ 0.2206,  0.1554, -0.3722,  ..., -0.3322, -0.0893,  0.0454],\n",
       "         [ 0.6337, -0.0166,  0.3392,  ...,  0.0278, -0.1870, -0.4454],\n",
       "         [ 0.4807, -0.1610,  0.1452,  ..., -0.0378, -0.4108, -0.0787]],\n",
       "\n",
       "        [[-0.1793,  0.0681, -0.4745,  ..., -0.4990, -0.1789,  0.1409],\n",
       "         [-0.3995, -0.0952, -0.5204,  ..., -0.8112,  0.5072, -0.2267],\n",
       "         [-0.3378,  0.3880,  0.1896,  ..., -0.1286,  0.6254,  0.0523],\n",
       "         ...,\n",
       "         [-0.0118,  0.5804,  0.1483,  ..., -0.3207,  0.0105, -0.1328],\n",
       "         [-0.0682,  0.5423, -0.0024,  ..., -0.2872, -0.3422,  0.3437],\n",
       "         [ 0.2522,  0.7444,  0.2200,  ..., -0.2481,  0.1200, -0.3124]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from torchvision.transforms import transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[7, 7]' is invalid for input of size 2450",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/caghankoksal/Desktop/SS2022/mlmi-vqa/notebooks/mimic_cxr_FlamingoClipPalm_playground.ipynb Cell 22'\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/caghankoksal/Desktop/SS2022/mlmi-vqa/notebooks/mimic_cxr_FlamingoClipPalm_playground.ipynb#ch0000016?line=11'>12</a>\u001b[0m cur_attn_map \u001b[39m=\u001b[39m att_mat[\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m:]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/caghankoksal/Desktop/SS2022/mlmi-vqa/notebooks/mimic_cxr_FlamingoClipPalm_playground.ipynb#ch0000016?line=13'>14</a>\u001b[0m grid_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(np\u001b[39m.\u001b[39msqrt(cur_attn_map\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])) \u001b[39m# 49, 49\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/caghankoksal/Desktop/SS2022/mlmi-vqa/notebooks/mimic_cxr_FlamingoClipPalm_playground.ipynb#ch0000016?line=14'>15</a>\u001b[0m mask \u001b[39m=\u001b[39m cur_attn_map\u001b[39m.\u001b[39;49mreshape(grid_size, grid_size)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/caghankoksal/Desktop/SS2022/mlmi-vqa/notebooks/mimic_cxr_FlamingoClipPalm_playground.ipynb#ch0000016?line=16'>17</a>\u001b[0m im \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mToPILImage()(img\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/caghankoksal/Desktop/SS2022/mlmi-vqa/notebooks/mimic_cxr_FlamingoClipPalm_playground.ipynb#ch0000016?line=17'>18</a>\u001b[0m \u001b[39m# Maybe use interpolate instead of resize\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[7, 7]' is invalid for input of size 2450"
     ]
    }
   ],
   "source": [
    "# Attention Maps\n",
    "batch = next(iterator)\n",
    "out, attns = model(batch, return_attn = True)\n",
    "img = batch[\"image\"]\n",
    "# Stack attention maps\n",
    "att_mat = torch.stack(attns)\n",
    "att_mat = att_mat.squeeze(1)\n",
    "# Average the attention weights across all heads.\n",
    "#att_mat = torch.mean(att_mat, dim=0)\n",
    "\n",
    "# Take the Last attention map\n",
    "att_mat = att_mat[-1,:,:]\n",
    "#Use Classification Token's attention\n",
    "cur_attn_map = att_mat[0,1:]\n",
    "\n",
    "grid_size = int(np.sqrt(cur_attn_map.shape[0])) # 49, 49\n",
    "mask = cur_attn_map.reshape(grid_size, grid_size).detach().numpy()\n",
    "\n",
    "im = transforms.ToPILImage()(img.squeeze(0))\n",
    "# Maybe use interpolate instead of resize\n",
    "mask2 = cv2.resize(mask / mask.max(), im.size)[..., np.newaxis]\n",
    "result = (torch.tensor(mask2[:,:,0]) * img[0][0])\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 16))\n",
    "\n",
    "ax1.set_title('Original')\n",
    "ax2.set_title('Attention Map')\n",
    "_ = ax1.imshow(im)\n",
    "_ = ax2.imshow(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iterator)\n",
    "out, attns = model(batch, return_attn = True)\n",
    "# Attention Maps\n",
    "img = batch[\"image\"]\n",
    "# Stack attention maps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocess.visualize_attention import plot_attention_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from torchvision.transforms import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def plot_attention_maps2(attns, img, num_heads=12):\n",
    "\n",
    "    \n",
    "    fig, ax = plt.subplots(num_heads, 4)\n",
    "    fig.set_size_inches(18.5, 70)\n",
    "\n",
    "\n",
    "    # Stack attention maps\n",
    "    att_maps = torch.stack(attns)\n",
    "    att_maps = att_maps.squeeze(1)\n",
    "    # Average the attention weights across all heads.\n",
    "    #att_mat = torch.mean(att_mat, dim=0)\n",
    "\n",
    "\n",
    "    for j in range(num_heads):\n",
    "        #fname = os.path.join(args.output_dir, \"attn-head\" + str(j) + \".png\")\n",
    "        #plt.imsave(fname=fname, arr=attentions[j], format='png')\n",
    "        #print(f\"{fname} saved.\")\n",
    "\n",
    "        # Take the Last attention map\n",
    "        cur_attention_head = att_maps[j,:,:]\n",
    "        #Use Classification Token's attention\n",
    "        cur_attn_map = cur_attention_head[0,1:]\n",
    "\n",
    "        grid_size = int(np.sqrt(cur_attn_map.shape[0])) # 49, 49\n",
    "        mask = cur_attn_map.reshape(grid_size, grid_size).detach().numpy()\n",
    "\n",
    "        im = transforms.ToPILImage()(img.squeeze(0))\n",
    "        # Maybe use interpolate instead of resize\n",
    "        \n",
    "        interpol_mask = nn.functional.interpolate(torch.tensor(mask).unsqueeze(0).unsqueeze(1), scale_factor=32, mode=\"nearest\")\n",
    "        interpol_mask = interpol_mask[0,0,:,:].unsqueeze(2)\n",
    "        result_interpol = (torch.tensor(interpol_mask[:,:,0]) * img[0][0])\n",
    "        \n",
    "        resize_mask = cv2.resize(mask / mask.max(), im.size)[..., np.newaxis]\n",
    "        result_resize = (torch.tensor(resize_mask[:,:,0]) * img[0][0])\n",
    "\n",
    "\n",
    "        ax[j][0].imshow(im)\n",
    "        ax[j][0].set_title(\"Orginal\")\n",
    "        \n",
    "        \n",
    "        ax[j][1].imshow(im)\n",
    "        #ax[j][1].imshow(mask, cmap='hot', alpha=0.7)\n",
    "        ax[j][1].imshow(resize_mask, cmap='hot', alpha=0.7)\n",
    "        ax[j][1].set_title(\"Head Attention\" + str(j))\n",
    "        \n",
    "        ax[j][2].imshow(result_resize)\n",
    "        ax[j][2].set_title(\"Head result_resize Attention\" + str(j))\n",
    "\n",
    "        ax[j][3].imshow(resize_mask)\n",
    "        ax[j][3].set_title(\"Head resize_mask Attention\" + str(j))\n",
    "\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_maps2(attns, img, num_heads=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iterator)\n",
    "out, attns = model(batch, return_attn = True)\n",
    "# Attention Maps\n",
    "img = batch[\"image\"]\n",
    "# Stack attention maps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To account for residual connections, we add an identity matrix to the\n",
    "# attention matrix and re-normalize the weights.\n",
    "residual_att = torch.eye(att_mat.size(1))\n",
    "aug_att_mat = att_mat + residual_att\n",
    "aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1).unsqueeze(-1)\n",
    "\n",
    "# Recursively multiply the weight matrices\n",
    "joint_attentions = torch.zeros(aug_att_mat.size())\n",
    "joint_attentions[0] = aug_att_mat[0]\n",
    "im = transforms.ToPILImage()(img.squeeze(0))\n",
    "for n in range(1, aug_att_mat.size(0)):\n",
    "    joint_attentions[n] = torch.matmul(aug_att_mat[n], joint_attentions[n-1])\n",
    "print(\"JA shape: \",joint_attentions.shape)\n",
    "# Attention from the output token to the input space.\n",
    "\n",
    "grid_size = int(np.sqrt(aug_att_mat.size(-1)))\n",
    "mask = joint_attentions[0, 1:].reshape(grid_size, grid_size).detach().numpy()\n",
    "mask = cv2.resize(mask / mask.max(), im.size)[..., np.newaxis]\n",
    "result = (mask * im).astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 16))\n",
    "\n",
    "ax1.set_title('Original')\n",
    "ax2.set_title('Attention Map')\n",
    "_ = ax1.imshow(im)\n",
    "_ = ax2.imshow(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMET_API_KEY = \"F2L19mQwKXSoeF1IYEDA2AeHD\",\n",
    "PROJECT_KEY = \"flamingo-playground\",\n",
    "import os\n",
    "from pytorch_lightning.loggers import CometLogger\n",
    "\n",
    "comet_logger = CometLogger(\n",
    "    api_key= \"F2L19mQwKXSoeF1IYEDA2AeHD\",\n",
    "    project_name=\"flamingo-gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=6,\n",
    "                     accelerator=\"cpu\", devices=1,\n",
    "                     callbacks=[lr_monitor],\n",
    "                     log_every_n_steps=1,\n",
    "                      )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.utilities.model_summary import LayerSummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = LayerSummary(model.flamingo_palm.token_emb)\n",
    "summary.num_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from transformers import GPT2Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn as nn\n",
    "import torch.nn.functional as F\n",
    "def generate(image, context, ntok=20):\n",
    "    for _ in range(ntok):\n",
    "        out = model({'image': image,'input_ids': context })\n",
    "        logits = out[:, -1, :]\n",
    "        indices_to_remove = logits < torch.topk(logits, 10)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = np.NINF\n",
    "        next_tok = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1).squeeze(1)\n",
    "        context = torch.cat([context, next_tok.unsqueeze(-1)], dim=-1)\n",
    "    return context\n",
    "\n",
    "\n",
    "tokenizer = mimic_datamodule.train_dataset.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_folder_path = \"/Users/caghankoksal/Desktop/logs_from_cluster/lightning_logs_flamingo-gpt2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(ckpt_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder_name = '9_da477150ff4a4f50866683712eb8c707'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(os.path.join(ckpt_folder_path,model_folder_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(os.path.join(ckpt_folder_path,model_folder_name, \"checkpoints\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = os.path.join(ckpt_folder_path,model_folder_name,\"checkpoints\",'epoch=176-step=49206.ckpt')\n",
    "model.load_state_dict(torch.load(CHECKPOINT_PATH,map_location=torch.device('cpu'))[\"state_dict\"])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT_NUM_SAMPLES = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIMIC_CXR_DCM_PATH = '/Users/caghankoksal/Desktop/development/Flamingo-playground/physionet.org/files/mimic-cxr/2.0.0/files/'\n",
    "MIMIC_CXR_JPG_PATH = '/Users/caghankoksal/Desktop/development/physionet.org/files/mimic-cxr-jpg/2.0.0/files/'\n",
    "mimic_datamodule = MIMICCXRDataModule(MIMIC_CXR_DCM_PATH, MIMIC_CXR_JPG_PATH, \n",
    "                                      transforms=augmentations, only_images=False, batch_size=1,\n",
    "                                      limit_num_samples=LIMIT_NUM_SAMPLES, num_data_workers=0,\n",
    "                                      tokenizer=\"gpt2\",image_type=\"jpg\")\n",
    "val_loader = mimic_datamodule.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_val = iter(val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(load_val)\n",
    "print(batch[\"text\"]),\n",
    "img = T.ToPILImage()(batch[\"image\"][0])\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context   = torch.tensor([tokenizer.encode('<|endoftext|> <image> Output:')])\n",
    "out = generate(batch[\"image\"], context, ntok=180)\n",
    "tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context   = torch.tensor([tokenizer.encode('<|endoftext|> <image> Output: ')])\n",
    "out = generate(batch[\"image\"], context, ntok=180)\n",
    "tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context   = torch.tensor([tokenizer.encode('<|endoftext|> <image> Output: FINDINGS: ')])\n",
    "out = generate(batch[\"image\"], context, ntok=180)\n",
    "tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context   = torch.tensor([tokenizer.encode('<|endoftext|> <image> Question: Is there a penumothorax? If yes, explain why? Answer: ')])\n",
    "out = generate(batch[\"image\"], context, ntok=180)\n",
    "tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context   = torch.tensor([tokenizer.encode('<|endoftext|> <image> Output: ')])\n",
    "out = generate(batch[\"image\"], context, ntok=180)\n",
    "tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(load_val)\n",
    "print(batch[\"text\"]),\n",
    "img = T.ToPILImage()(batch[\"image\"][0])\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context   = torch.tensor([tokenizer.encode('<|endoftext|> <image> Output: ')])\n",
    "out = generate(batch[\"image\"], context, ntok=180)\n",
    "tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZE VIT ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEAM SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from queue import PriorityQueue\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "MAX_LENGTH = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearchNode(object):\n",
    "    def __init__(self, hiddenstate, previousNode, wordId, logProb, length):\n",
    "        '''\n",
    "        :param hiddenstate:\n",
    "        :param previousNode:\n",
    "        :param wordId:\n",
    "        :param logProb:\n",
    "        :param length:\n",
    "        '''\n",
    "        self.h = hiddenstate\n",
    "        self.prevNode = previousNode\n",
    "        self.wordid = wordId\n",
    "        self.logp = logProb\n",
    "        self.leng = length\n",
    "\n",
    "    def eval(self, alpha=1.0):\n",
    "        reward = 0\n",
    "        # Add here a function for shaping a reward\n",
    "\n",
    "        return self.logp / float(self.leng - 1 + 1e-6) + alpha * reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_decode(target_tensor, decoder_hiddens, encoder_outputs=None):\n",
    "    '''\n",
    "    :param target_tensor: target indexes tensor of shape [B, T] where B is the batch size and T is the maximum length of the output sentence\n",
    "    :param decoder_hidden: input tensor of shape [1, B, H] for start of the decoding\n",
    "    :param encoder_outputs: if you are using attention mechanism you can pass encoder outputs, [T, B, H] where T is the maximum length of input sentence\n",
    "    :return: decoded_batch\n",
    "    '''\n",
    "\n",
    "    beam_width = 10\n",
    "    topk = 1  # how many sentence do you want to generate\n",
    "    decoded_batch = []\n",
    "\n",
    "    # decoding goes sentence by sentence\n",
    "    for idx in range(target_tensor.size(0)):\n",
    "    \n",
    "        decoder_hidden = decoder_hiddens[:, idx, :].unsqueeze(0)\n",
    "        encoder_output = encoder_outputs[:,idx, :].unsqueeze(1)\n",
    "\n",
    "        # Start with the start of the sentence token\n",
    "        decoder_input = torch.LongTensor([[SOS_token]], device=device)\n",
    "\n",
    "        # Number of sentence to generate\n",
    "        endnodes = []\n",
    "        number_required = min((topk + 1), topk - len(endnodes))\n",
    "\n",
    "        # starting node -  hidden vector, previous node, word id, logp, length\n",
    "        node = BeamSearchNode(decoder_hidden, None, decoder_input, 0, 1)\n",
    "        nodes = PriorityQueue()\n",
    "\n",
    "        # start the queue\n",
    "        nodes.put((-node.eval(), node))\n",
    "        qsize = 1\n",
    "\n",
    "        # start beam search\n",
    "        while True:\n",
    "            # give up when decoding takes too long\n",
    "            if qsize > 2000: break\n",
    "\n",
    "            # fetch the best node\n",
    "            score, n = nodes.get()\n",
    "            decoder_input = n.wordid\n",
    "            decoder_hidden = n.h\n",
    "\n",
    "            if n.wordid.item() == EOS_token and n.prevNode != None:\n",
    "                endnodes.append((score, n))\n",
    "                # if we reached maximum # of sentences required\n",
    "                if len(endnodes) >= number_required:\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            # decode for one step using decoder\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "\n",
    "            # PUT HERE REAL BEAM SEARCH OF TOP\n",
    "            log_prob, indexes = torch.topk(decoder_output, beam_width)\n",
    "            nextnodes = []\n",
    "\n",
    "            for new_k in range(beam_width):\n",
    "                decoded_t = indexes[0][new_k].view(1, -1)\n",
    "                log_p = log_prob[0][new_k].item()\n",
    "\n",
    "                node = BeamSearchNode(decoder_hidden, n, decoded_t, n.logp + log_p, n.leng + 1)\n",
    "                score = -node.eval()\n",
    "                nextnodes.append((score, node))\n",
    "\n",
    "            # put them into queue\n",
    "            for i in range(len(nextnodes)):\n",
    "                score, nn = nextnodes[i]\n",
    "                nodes.put((score, nn))\n",
    "                # increase qsize\n",
    "            qsize += len(nextnodes) - 1\n",
    "\n",
    "        # choose nbest paths, back trace them\n",
    "        if len(endnodes) == 0:\n",
    "            endnodes = [nodes.get() for _ in range(topk)]\n",
    "\n",
    "        utterances = []\n",
    "        for score, n in sorted(endnodes, key=operator.itemgetter(0)):\n",
    "            utterance = []\n",
    "            utterance.append(n.wordid)\n",
    "            # back trace\n",
    "            while n.prevNode != None:\n",
    "                n = n.prevNode\n",
    "                utterance.append(n.wordid)\n",
    "\n",
    "            utterance = utterance[::-1]\n",
    "            utterances.append(utterance)\n",
    "\n",
    "        decoded_batch.append(utterances)\n",
    "\n",
    "    return decoded_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('torch-nightly')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "892a7f8aeabe86b99d45932805d162784b758c544538f3ce4737e4a115db3cfd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
