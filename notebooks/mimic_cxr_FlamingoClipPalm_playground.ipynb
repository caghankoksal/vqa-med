{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('..')\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mlmi-matthias/anaconda3/envs/mlmi_caghan/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from src.datasets.mimic_cxr_dataset import MIMICCXRDataModule\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "# sets seeds for numpy, torch, python.random and PYTHONHASHSEED.\n",
    "seed_everything(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.mimic_cxr_rachet import MIMICCXRRachetDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "src.datasets.mimic_cxr_rachet.MIMICCXRRachetDataModule"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rachet_hyperparameters = {\n",
    "    \n",
    "    \n",
    "}\n",
    "MIMICCXRRachetDataModule(abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_indices = pd.read_csv('MIMIC_AP_PA_validate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(validate_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_indices.iloc[0][\"img_path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_indices.iloc[0][\"report\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentations = {'train':\n",
    "        T.Compose(\n",
    "        [\n",
    "        # T.Resize((224, 224)),\n",
    "        # T.ToTensor(),\n",
    "        \n",
    "            T.RandomRotation(10),\n",
    "            T.ToTensor(),\n",
    "            #T.Normalize(mean=(0.4719, 0.4719, 0.4719), std=(0.3029, 0.3029, 0.3029))\n",
    "        ]),\n",
    "        'val':\n",
    "        T.Compose(\n",
    "        [\n",
    "            #T.Resize((224, 224)),\n",
    "            #T.ToTensor(),\n",
    "            #T.RandomRotation(10),\n",
    "            T.ToTensor(),\n",
    "            #T.Normalize(mean=(0.4719, 0.4719, 0.4719), std=(0.3029, 0.3029, 0.3029))\n",
    "        ]),\n",
    "        'test':\n",
    "        T.Compose(\n",
    "        [\n",
    "            #T.Resize((224, 224)),\n",
    "            #T.ToTensor(),\n",
    "            T.RandomRotation(10),\n",
    "            T.ToTensor(),\n",
    "            #T.Normalize(mean=(0.4719, 0.4719, 0.4719), std=(0.3029, 0.3029, 0.3029))\n",
    "        ])\n",
    "    }\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "NUM_DATA_WORKERS  = 4\n",
    "ONLY_IMAGES = False\n",
    "BATCH_SIZE = 1\n",
    "NUM_EPOCHS = 200\n",
    "LIMIT_NUM_SAMPLES = 5000\n",
    "\n",
    "if os.getcwd().startswith('/home/mlmi-matthias'):\n",
    "    ACCELERATOR = \"gpu\"\n",
    "    DEVICES = [4,5,6,7]\n",
    "    PRETRAINED_CLIP_PATH = '/home/mlmi-matthias/Caghan/pretrained_models/PubMedCLIP_ViT32.pth'\n",
    "    PRETRAINED_GPT2_PATH = \"/home/mlmi-matthias/Caghan/pretrained_models/gpt2-pytorch_model.bin\"\n",
    "    MIMIC_CXR_DCM_PATH = '/home/mlmi-matthias/physionet.org/files/mimic-cxr/2.0.0/files/'\n",
    "    MIMIC_CXR_JPG_PATH = \"/home/mlmi-matthias/physionet.org/files/mimic-cxr-jpg/2.0.0/files/\"\n",
    "    SPLIT_PATH = '/home/mlmi-matthias/Caghan/mlmi-vqa/data/external/'\n",
    "\n",
    "elif os.getcwd().startswith('/Users/caghankoksal'):\n",
    "    PRETRAINED_CLIP_PATH = '/Users/caghankoksal/Desktop/development/PubMedCLIP_ViT32.pth'\n",
    "    PRETRAINED_GPT2_PATH = \"/Users/caghankoksal/Desktop/development/TransformerPlay/gpt2-pytorch_model.bin\"\n",
    "    ACCELERATOR = \"cpu\"\n",
    "    DEVICES = 1\n",
    "    MIMIC_CXR_DCM_PATH = '/Users/caghankoksal/Desktop/development/Flamingo-playground/physionet.org/files/mimic-cxr/2.0.0/files/'\n",
    "    MIMIC_CXR_JPG_PATH = '/Users/caghankoksal/Desktop/development/physionet.org/files/mimic-cxr-jpg/2.0.0/files/'\n",
    "    SPLIT_PATH = '/Users/caghankoksal/Desktop/SS2022/mlmi-vqa/data/external/'\n",
    "\n",
    "\n",
    "IMAGE_TYPE = \"jpg\"\n",
    "TOKENIZER  = \"gpt2\"\n",
    "PREPROCESSED = True\n",
    "\n",
    "mimic_datamodule = MIMICCXRDataModule(MIMIC_CXR_DCM_PATH, MIMIC_CXR_JPG_PATH, \n",
    "                                    transforms=augmentations, only_images=False, batch_size=BATCH_SIZE,\n",
    "                                    limit_num_samples=LIMIT_NUM_SAMPLES, num_data_workers=NUM_DATA_WORKERS,\n",
    "                                    tokenizer=\"gpt2\",image_type=\"jpg\", split_path=SPLIT_PATH, preprocessed=PREPROCESSED\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = mimic_datamodule.train_dataloader()\n",
    "val_loader = mimic_datamodule.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_datamodule.train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mimic_datamodule.train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mimic_datamodule.validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mimic_datamodule.test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    print(\"Image shape : \",batch[\"image\"].shape)\n",
    "    print(batch[\"text\"][0]) # First sample in the batch\n",
    "    print(\"Input Ids shape : \",batch[\"input_ids\"].shape)\n",
    "    print(\"Targets shape : \",batch[\"targets\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.multimodal.flamingo_module import FlamingoModule\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE_OF_TOKENIZER = 50257 # mimic_datamodule.train_dataset.tokenizer.vocab_size\n",
    "LANGUAGE_MODEL = 'gpt2'\n",
    "NUM_TOKENS = VOCAB_SIZE_OF_TOKENIZER +3 if LANGUAGE_MODEL==\"gpt2\" else 31092\n",
    "FLAMINGO_EMBED_DIM = 768\n",
    "DEPTH = 12\n",
    "NUM_HEADS = 8\n",
    "ATT_HEAD_DIM = 64\n",
    "CROOS_ATT_EVERY=3\n",
    "MEDIA_TOKEN_ID = mimic_datamodule.train_dataset.tokenizer.all_special_ids[mimic_datamodule.train_dataset.tokenizer.all_special_tokens.index('<image>')]\n",
    "PERCEIVER_NUM_LATENTS = 64\n",
    "PERCEIVER_DEPTH = 2\n",
    "IMAGE_ENCODER = \"clip\"\n",
    "\n",
    "\n",
    "print(  \"VOCAB_SIZE_OF_TOKENIZER : \",VOCAB_SIZE_OF_TOKENIZER, \"\\n\"\n",
    "        \"LANGUAGE_MODEL : \",LANGUAGE_MODEL, \"\\n\"\n",
    "        \"NUM_TOKENS : \",NUM_TOKENS, \"\\n\"\n",
    "        \"FLAMINGO_EMBED_DIM : \",FLAMINGO_EMBED_DIM, \"\\n\"\n",
    "        \"DEPTH : \",DEPTH, \"\\n\"\n",
    "        \"NUM_HEADS : \",NUM_HEADS, \"\\n\"\n",
    "        \"ATT_HEAD_DIM : \",ATT_HEAD_DIM, \"\\n\"\n",
    "        \"CROOS_ATT_EVERY : \",CROOS_ATT_EVERY, \"\\n\"\n",
    "        \"MEDIA_TOKEN_ID : \",MEDIA_TOKEN_ID, \"\\n\"\n",
    "        \"PERCEIVER_NUM_LATENTS : \",PERCEIVER_NUM_LATENTS, \"\\n\"\n",
    "        \"PERCEIVER_DEPTH : \",PERCEIVER_DEPTH, \"\\n\"\n",
    "        \"IMAGE_ENCODER : \",IMAGE_ENCODER, \"\\n\"\n",
    "        \"PRETRAINED_CLIP_PATH : \",PRETRAINED_CLIP_PATH, \"\\n\"\n",
    "        \"PRETRAINED_GPT2_PATH : \",PRETRAINED_GPT2_PATH, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'pretrained_clip_path': PRETRAINED_CLIP_PATH,\n",
    "    'warmup_steps': 569,\n",
    "    'num_tokens': NUM_TOKENS,\n",
    "    'dim': FLAMINGO_EMBED_DIM,\n",
    "    'depth': DEPTH,\n",
    "    'num_heads': NUM_HEADS,\n",
    "    'dim_head': ATT_HEAD_DIM,\n",
    "    'cross_attn_every': CROOS_ATT_EVERY,\n",
    "    'media_token_id': MEDIA_TOKEN_ID,\n",
    "    'perceiver_num_latents': PERCEIVER_NUM_LATENTS,\n",
    "    'perceiver_depth': PERCEIVER_DEPTH,\n",
    "    'image_encoder': IMAGE_ENCODER,\n",
    "    'language_model': LANGUAGE_MODEL,\n",
    "    'pretrained_gpt2_path': PRETRAINED_GPT2_PATH,\n",
    "    \"classification_mode\": False,\n",
    "    \"use_positional_embedding\":False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.multimodal.flamingo_module import FlamingoModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FlamingoModule(**hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iterator)\n",
    "out, attns = model(batch, return_attn = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from torchvision.transforms import transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Maps\n",
    "batch = next(iterator)\n",
    "out, attns = model(batch, return_attn = True)\n",
    "img = batch[\"image\"]\n",
    "# Stack attention maps\n",
    "att_mat = torch.stack(attns)\n",
    "att_mat = att_mat.squeeze(1)\n",
    "# Average the attention weights across all heads.\n",
    "#att_mat = torch.mean(att_mat, dim=0)\n",
    "\n",
    "# Take the Last attention map\n",
    "att_mat = att_mat[-1,:,:]\n",
    "#Use Classification Token's attention\n",
    "cur_attn_map = att_mat[0,1:]\n",
    "\n",
    "grid_size = int(np.sqrt(cur_attn_map.shape[0])) # 49, 49\n",
    "mask = cur_attn_map.reshape(grid_size, grid_size).detach().numpy()\n",
    "\n",
    "im = transforms.ToPILImage()(img.squeeze(0))\n",
    "# Maybe use interpolate instead of resize\n",
    "mask2 = cv2.resize(mask / mask.max(), im.size)[..., np.newaxis]\n",
    "result = (torch.tensor(mask2[:,:,0]) * img[0][0])\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 16))\n",
    "\n",
    "ax1.set_title('Original')\n",
    "ax2.set_title('Attention Map')\n",
    "_ = ax1.imshow(im)\n",
    "_ = ax2.imshow(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iterator)\n",
    "out, attns = model(batch, return_attn = True)\n",
    "# Attention Maps\n",
    "img = batch[\"image\"]\n",
    "# Stack attention maps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocess.visualize_attention import plot_attention_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from torchvision.transforms import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def plot_attention_maps2(attns, img, num_heads=12):\n",
    "\n",
    "    \n",
    "    fig, ax = plt.subplots(num_heads, 4)\n",
    "    fig.set_size_inches(18.5, 70)\n",
    "\n",
    "\n",
    "    # Stack attention maps\n",
    "    att_maps = torch.stack(attns)\n",
    "    att_maps = att_maps.squeeze(1)\n",
    "    # Average the attention weights across all heads.\n",
    "    #att_mat = torch.mean(att_mat, dim=0)\n",
    "\n",
    "\n",
    "    for j in range(num_heads):\n",
    "        #fname = os.path.join(args.output_dir, \"attn-head\" + str(j) + \".png\")\n",
    "        #plt.imsave(fname=fname, arr=attentions[j], format='png')\n",
    "        #print(f\"{fname} saved.\")\n",
    "\n",
    "        # Take the Last attention map\n",
    "        cur_attention_head = att_maps[j,:,:]\n",
    "        #Use Classification Token's attention\n",
    "        cur_attn_map = cur_attention_head[0,1:]\n",
    "\n",
    "        grid_size = int(np.sqrt(cur_attn_map.shape[0])) # 49, 49\n",
    "        mask = cur_attn_map.reshape(grid_size, grid_size).detach().numpy()\n",
    "\n",
    "        im = transforms.ToPILImage()(img.squeeze(0))\n",
    "        # Maybe use interpolate instead of resize\n",
    "        \n",
    "        interpol_mask = nn.functional.interpolate(torch.tensor(mask).unsqueeze(0).unsqueeze(1), scale_factor=32, mode=\"nearest\")\n",
    "        interpol_mask = interpol_mask[0,0,:,:].unsqueeze(2)\n",
    "        result_interpol = (torch.tensor(interpol_mask[:,:,0]) * img[0][0])\n",
    "        \n",
    "        resize_mask = cv2.resize(mask / mask.max(), im.size)[..., np.newaxis]\n",
    "        result_resize = (torch.tensor(resize_mask[:,:,0]) * img[0][0])\n",
    "\n",
    "\n",
    "        ax[j][0].imshow(im)\n",
    "        ax[j][0].set_title(\"Orginal\")\n",
    "        \n",
    "        \n",
    "        ax[j][1].imshow(im)\n",
    "        #ax[j][1].imshow(mask, cmap='hot', alpha=0.7)\n",
    "        ax[j][1].imshow(resize_mask, cmap='hot', alpha=0.7)\n",
    "        ax[j][1].set_title(\"Head Attention\" + str(j))\n",
    "        \n",
    "        ax[j][2].imshow(result_resize)\n",
    "        ax[j][2].set_title(\"Head result_resize Attention\" + str(j))\n",
    "\n",
    "        ax[j][3].imshow(resize_mask)\n",
    "        ax[j][3].set_title(\"Head resize_mask Attention\" + str(j))\n",
    "\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_maps2(attns, img, num_heads=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iterator)\n",
    "out, attns = model(batch, return_attn = True)\n",
    "# Attention Maps\n",
    "img = batch[\"image\"]\n",
    "# Stack attention maps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To account for residual connections, we add an identity matrix to the\n",
    "# attention matrix and re-normalize the weights.\n",
    "residual_att = torch.eye(att_mat.size(1))\n",
    "aug_att_mat = att_mat + residual_att\n",
    "aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1).unsqueeze(-1)\n",
    "\n",
    "# Recursively multiply the weight matrices\n",
    "joint_attentions = torch.zeros(aug_att_mat.size())\n",
    "joint_attentions[0] = aug_att_mat[0]\n",
    "im = transforms.ToPILImage()(img.squeeze(0))\n",
    "for n in range(1, aug_att_mat.size(0)):\n",
    "    joint_attentions[n] = torch.matmul(aug_att_mat[n], joint_attentions[n-1])\n",
    "print(\"JA shape: \",joint_attentions.shape)\n",
    "# Attention from the output token to the input space.\n",
    "\n",
    "grid_size = int(np.sqrt(aug_att_mat.size(-1)))\n",
    "mask = joint_attentions[0, 1:].reshape(grid_size, grid_size).detach().numpy()\n",
    "mask = cv2.resize(mask / mask.max(), im.size)[..., np.newaxis]\n",
    "result = (mask * im).astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 16))\n",
    "\n",
    "ax1.set_title('Original')\n",
    "ax2.set_title('Attention Map')\n",
    "_ = ax1.imshow(im)\n",
    "_ = ax2.imshow(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMET_API_KEY = \"F2L19mQwKXSoeF1IYEDA2AeHD\",\n",
    "PROJECT_KEY = \"flamingo-playground\",\n",
    "import os\n",
    "from pytorch_lightning.loggers import CometLogger\n",
    "\n",
    "comet_logger = CometLogger(\n",
    "    api_key= \"F2L19mQwKXSoeF1IYEDA2AeHD\",\n",
    "    project_name=\"flamingo-gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "#lr_monitor = LearningRateMonitor(logging_interval='step')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trainer = pl.Trainer(max_epochs=6,\n",
    "                     accelerator=\"cpu\", devices=1,\n",
    "                     callbacks=[lr_monitor],\n",
    "                     log_every_n_steps=1,\n",
    "                      )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!tensorboard --logdir=lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pytorch_lightning.utilities.model_summary import LayerSummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary = LayerSummary(model.flamingo_palm.token_emb)\n",
    "#summary.num_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from transformers import GPT2Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn as nn\n",
    "import torch.nn.functional as F\n",
    "def generate(image, context, ntok=20):\n",
    "    for _ in range(ntok):\n",
    "        out = model({'image': image,'input_ids': context })\n",
    "        logits = out[:, -1, :]\n",
    "        indices_to_remove = logits < torch.topk(logits, 10)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = np.NINF\n",
    "        next_tok = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1).squeeze(1)\n",
    "        # EOC\n",
    "        if next_tok ==  50259:\n",
    "            break\n",
    "        context = torch.cat([context, next_tok.unsqueeze(-1)], dim=-1)\n",
    "    return context\n",
    "\n",
    "\n",
    "tokenizer = mimic_datamodule.train_dataset.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_FROM_CHECKPOINT = True\n",
    "\n",
    "if START_FROM_CHECKPOINT:\n",
    "    # MIMIC CXR trained without positional embedding\n",
    "    CHECKPOINT_PATH = '/home/mlmi-matthias/Caghan/mlmi-vqa/notebooks/lightning_logs/version_20/checkpoints/epoch=114-val_loss=0.84-other_metric=0.00.ckpt'\n",
    "    print(\"Pretrained Flamingo Model is loaded from checkpoint : \",CHECKPOINT_PATH)\n",
    "    if os.getcwd().startswith('/home/mlmi-matthias'):\n",
    "        model.load_state_dict(torch.load(CHECKPOINT_PATH)[\"state_dict\"],strict=False)\n",
    "    else:\n",
    "        ckpt_folder_path = \"/Users/caghankoksal/Desktop/logs_from_cluster/lightning_logs_flamingo-gpt2/\"\n",
    "        os.listdir(ckpt_folder_path)\n",
    "        model_folder_name = '9_da477150ff4a4f50866683712eb8c707'\n",
    "        os.listdir(os.path.join(ckpt_folder_path,model_folder_name))\n",
    "        os.listdir(os.path.join(ckpt_folder_path,model_folder_name, \"checkpoints\"))\n",
    "        CHECKPOINT_PATH = os.path.join(ckpt_folder_path,model_folder_name,\"checkpoints\",'epoch=176-step=49206.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_val = iter(val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(load_val)\n",
    "print(batch[\"text\"]),\n",
    "img = T.ToPILImage()(batch[\"image\"][0])\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context   = torch.tensor([tokenizer.encode('<|endoftext|> <image> Output:')])\n",
    "out = generate(batch[\"image\"], context, ntok=180)\n",
    "tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context   = torch.tensor([tokenizer.encode('<|endoftext|> <image> Output: ')])\n",
    "out = generate(batch[\"image\"], context, ntok=180)\n",
    "tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context   = torch.tensor([tokenizer.encode('<|endoftext|> <image> Output: FINDINGS: ')])\n",
    "out = generate(batch[\"image\"], context, ntok=180)\n",
    "tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context   = torch.tensor([tokenizer.encode('<|endoftext|> <image> Question: Is there a penumothorax? If yes, explain why? Answer: ')])\n",
    "out = generate(batch[\"image\"], context, ntok=180)\n",
    "tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context   = torch.tensor([tokenizer.encode('<|endoftext|> <image> Output: ')])\n",
    "out = generate(batch[\"image\"], context, ntok=180)\n",
    "tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(load_val)\n",
    "print(batch[\"text\"]),\n",
    "img = T.ToPILImage()(batch[\"image\"][0])\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context   = torch.tensor([tokenizer.encode('<|endoftext|> <image> Output: ')])\n",
    "out = generate(batch[\"image\"], context, ntok=180)\n",
    "tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gts_report = batch[\"text\"][0].split('<|endoftext|> <image> Output:')[1].split('<EOC>')[0].rstrip().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_model = tokenizer.decode(out[0]).split('<|endoftext|> <image> Output:')[1].split('<EOC>')[0].rstrip().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gts ={'img_id': gts_report}\n",
    "res = {'img_id': res_model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXT EVALUATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pycocoevalcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = mimic_datamodule.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gts = {}\n",
    "preds = {}\n",
    "for i,batch in tqdm(enumerate(val_loader)):\n",
    "    context   = torch.tensor([tokenizer.encode('<|endoftext|> <image> Output: ')])\n",
    "    out = generate(batch[\"image\"], context, ntok=180)\n",
    "    models_output = tokenizer.decode(out[0])\n",
    "\n",
    "    preds[i] = [models_output.split('<|endoftext|> <image> Output:')[1].split('<EOC>')[0].rstrip().strip()]\n",
    "    \n",
    "    gts[i] = [batch[\"text\"][0].split('<|endoftext|> <image> Output:')[1].split('<EOC>')[0].rstrip().strip()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('preds_mimic_cxr_5000samples', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('gts_mimic_cxr_5000samples.npy', gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocoevalcap.bleu.bleu import Bleu\n",
    "from pycocoevalcap.meteor.meteor import Meteor\n",
    "from pycocoevalcap.rouge.rouge import Rouge\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "from pycocoevalcap.spice.spice import Spice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scorers = [\n",
    "    (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"]),\n",
    "    #(Meteor(),\"METEOR\"),\n",
    "    (Rouge(), \"ROUGE_L\"),\n",
    "    (Cider(), \"CIDEr\"),\n",
    "    #(Spice(), \"SPICE\")\n",
    "        ]\n",
    "\n",
    "# =================================================\n",
    "# Compute scores\n",
    "# =================================================\n",
    "for scorer, method in scorers:\n",
    "    print('computing %s score...'%(scorer.method()))\n",
    "    score, scores = scorer.compute_score(gts, preds)\n",
    "    if type(method) == list:\n",
    "        for sc, scs, m in zip(score, scores, method):\n",
    "            #self.setEval(sc, m)\n",
    "            #self.setImgToEvalImgs(scs, gts.keys(), m)\n",
    "            print(\"%s: %0.3f\"%(m, sc))\n",
    "    else:\n",
    "        #self.setEval(score, method)\n",
    "        #self.setImgToEvalImgs(scores, gts.keys(), method)\n",
    "        print(\"%s: %0.3f\"%(method, score))\n",
    "#self.setEvalImgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIMICEvalCap:\n",
    "    def __init__(self, true_gt, preds):\n",
    "\n",
    "        self.preds = preds\n",
    "        self.true_gts = true_gt\n",
    "\n",
    "        self.eval = dict()\n",
    "        self.imgToEval = dict()\n",
    "\n",
    "    def preprocess(self, s):\n",
    "        s = s.replace('\\n', '')\n",
    "        s = s.replace('<s>', '')\n",
    "        s = s.replace('</s>', '')\n",
    "        # s = s.translate(str.maketrans('', '', '0123456789'))\n",
    "        # s = s.translate(str.maketrans('', '', string.punctuation))\n",
    "        return s\n",
    "\n",
    "    def evaluate(self):\n",
    "\n",
    "        gts = dict()\n",
    "        res = dict()\n",
    "\n",
    "        # Sanity Checks\n",
    "        assert len(self.preds.keys()) == len(self.true_gts.keys())\n",
    "\n",
    "        # =================================================\n",
    "        # Pre-process sentences\n",
    "        # =================================================\n",
    "        #print('tokenization...')\n",
    "        #for i in range(self.pred_df.shape[0]):\n",
    "        #    pred_text = ' '.join(word_tokenize(self.preprocess(self.pred_df[i][0])))\n",
    "        #    true_text = ' '.join(word_tokenize(self.preprocess(self.true_df[i][0])))\n",
    "\n",
    "        #    res[i] = [pred_text]\n",
    "        #    gts[i] = [true_text]\n",
    "        \n",
    "        res = self.preds\n",
    "        gts = self.true_gts\n",
    "\n",
    "        # =================================================\n",
    "        # Set up scorers\n",
    "        # =================================================\n",
    "        print('setting up scorers...')\n",
    "        scorers = [\n",
    "            (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"]),\n",
    "            (Meteor(),\"METEOR\"),\n",
    "            (Rouge(), \"ROUGE_L\"),\n",
    "            (Cider(), \"CIDEr\"),\n",
    "            (Spice(), \"SPICE\")\n",
    "        ]\n",
    "\n",
    "        # =================================================\n",
    "        # Compute scores\n",
    "        # =================================================\n",
    "        for scorer, method in scorers:\n",
    "            print('computing %s score...'%(scorer.method()))\n",
    "            score, scores = scorer.compute_score(gts, res)\n",
    "            if type(method) == list:\n",
    "                for sc, scs, m in zip(score, scores, method):\n",
    "                    self.setEval(sc, m)\n",
    "                    self.setImgToEvalImgs(scs, gts.keys(), m)\n",
    "                    print(\"%s: %0.3f\"%(m, sc))\n",
    "            else:\n",
    "                self.setEval(score, method)\n",
    "                self.setImgToEvalImgs(scores, gts.keys(), method)\n",
    "                print(\"%s: %0.3f\"%(method, score))\n",
    "        self.setEvalImgs()\n",
    "\n",
    "    def setEval(self, score, method):\n",
    "        self.eval[method] = score\n",
    "\n",
    "    def setImgToEvalImgs(self, scores, imgIds, method):\n",
    "        for imgId, score in zip(imgIds, scores):\n",
    "            if not imgId in self.imgToEval:\n",
    "                self.imgToEval[imgId] = dict()\n",
    "                self.imgToEval[imgId][\"image_id\"] = imgId\n",
    "            self.imgToEval[imgId][method] = score\n",
    "\n",
    "    def setEvalImgs(self):\n",
    "        self.evalImgs = [eval for imgId, eval in self.imgToEval.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIMICEvalCap(gts, preds).evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEAM SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from queue import PriorityQueue\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "MAX_LENGTH = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearchNode(object):\n",
    "    def __init__(self, hiddenstate, previousNode, wordId, logProb, length):\n",
    "        '''\n",
    "        :param hiddenstate:\n",
    "        :param previousNode:\n",
    "        :param wordId:\n",
    "        :param logProb:\n",
    "        :param length:\n",
    "        '''\n",
    "        self.h = hiddenstate\n",
    "        self.prevNode = previousNode\n",
    "        self.wordid = wordId\n",
    "        self.logp = logProb\n",
    "        self.leng = length\n",
    "\n",
    "    def eval(self, alpha=1.0):\n",
    "        reward = 0\n",
    "        # Add here a function for shaping a reward\n",
    "\n",
    "        return self.logp / float(self.leng - 1 + 1e-6) + alpha * reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_decode(target_tensor, decoder_hiddens, encoder_outputs=None):\n",
    "    '''\n",
    "    :param target_tensor: target indexes tensor of shape [B, T] where B is the batch size and T is the maximum length of the output sentence\n",
    "    :param decoder_hidden: input tensor of shape [1, B, H] for start of the decoding\n",
    "    :param encoder_outputs: if you are using attention mechanism you can pass encoder outputs, [T, B, H] where T is the maximum length of input sentence\n",
    "    :return: decoded_batch\n",
    "    '''\n",
    "\n",
    "    beam_width = 10\n",
    "    topk = 1  # how many sentence do you want to generate\n",
    "    decoded_batch = []\n",
    "\n",
    "    # decoding goes sentence by sentence\n",
    "    for idx in range(target_tensor.size(0)):\n",
    "    \n",
    "        decoder_hidden = decoder_hiddens[:, idx, :].unsqueeze(0)\n",
    "        encoder_output = encoder_outputs[:,idx, :].unsqueeze(1)\n",
    "\n",
    "        # Start with the start of the sentence token\n",
    "        decoder_input = torch.LongTensor([[SOS_token]], device=device)\n",
    "\n",
    "        # Number of sentence to generate\n",
    "        endnodes = []\n",
    "        number_required = min((topk + 1), topk - len(endnodes))\n",
    "\n",
    "        # starting node -  hidden vector, previous node, word id, logp, length\n",
    "        node = BeamSearchNode(decoder_hidden, None, decoder_input, 0, 1)\n",
    "        nodes = PriorityQueue()\n",
    "\n",
    "        # start the queue\n",
    "        nodes.put((-node.eval(), node))\n",
    "        qsize = 1\n",
    "\n",
    "        # start beam search\n",
    "        while True:\n",
    "            # give up when decoding takes too long\n",
    "            if qsize > 2000: break\n",
    "\n",
    "            # fetch the best node\n",
    "            score, n = nodes.get()\n",
    "            decoder_input = n.wordid\n",
    "            decoder_hidden = n.h\n",
    "\n",
    "            if n.wordid.item() == EOS_token and n.prevNode != None:\n",
    "                endnodes.append((score, n))\n",
    "                # if we reached maximum # of sentences required\n",
    "                if len(endnodes) >= number_required:\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            # decode for one step using decoder\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "\n",
    "            # PUT HERE REAL BEAM SEARCH OF TOP\n",
    "            log_prob, indexes = torch.topk(decoder_output, beam_width)\n",
    "            nextnodes = []\n",
    "\n",
    "            for new_k in range(beam_width):\n",
    "                decoded_t = indexes[0][new_k].view(1, -1)\n",
    "                log_p = log_prob[0][new_k].item()\n",
    "\n",
    "                node = BeamSearchNode(decoder_hidden, n, decoded_t, n.logp + log_p, n.leng + 1)\n",
    "                score = -node.eval()\n",
    "                nextnodes.append((score, node))\n",
    "\n",
    "            # put them into queue\n",
    "            for i in range(len(nextnodes)):\n",
    "                score, nn = nextnodes[i]\n",
    "                nodes.put((score, nn))\n",
    "                # increase qsize\n",
    "            qsize += len(nextnodes) - 1\n",
    "\n",
    "        # choose nbest paths, back trace them\n",
    "        if len(endnodes) == 0:\n",
    "            endnodes = [nodes.get() for _ in range(topk)]\n",
    "\n",
    "        utterances = []\n",
    "        for score, n in sorted(endnodes, key=operator.itemgetter(0)):\n",
    "            utterance = []\n",
    "            utterance.append(n.wordid)\n",
    "            # back trace\n",
    "            while n.prevNode != None:\n",
    "                n = n.prevNode\n",
    "                utterance.append(n.wordid)\n",
    "\n",
    "            utterance = utterance[::-1]\n",
    "            utterances.append(utterance)\n",
    "\n",
    "        decoded_batch.append(utterances)\n",
    "\n",
    "    return decoded_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Attention Map and Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_val = iter(val_loader)\n",
    "batch = next(load_val)\n",
    "out, attns = model(batch, return_attn = True)\n",
    "# Attention Maps\n",
    "img = batch[\"image\"]\n",
    "# Stack attention maps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context   = torch.tensor([tokenizer.encode('<|endoftext|> <image> Output: ')])\n",
    "out = generate(batch[\"image\"], context, ntok=180)\n",
    "tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_maps2(attns, img, num_heads=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlmi_caghan",
   "language": "python",
   "name": "mlmi_caghan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "892a7f8aeabe86b99d45932805d162784b758c544538f3ce4737e4a115db3cfd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
