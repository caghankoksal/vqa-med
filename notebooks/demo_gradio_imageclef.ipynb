{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('..')\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "from src.models.multimodal.flamingo_module import FlamingoModule\n",
    "from src.datasets.imageclef_dataset import ImageCLEF2021DataModule\n",
    "from src.utils.utils import load_flamingo_weights, print_hyperparams\n",
    "\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "import torchvision.transforms as T\n",
    "from pytorch_lightning.loggers import CometLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from transformers import GPT2Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len training dataset :  4500 Batch Size :  1 NUM_EPOCHS :  120\n",
      "Total training steps :  540000\n"
     ]
    }
   ],
   "source": [
    "augmentations = {\n",
    "        \n",
    "        'train': T.Compose([T.Resize((224,224)),\n",
    "                            T.ToTensor(),\n",
    "                            T.RandomHorizontalFlip(p=0.5),\n",
    "                            T.RandomRotation(degrees=10),\n",
    "                            T.Normalize(mean=(0.2570, 0.2570, 0.2570), std=(0.2710, 0.2710, 0.2710))\n",
    "                            ]),\n",
    "        'val': T.Compose([T.Resize((224,224)),\n",
    "                            T.ToTensor(),\n",
    "                            T.RandomHorizontalFlip(p=0.5),\n",
    "                            T.RandomRotation(degrees=10),\n",
    "                            T.Normalize(mean=(0.2570, 0.2570, 0.2570), std=(0.2710, 0.2710, 0.2710))\n",
    "                            ]),\n",
    "        'test': T.Compose([T.Resize((224,224)),\n",
    "                            T.ToTensor(),\n",
    "                            T.RandomHorizontalFlip(p=0.5),\n",
    "                            T.RandomRotation(degrees=10),   \n",
    "                            T.Normalize(mean=(0.2570, 0.2570, 0.2570), std=(0.2710, 0.2710, 0.2710))\n",
    "                            ])\n",
    "    }\n",
    "\n",
    "    \n",
    "# Hyperparameters\n",
    "NUM_DATA_WORKERS  = 2\n",
    "ONLY_IMAGES = False\n",
    "BATCH_SIZE = 1\n",
    "NUM_EPOCHS = 120\n",
    "LIMIT_NUM_SAMPLES = None\n",
    "DATASET = \"IMAGECLEF\"\n",
    "LOAD_TRAINED_IMAGECLEF = True\n",
    "\n",
    "\n",
    "if os.getcwd().startswith('/home/mlmi-matthias'):\n",
    "    ACCELERATOR = \"gpu\"\n",
    "    DEVICES = [6,7]\n",
    "    PRETRAINED_CLIP_PATH = '/home/mlmi-matthias/Caghan/pretrained_models/PubMedCLIP_ViT32.pth'\n",
    "    PRETRAINED_GPT2_PATH = \"/home/mlmi-matthias/Caghan/pretrained_models/gpt2-pytorch_model.bin\"\n",
    "    MIMIC_CXR_DCM_PATH = '/home/mlmi-matthias/physionet.org/files/mimic-cxr/2.0.0/files/'\n",
    "    MIMIC_CXR_JPG_PATH = \"/home/mlmi-matthias/physionet.org/files/mimic-cxr-jpg/2.0.0/files/\"\n",
    "    SPLIT_PATH = '/home/mlmi-matthias/Caghan/mlmi-vqa/data/external/'\n",
    "    IMAGECLEF_PATH ='/home/mlmi-matthias/imageclef/'\n",
    "    #CHECKPOINT_PATH = \"/home/mlmi-matthias/Caghan/mlmi-vqa/notebooks/lightning_logs/version_20/checkpoints/epoch=114-val_loss=0.84-other_metric=0.00.ckpt\"\n",
    "    # Latest ROCO Training \n",
    "    CHECKPOINT_PATH =\"/home/mlmi-matthias/Caghan/mlmi-vqa/notebooks/lightning_logs/version_77/checkpoints/epoch=61-val_loss_generation_epoch=1.80.ckpt\"\n",
    "    ANSWERS_LIST_PATH = '/home/mlmi-matthias/Caghan/mlmi-vqa//data/external/answer_list_imageclef.txt'\n",
    "    IMAGECLEF_CHECKPOINT_PATH = \"/home/mlmi-matthias/Caghan/mlmi-vqa/notebooks/lightning_logs/version_101/checkpoints/last.ckpt\"\n",
    "\n",
    "\n",
    "elif os.getcwd().startswith('/Users/caghankoksal'):\n",
    "    PRETRAINED_CLIP_PATH = '/Users/caghankoksal/Desktop/development/PubMedCLIP_ViT32.pth'\n",
    "    PRETRAINED_GPT2_PATH = \"/Users/caghankoksal/Desktop/development/TransformerPlay/gpt2-pytorch_model.bin\"\n",
    "    ACCELERATOR = \"cpu\"\n",
    "    DEVICES = 1\n",
    "    MIMIC_CXR_DCM_PATH = '/Users/caghankoksal/Desktop/development/Flamingo-playground/physionet.org/files/mimic-cxr/2.0.0/files/'\n",
    "    MIMIC_CXR_JPG_PATH = '/Users/caghankoksal/Desktop/development/physionet.org/files/mimic-cxr-jpg/2.0.0/files/'\n",
    "    SPLIT_PATH = '/Users/caghankoksal/Desktop/SS2022/mlmi-vqa/data/external/'\n",
    "    IMAGECLEF_PATH = \"/Users/caghankoksal/Desktop/imageclef/\"\n",
    "    CHECKPOINT_PATH = \"/Users/caghankoksal/Desktop/SS2022/lightning_logs/version_77/checkpoints/epoch=66-val_loss_generation_epoch=1.80.ckpt\"\n",
    "    ANSWERS_LIST_PATH = '/Users/caghankoksal/Desktop/SS2022/mlmi-vqa/data/external/answer_list_imageclef.txt'\n",
    "    IMAGECLEF_CHECKPOINT_PATH = \"/Users/caghankoksal/Desktop/SS2022/lightning_logs/version_102/checkpoints/last.ckpt\"\n",
    "\n",
    "\n",
    "IMAGE_TYPE = \"jpg\"\n",
    "TOKENIZER  = \"gpt2\"\n",
    "PREPROCESSED = True\n",
    "RETURN_IDX_EOC = True\n",
    "\n",
    "dataset_hyperparameters = {\n",
    "    \"root\": IMAGECLEF_PATH,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"tokenizer\": TOKENIZER,\n",
    "    \"num_data_workers\": NUM_DATA_WORKERS,\n",
    "    \"return_size\": False,\n",
    "    \"answers_list_path\": ANSWERS_LIST_PATH,\n",
    "    \"return_idx_answer_eoc\": RETURN_IDX_EOC,\n",
    "    \"transforms\": augmentations,\n",
    "    \"limit_num_samples\": LIMIT_NUM_SAMPLES,\n",
    "}\n",
    "\n",
    "\n",
    "datamodule = ImageCLEF2021DataModule(**dataset_hyperparameters)\n",
    "\n",
    "\n",
    "train_loader = datamodule.train_dataloader()\n",
    "val_loader = datamodule.val_dataloader()\n",
    "\n",
    "print(\"Len training dataset : \", len(datamodule.train_dataset),\n",
    "    \"Batch Size : \", BATCH_SIZE, \"NUM_EPOCHS : \",NUM_EPOCHS )\n",
    "print(\"Total training steps : \", len(datamodule.train_dataset)//BATCH_SIZE*NUM_EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /var/folders/61/9c2llh9n2pjb81c4dmhmb67w0000gn/T/tmpe0ztr82p\n",
      "INFO:torch.distributed.nn.jit.instantiator:Writing /var/folders/61/9c2llh9n2pjb81c4dmhmb67w0000gn/T/tmpe0ztr82p/_remote_module_non_sriptable.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained_clip_path /Users/caghankoksal/Desktop/development/PubMedCLIP_ViT32.pth\n",
      "warmup_steps 30\n",
      "num_tokens 50261\n",
      "dim 768\n",
      "depth 12\n",
      "num_heads 8\n",
      "dim_head 64\n",
      "cross_attn_every 3\n",
      "media_token_id 50258\n",
      "perceiver_num_latents 64\n",
      "perceiver_depth 2\n",
      "image_encoder clip\n",
      "language_model gpt2\n",
      "pretrained_gpt2_path /Users/caghankoksal/Desktop/development/TransformerPlay/gpt2-pytorch_model.bin\n",
      "classification_mode False\n",
      "classification_num_classes 332\n",
      "flamingo_mode True\n",
      "label_smoothing 0.5\n",
      "token_label_smoothing 0.0\n",
      "learning_rate 0.0001\n",
      "use_image_embeddings True\n",
      "train_embedding_layer True\n",
      "classifier_dropout 0.5\n",
      "Clip architecture is being loaded\n",
      "Clip pretrained weights are being loaded\n",
      "Flamingo is being initialized with  gpt2  as language model\n",
      "GPT 2 Weights are loading...\n",
      "Loaded GPT2 weights and Embeddings num_weights loaded :  156\n"
     ]
    }
   ],
   "source": [
    "# MODEL HPRAMS\n",
    "VOCAB_SIZE_OF_TOKENIZER = 50257 # mimic_datamodule.train_dataset.tokenizer.vocab_size\n",
    "LANGUAGE_MODEL = 'gpt2'\n",
    "NUM_TOKENS = VOCAB_SIZE_OF_TOKENIZER +4 if LANGUAGE_MODEL==\"gpt2\" else 31092\n",
    "FLAMINGO_EMBED_DIM = 768\n",
    "DEPTH = 12\n",
    "NUM_HEADS = 8\n",
    "ATT_HEAD_DIM = 64\n",
    "CROOS_ATT_EVERY=3\n",
    "MEDIA_TOKEN_ID = datamodule.train_dataset.tokenizer.\\\n",
    "    all_special_ids[datamodule.train_dataset.tokenizer.all_special_tokens.index('<image>')]\n",
    "PERCEIVER_NUM_LATENTS = 64\n",
    "PERCEIVER_DEPTH = 2\n",
    "IMAGE_ENCODER = \"clip\"\n",
    "CLASSIFICATION_MODE = False\n",
    "NUM_CLASSES = 332\n",
    "FLAMINGO_MODE = True\n",
    "LABEL_SMOOTHING = 0.5\n",
    "# Label smoothing for classification task\n",
    "TOKEN_LABEL_SMOOTHING = 0.0\n",
    "GRADIENT_CLIP_VAL = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "USE_IMAGE_EMBEDDINGS = True\n",
    "TRAIN_EMBEDDING_LAYER = True\n",
    "CLASSIFIER_DROPOUT = 0.5\n",
    "\n",
    "\n",
    "hyperparams = {\n",
    "    'pretrained_clip_path': PRETRAINED_CLIP_PATH,\n",
    "    'warmup_steps': 30,\n",
    "    'num_tokens': NUM_TOKENS,\n",
    "    'dim': FLAMINGO_EMBED_DIM,\n",
    "    'depth': DEPTH,\n",
    "    'num_heads': NUM_HEADS,\n",
    "    'dim_head': ATT_HEAD_DIM,\n",
    "    'cross_attn_every': CROOS_ATT_EVERY,\n",
    "    'media_token_id': MEDIA_TOKEN_ID,\n",
    "    'perceiver_num_latents': PERCEIVER_NUM_LATENTS,\n",
    "    'perceiver_depth': PERCEIVER_DEPTH,\n",
    "    'image_encoder': IMAGE_ENCODER,\n",
    "    'language_model': LANGUAGE_MODEL,\n",
    "    'pretrained_gpt2_path': PRETRAINED_GPT2_PATH,\n",
    "    'classification_mode': CLASSIFICATION_MODE,\n",
    "    'classification_num_classes': NUM_CLASSES,  # 332 if DATASET==\"IMAGECLEF\"\n",
    "    'flamingo_mode': FLAMINGO_MODE,\n",
    "    \"label_smoothing\": LABEL_SMOOTHING,\n",
    "    \"token_label_smoothing\": TOKEN_LABEL_SMOOTHING,\n",
    "    \"learning_rate\":LEARNING_RATE,\n",
    "    \"use_image_embeddings\": USE_IMAGE_EMBEDDINGS,\n",
    "    \"train_embedding_layer\": TRAIN_EMBEDDING_LAYER,\n",
    "    \"classifier_dropout\": CLASSIFIER_DROPOUT\n",
    "    }\n",
    "\n",
    "print_hyperparams(hyperparams)\n",
    "\n",
    "model = FlamingoModule(**hyperparams)\n",
    "START_FROM_CHECKPOINT = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGECLEF_CHECKPOINT_PATH = \"/Users/caghankoksal/Desktop/SS2022/lightning_logs/version_110/checkpoints/last.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained Flamingo Model is loaded from checkpoint :  /Users/caghankoksal/Desktop/SS2022/lightning_logs/version_110/checkpoints/last.ckpt\n",
      "Checkpoint Weights are loaded\n"
     ]
    }
   ],
   "source": [
    "if LOAD_TRAINED_IMAGECLEF:\n",
    "    print(\"Pretrained Flamingo Model is loaded from checkpoint : \",IMAGECLEF_CHECKPOINT_PATH)\n",
    "    if os.getcwd().startswith('/home/mlmi-matthias'):\n",
    "        model.load_state_dict(torch.load(IMAGECLEF_CHECKPOINT_PATH)[\"state_dict\"],strict=False)\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(IMAGECLEF_CHECKPOINT_PATH,map_location=torch.device('cpu'))[\"state_dict\"],strict=False)\n",
    "        print(\"Checkpoint Weights are loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_gradio(image, context, cur_model, ntok=20):\n",
    "    for _ in range(ntok):\n",
    "        out= cur_model({'image': image,'input_ids': context})\n",
    "        logits = out[:, -1, :]\n",
    "        indices_to_remove = logits < torch.topk(logits, 10)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = np.NINF\n",
    "        #next_tok1 = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1).squeeze(1)\n",
    "        #print(next_tok1.shape)\n",
    "        softmax_out = F.softmax(logits, dim=-1)\n",
    "        #print(softmax_out.shape)\n",
    "        next_tok = torch.argmax(softmax_out,dim=-1,keepdim=False)\n",
    "        #print(next_tok.shape)\n",
    "        context = torch.cat([context, next_tok.unsqueeze(-1)], dim=-1)\n",
    "    return context\n",
    "\n",
    "\n",
    "tokenizer = datamodule.train_dataset.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_gradio(image, question):\n",
    "    print(\"Input question\")\n",
    "    process_img = augmentations[\"val\"](image).unsqueeze(0)\n",
    "    print(\"Process_img succesfull\")\n",
    "    context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+question + ' <EOQ>'+ ' answer:')]) \n",
    "    out = generate_gradio( process_img,context, model, ntok=20)\n",
    "    #print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "    result = tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0]\n",
    "    return result\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7884/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:paramiko.transport:Connected (version 2.0, client OpenSSH_7.6p1)\n",
      "INFO:paramiko.transport:Authentication (publickey) successful!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on public URL: https://45285.gradio.app\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting, check out Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://45285.gradio.app\" width=\"900\" height=\"500\" allow=\"autoplay; camera; microphone;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<gradio.routes.App at 0x29d6f6ee0>,\n",
       " 'http://127.0.0.1:7884/',\n",
       " 'https://45285.gradio.app')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "title = \"Visual_Question_Answering on Medical Data ImageCLEF\"\n",
    "description = \"Gradio Demo for Medical Visual_Question_Answering. Upload your own image (high-resolution images are recommended) or click any one of the examples, and click \" \\\n",
    "              \"\\\"Submit\\\" and then wait for our's answer. \"\n",
    "article = \"<p style='text-align: center'><a href='https://github.com/OFA-Sys/OFA' target='_blank'>OFA Github \" \\\n",
    "          \"Repo</a></p> \"\n",
    "examples = [['demo_images/synpic16279.jpg','what is abnormal in the x-ray?' ], ['demo_images/synpic17959.jpg',  'what is most alarming about this mri?']]\n",
    "io = gr.Interface(fn=predict_gradio, inputs=[gr.inputs.Image(type='pil'), \"textbox\"], outputs=gr.outputs.Textbox(label=\"Answer\"),\n",
    "                 examples=examples,title=title, description=description,\n",
    "                  debug=False)\n",
    "io.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('torch-nightly')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "892a7f8aeabe86b99d45932805d162784b758c544538f3ce4737e4a115db3cfd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
