{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os\n",
    "sys.path.append('..')\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/home/koksal/.conda/envs/mlmi/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "from src.models.multimodal.flamingo_module import FlamingoModule\n",
    "from src.datasets.imageclef_dataset import ImageCLEF2021DataModule\n",
    "from src.utils.utils import load_flamingo_weights, print_hyperparams\n",
    "\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "import torchvision.transforms as T\n",
    "from pytorch_lightning.loggers import CometLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "CONFIG_PATH = '/u/home/koksal/mlmi-vqa/configs'\n",
    "# Read CONFIGS\n",
    "def load_config(config_name):\n",
    "    with open(os.path.join(CONFIG_PATH, config_name)) as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pretrained': None,\n",
       " 'dataset': {'imageclef_path': '/u/home/koksal/mlmi-vqa/data/external/imageclef/',\n",
       "  'vqa_rad_path': '/u/home/koksal/mlmi-vqa/data/external/VQA_RAD',\n",
       "  'answers_list_path': None,\n",
       "  'mimic_cxr_jpg_path': None,\n",
       "  'split_path': '/home/mlmi-matthias/Caghan/mlmi-vqa/data/external/',\n",
       "  'num_workers': 8,\n",
       "  'only_images': False,\n",
       "  'limit_num_samples': None,\n",
       "  'shuffle': True,\n",
       "  'load_in_memory': False,\n",
       "  'preprocessed': False,\n",
       "  'tokenizer': 'gpt2'},\n",
       " 'model': {'script_path': '../models/optimized.py',\n",
       "  'pretrained_clip_path': None,\n",
       "  'pretrained_language_path': '/u/home/koksal/mlmi-vqa/models/gpt2-pytorch_model.bin',\n",
       "  'pretrained_model_path': None,\n",
       "  'image_encoder': 'clip',\n",
       "  'language_encoder': 'gpt2',\n",
       "  'vocab_size': 50258,\n",
       "  'num_tokens': 50261,\n",
       "  'flamingo_embed_dim': 768,\n",
       "  'depth': 12,\n",
       "  'dim_head': None,\n",
       "  'num_heads': 8,\n",
       "  'att_head_dim': 64,\n",
       "  'cross_att_every': 3,\n",
       "  'media_token_id': 4142,\n",
       "  'perceiver_num_latents': 64,\n",
       "  'perceicer_depth': 2,\n",
       "  'classification_mode': True,\n",
       "  'flamingo_mode': True,\n",
       "  'train_embedding_layer': True,\n",
       "  'use_positional_embedding': True,\n",
       "  'use_image_embeddings': True,\n",
       "  'label_smoothing': 0.1,\n",
       "  'token_label_smoothing': 0.1,\n",
       "  'classifier_dropout': 0.5,\n",
       "  'num_classification_classes': 402,\n",
       "  'return_idx_eoc': True},\n",
       " 'optimizer': {'script_path': '../optimizers/adam_keras.py',\n",
       "  'initial_lr': 0.0001},\n",
       " 'train': {'script_path': '../train/train_keras.py',\n",
       "  'artifacts_path': '../artifacts/cifar10_opt/',\n",
       "  'batch_size': 4,\n",
       "  'num_epochs': 80,\n",
       "  'warm_up_steps': 0,\n",
       "  'learning_rate': 0.0001,\n",
       "  'devices': 6,\n",
       "  'accelerator': 'gpu',\n",
       "  'warmup_steps': 0,\n",
       "  'data_augmentation': {'resize_size': [224, 224],\n",
       "   'horizontal_flip': 0.5,\n",
       "   'random_rotation': 10}},\n",
       " 'evaluate': {'batch_size': 1000,\n",
       "  'augmentation_factor': 32,\n",
       "  'data_augmentation': {'samplewise_center': False,\n",
       "   'samplewise_std_normalization': False,\n",
       "   'rotation_range': 0,\n",
       "   'width_shift_range': 0.15,\n",
       "   'height_shift_range': 0.15,\n",
       "   'horizontal_flip': True,\n",
       "   'vertical_flip': False,\n",
       "   'zoom_range': 0,\n",
       "   'shear_range': 0,\n",
       "   'channel_shift_range': 0,\n",
       "   'featurewise_center': False,\n",
       "   'zca_whitening': False}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_config('config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len training dataset :  4500 Batch Size :  1 NUM_EPOCHS :  120\n",
      "Total training steps :  540000\n"
     ]
    }
   ],
   "source": [
    "augmentations = {\n",
    "        \n",
    "        'train': T.Compose([T.Resize((224,224)),\n",
    "                            T.ToTensor(),\n",
    "                            T.RandomHorizontalFlip(p=0.5),\n",
    "                            T.RandomRotation(degrees=10),\n",
    "                            T.Normalize(mean=(0.2570, 0.2570, 0.2570), std=(0.2710, 0.2710, 0.2710))\n",
    "                            ]),\n",
    "        'val': T.Compose([T.Resize((224,224)),\n",
    "                            T.ToTensor(),\n",
    "                            T.RandomHorizontalFlip(p=0.5),\n",
    "                            T.RandomRotation(degrees=10),\n",
    "                            T.Normalize(mean=(0.2570, 0.2570, 0.2570), std=(0.2710, 0.2710, 0.2710))\n",
    "                            ]),\n",
    "        'test': T.Compose([T.Resize((224,224)),\n",
    "                            T.ToTensor(),\n",
    "                            T.RandomHorizontalFlip(p=0.5),\n",
    "                            T.RandomRotation(degrees=10),   \n",
    "                            T.Normalize(mean=(0.2570, 0.2570, 0.2570), std=(0.2710, 0.2710, 0.2710))\n",
    "                            ])\n",
    "    }\n",
    "\n",
    "    \n",
    "# Hyperparameters\n",
    "NUM_DATA_WORKERS  = 2\n",
    "ONLY_IMAGES = False\n",
    "BATCH_SIZE = 1\n",
    "NUM_EPOCHS = 120\n",
    "LIMIT_NUM_SAMPLES = None\n",
    "DATASET = \"IMAGECLEF\"\n",
    "LOAD_TRAINED_IMAGECLEF = True\n",
    "\n",
    "\n",
    "if os.getcwd().startswith('/u/home/koksal'):\n",
    "    ACCELERATOR = \"gpu\"\n",
    "    DEVICES = [6,7]\n",
    "    PRETRAINED_CLIP_PATH = None#'/home/mlmi-matthias/Caghan/pretrained_models/PubMedCLIP_ViT32.pth'\n",
    "    PRETRAINED_GPT2_PATH = \"/u/home/koksal/mlmi-vqa/models/gpt2-pytorch_model.bin\"\n",
    "    MIMIC_CXR_DCM_PATH = '/home/mlmi-matthias/physionet.org/files/mimic-cxr/2.0.0/files/'\n",
    "    MIMIC_CXR_JPG_PATH = \"/home/mlmi-matthias/physionet.org/files/mimic-cxr-jpg/2.0.0/files/\"\n",
    "    SPLIT_PATH = '/home/mlmi-matthias/Caghan/mlmi-vqa/data/external/'\n",
    "    IMAGECLEF_PATH ='/u/home/koksal/mlmi-vqa/data/external/imageclef/'\n",
    "    #CHECKPOINT_PATH = \"/home/mlmi-matthias/Caghan/mlmi-vqa/notebooks/lightning_logs/version_20/checkpoints/epoch=114-val_loss=0.84-other_metric=0.00.ckpt\"\n",
    "    # Latest ROCO Training \n",
    "    CHECKPOINT_PATH =\"/home/mlmi-matthias/Caghan/mlmi-vqa/notebooks/lightning_logs/version_77/checkpoints/epoch=61-val_loss_generation_epoch=1.80.ckpt\"\n",
    "    ANSWERS_LIST_PATH = '/u/home/koksal/mlmi-vqa//data/external/answer_list_imageclef.txt'\n",
    "    IMAGECLEF_CHECKPOINT_PATH = \"/home/mlmi-matthias/Caghan/mlmi-vqa/notebooks/lightning_logs/version_101/checkpoints/last.ckpt\"\n",
    "\n",
    "\n",
    "elif os.getcwd().startswith('/Users/caghankoksal'):\n",
    "    PRETRAINED_CLIP_PATH = '/Users/caghankoksal/Desktop/development/PubMedCLIP_ViT32.pth'\n",
    "    PRETRAINED_GPT2_PATH = \"/Users/caghankoksal/Desktop/development/TransformerPlay/gpt2-pytorch_model.bin\"\n",
    "    ACCELERATOR = \"cpu\"\n",
    "    DEVICES = 1\n",
    "    MIMIC_CXR_DCM_PATH = '/Users/caghankoksal/Desktop/development/Flamingo-playground/physionet.org/files/mimic-cxr/2.0.0/files/'\n",
    "    MIMIC_CXR_JPG_PATH = '/Users/caghankoksal/Desktop/development/physionet.org/files/mimic-cxr-jpg/2.0.0/files/'\n",
    "    SPLIT_PATH = '/Users/caghankoksal/Desktop/SS2022/mlmi-vqa/data/external/'\n",
    "    IMAGECLEF_PATH = \"/Users/caghankoksal/Desktop/imageclef/\"\n",
    "    CHECKPOINT_PATH = \"/Users/caghankoksal/Desktop/SS2022/lightning_logs/version_77/checkpoints/epoch=66-val_loss_generation_epoch=1.80.ckpt\"\n",
    "    ANSWERS_LIST_PATH = '/Users/caghankoksal/Desktop/SS2022/mlmi-vqa/data/external/answer_list_imageclef.txt'\n",
    "    IMAGECLEF_CHECKPOINT_PATH = \"/Users/caghankoksal/Desktop/SS2022/lightning_logs/version_102/checkpoints/last.ckpt\"\n",
    "\n",
    "\n",
    "IMAGE_TYPE = \"jpg\"\n",
    "TOKENIZER  = \"bert-base-uncased\"\n",
    "PREPROCESSED = True\n",
    "RETURN_IDX_EOC = True\n",
    "\n",
    "dataset_hyperparameters = {\n",
    "    \"root\": IMAGECLEF_PATH,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"tokenizer\": TOKENIZER,\n",
    "    \"num_data_workers\": NUM_DATA_WORKERS,\n",
    "    \"return_size\": False,\n",
    "    \"answers_list_path\": ANSWERS_LIST_PATH,\n",
    "    \"return_idx_answer_eoc\": RETURN_IDX_EOC,\n",
    "    \"transforms\": augmentations,\n",
    "    \"limit_num_samples\": LIMIT_NUM_SAMPLES,\n",
    "}\n",
    "\n",
    "\n",
    "datamodule = ImageCLEF2021DataModule(**dataset_hyperparameters)\n",
    "\n",
    "\n",
    "train_loader = datamodule.train_dataloader()\n",
    "val_loader = datamodule.val_dataloader()\n",
    "\n",
    "print(\"Len training dataset : \", len(datamodule.train_dataset),\n",
    "    \"Batch Size : \", BATCH_SIZE, \"NUM_EPOCHS : \",NUM_EPOCHS )\n",
    "print(\"Total training steps : \", len(datamodule.train_dataset)//BATCH_SIZE*NUM_EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    cur_images = batch['image']\n",
    "    print(cur_images.shape )\n",
    "    cur_batch = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<BOS> <image> question: is this a normal gastrointestinal image? <EOQ> answer: yes <EOC>']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_batch['qa_pair']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_batch[\"index_answer\"]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['image', 'question', 'answer', 'label', 'index_answer', 'index_eoc', 'index_eoq', 'input_ids', 'token_type_ids', 'targets', 'ID', 'qa_pair'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_batch[\"input_ids\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from torch import nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "base_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_model = nn.Sequential(*list(base_model.children())[0:])\n",
    "bert_embedding = bert_model[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30522, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_embedding.word_embeddings.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Parameter' object has no attribute 'bias'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/u/home/koksal/mlmi-vqa/notebooks/imageclef_flamingo_clip_gpt2_playground.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B131.159.110.3/u/home/koksal/mlmi-vqa/notebooks/imageclef_flamingo_clip_gpt2_playground.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m bert_embedding\u001b[39m.\u001b[39;49mword_embeddings\u001b[39m.\u001b[39;49mweight\u001b[39m.\u001b[39;49mbias\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Parameter' object has no attribute 'bias'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "bert_embedding.word_embeddings.weight.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import pytorch_lightning as pl\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM,PreTrainedTokenizerFast, GPT2Tokenizer\n",
    "from PIL import Image\n",
    "from tqdm import tqdm as tqdm\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from src.datasets.imageclef_dataset import ImageCLEF2021DataModule\n",
    "from src.models.multimodal.flamingo_module import FlamingoModule\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained_clip_path None\n",
      "warmup_steps 30\n",
      "num_tokens 30526\n",
      "dim 768\n",
      "depth 12\n",
      "num_heads 8\n",
      "dim_head 64\n",
      "cross_attn_every 3\n",
      "media_token_id 3\n",
      "perceiver_num_latents 64\n",
      "perceiver_depth 2\n",
      "image_encoder clip\n",
      "language_model gpt2\n",
      "pretrained_gpt2_path /u/home/koksal/mlmi-vqa/models/gpt2-pytorch_model.bin\n",
      "classification_mode True\n",
      "classification_num_classes 332\n",
      "flamingo_mode True\n",
      "label_smoothing 0.5\n",
      "token_label_smoothing 0.0\n",
      "learning_rate 0.0001\n",
      "use_image_embeddings True\n",
      "train_embedding_layer True\n",
      "classifier_dropout 0.5\n"
     ]
    }
   ],
   "source": [
    "# MODEL HPRAMS\n",
    "VOCAB_SIZE_OF_TOKENIZER = 30522#For BERT # GPT2 50257 # datamodule.train_dataset.tokenizer.vocab_size\n",
    "LANGUAGE_MODEL = 'gpt2'\n",
    "NUM_TOKENS = VOCAB_SIZE_OF_TOKENIZER +4 if LANGUAGE_MODEL==\"gpt2\" else 31092\n",
    "FLAMINGO_EMBED_DIM = 768\n",
    "DEPTH = 12\n",
    "NUM_HEADS = 8\n",
    "ATT_HEAD_DIM = 64\n",
    "CROOS_ATT_EVERY=3\n",
    "#MEDIA_TOKEN_ID = datamodule.train_dataset.tokenizer.\\\n",
    "#    all_special_ids[datamodule.train_dataset.tokenizer.all_special_tokens.index('<image>')]\n",
    "MEDIA_TOKEN_ID=3\n",
    "PERCEIVER_NUM_LATENTS = 64\n",
    "PERCEIVER_DEPTH = 2\n",
    "IMAGE_ENCODER = \"clip\"\n",
    "CLASSIFICATION_MODE = True\n",
    "NUM_CLASSES = 332\n",
    "FLAMINGO_MODE = True\n",
    "LABEL_SMOOTHING = 0.5\n",
    "# Label smoothing for classification task\n",
    "TOKEN_LABEL_SMOOTHING = 0.0\n",
    "GRADIENT_CLIP_VAL = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "USE_IMAGE_EMBEDDINGS = True\n",
    "TRAIN_EMBEDDING_LAYER = True\n",
    "CLASSIFIER_DROPOUT = 0.5\n",
    "\n",
    "\n",
    "hyperparams = {\n",
    "    'pretrained_clip_path': None,#PRETRAINED_CLIP_PATH,\n",
    "    'warmup_steps': 30,\n",
    "    'num_tokens': NUM_TOKENS,\n",
    "    'dim': FLAMINGO_EMBED_DIM,\n",
    "    'depth': DEPTH,\n",
    "    'num_heads': NUM_HEADS,\n",
    "    'dim_head': ATT_HEAD_DIM,\n",
    "    'cross_attn_every': CROOS_ATT_EVERY,\n",
    "    'media_token_id': MEDIA_TOKEN_ID,\n",
    "    'perceiver_num_latents': PERCEIVER_NUM_LATENTS,\n",
    "    'perceiver_depth': PERCEIVER_DEPTH,\n",
    "    'image_encoder': IMAGE_ENCODER,\n",
    "    'language_model': LANGUAGE_MODEL,\n",
    "    'pretrained_gpt2_path': PRETRAINED_GPT2_PATH,\n",
    "    'classification_mode': CLASSIFICATION_MODE,\n",
    "    'classification_num_classes': NUM_CLASSES,  # 332 if DATASET==\"IMAGECLEF\"\n",
    "    'flamingo_mode': FLAMINGO_MODE,\n",
    "    \"label_smoothing\": LABEL_SMOOTHING,\n",
    "    \"token_label_smoothing\": TOKEN_LABEL_SMOOTHING,\n",
    "    \"learning_rate\":LEARNING_RATE,\n",
    "    \"use_image_embeddings\": USE_IMAGE_EMBEDDINGS,\n",
    "    \"train_embedding_layer\": TRAIN_EMBEDDING_LAYER,\n",
    "    \"classifier_dropout\": CLASSIFIER_DROPOUT\n",
    "    }\n",
    "\n",
    "print_hyperparams(hyperparams)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FlamingoModule(**hyperparams)\n",
    "START_FROM_CHECKPOINT = True\n",
    "\n",
    "if START_FROM_CHECKPOINT:\n",
    "    print(\"Pretrained Flamingo Model is loaded from checkpoint : \",CHECKPOINT_PATH)\n",
    "    if os.getcwd().startswith('/home/mlmi-matthias'):\n",
    "        model.load_state_dict(torch.load(CHECKPOINT_PATH)[\"state_dict\"],strict=False)\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(CHECKPOINT_PATH,map_location=torch.device('cpu'))[\"state_dict\"],strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams['language_model'] ='bert'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmpsjafz8qf\n",
      "INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmpsjafz8qf/_remote_module_non_sriptable.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vit is started from scratch\n",
      "Vit is initialized\n",
      "Flamingo is being initialized with  bert  as language model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert Embeddings are loaded \n"
     ]
    }
   ],
   "source": [
    "model = FlamingoModule(**hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0.fn.attention.proj_q.weight\n",
      "0.0.fn.attention.proj_q.bias\n",
      "0.0.fn.attention.proj_k.weight\n",
      "0.0.fn.attention.proj_k.bias\n",
      "0.0.fn.attention.proj_v.weight\n",
      "0.0.fn.attention.proj_v.bias\n",
      "0.0.fn.feedforward.fc1.weight\n",
      "0.0.fn.feedforward.fc1.bias\n",
      "0.0.fn.feedforward.fc2.weight\n",
      "0.0.fn.feedforward.fc2.bias\n",
      "0.0.fn.norm1.weight\n",
      "0.0.fn.norm1.bias\n",
      "0.0.fn.norm2.weight\n",
      "0.0.fn.norm2.bias\n",
      "0.0.fn.proj.weight\n",
      "0.0.fn.proj.bias\n",
      "0.1.attn_gate\n",
      "0.1.ff_gate\n",
      "0.1.attn.norm.weight\n",
      "0.1.attn.norm.bias\n",
      "0.1.attn.to_q.weight\n",
      "0.1.attn.to_kv.weight\n",
      "0.1.attn.to_out.weight\n",
      "0.1.ff.0.weight\n",
      "0.1.ff.0.bias\n",
      "0.1.ff.1.weight\n",
      "0.1.ff.3.weight\n",
      "1.0.fn.attention.proj_q.weight\n",
      "1.0.fn.attention.proj_q.bias\n",
      "1.0.fn.attention.proj_k.weight\n",
      "1.0.fn.attention.proj_k.bias\n",
      "1.0.fn.attention.proj_v.weight\n",
      "1.0.fn.attention.proj_v.bias\n",
      "1.0.fn.feedforward.fc1.weight\n",
      "1.0.fn.feedforward.fc1.bias\n",
      "1.0.fn.feedforward.fc2.weight\n",
      "1.0.fn.feedforward.fc2.bias\n",
      "1.0.fn.norm1.weight\n",
      "1.0.fn.norm1.bias\n",
      "1.0.fn.norm2.weight\n",
      "1.0.fn.norm2.bias\n",
      "1.0.fn.proj.weight\n",
      "1.0.fn.proj.bias\n",
      "2.0.fn.attention.proj_q.weight\n",
      "2.0.fn.attention.proj_q.bias\n",
      "2.0.fn.attention.proj_k.weight\n",
      "2.0.fn.attention.proj_k.bias\n",
      "2.0.fn.attention.proj_v.weight\n",
      "2.0.fn.attention.proj_v.bias\n",
      "2.0.fn.feedforward.fc1.weight\n",
      "2.0.fn.feedforward.fc1.bias\n",
      "2.0.fn.feedforward.fc2.weight\n",
      "2.0.fn.feedforward.fc2.bias\n",
      "2.0.fn.norm1.weight\n",
      "2.0.fn.norm1.bias\n",
      "2.0.fn.norm2.weight\n",
      "2.0.fn.norm2.bias\n",
      "2.0.fn.proj.weight\n",
      "2.0.fn.proj.bias\n",
      "3.0.fn.attention.proj_q.weight\n",
      "3.0.fn.attention.proj_q.bias\n",
      "3.0.fn.attention.proj_k.weight\n",
      "3.0.fn.attention.proj_k.bias\n",
      "3.0.fn.attention.proj_v.weight\n",
      "3.0.fn.attention.proj_v.bias\n",
      "3.0.fn.feedforward.fc1.weight\n",
      "3.0.fn.feedforward.fc1.bias\n",
      "3.0.fn.feedforward.fc2.weight\n",
      "3.0.fn.feedforward.fc2.bias\n",
      "3.0.fn.norm1.weight\n",
      "3.0.fn.norm1.bias\n",
      "3.0.fn.norm2.weight\n",
      "3.0.fn.norm2.bias\n",
      "3.0.fn.proj.weight\n",
      "3.0.fn.proj.bias\n",
      "3.1.attn_gate\n",
      "3.1.ff_gate\n",
      "3.1.attn.norm.weight\n",
      "3.1.attn.norm.bias\n",
      "3.1.attn.to_q.weight\n",
      "3.1.attn.to_kv.weight\n",
      "3.1.attn.to_out.weight\n",
      "3.1.ff.0.weight\n",
      "3.1.ff.0.bias\n",
      "3.1.ff.1.weight\n",
      "3.1.ff.3.weight\n",
      "4.0.fn.attention.proj_q.weight\n",
      "4.0.fn.attention.proj_q.bias\n",
      "4.0.fn.attention.proj_k.weight\n",
      "4.0.fn.attention.proj_k.bias\n",
      "4.0.fn.attention.proj_v.weight\n",
      "4.0.fn.attention.proj_v.bias\n",
      "4.0.fn.feedforward.fc1.weight\n",
      "4.0.fn.feedforward.fc1.bias\n",
      "4.0.fn.feedforward.fc2.weight\n",
      "4.0.fn.feedforward.fc2.bias\n",
      "4.0.fn.norm1.weight\n",
      "4.0.fn.norm1.bias\n",
      "4.0.fn.norm2.weight\n",
      "4.0.fn.norm2.bias\n",
      "4.0.fn.proj.weight\n",
      "4.0.fn.proj.bias\n",
      "5.0.fn.attention.proj_q.weight\n",
      "5.0.fn.attention.proj_q.bias\n",
      "5.0.fn.attention.proj_k.weight\n",
      "5.0.fn.attention.proj_k.bias\n",
      "5.0.fn.attention.proj_v.weight\n",
      "5.0.fn.attention.proj_v.bias\n",
      "5.0.fn.feedforward.fc1.weight\n",
      "5.0.fn.feedforward.fc1.bias\n",
      "5.0.fn.feedforward.fc2.weight\n",
      "5.0.fn.feedforward.fc2.bias\n",
      "5.0.fn.norm1.weight\n",
      "5.0.fn.norm1.bias\n",
      "5.0.fn.norm2.weight\n",
      "5.0.fn.norm2.bias\n",
      "5.0.fn.proj.weight\n",
      "5.0.fn.proj.bias\n",
      "6.0.fn.attention.proj_q.weight\n",
      "6.0.fn.attention.proj_q.bias\n",
      "6.0.fn.attention.proj_k.weight\n",
      "6.0.fn.attention.proj_k.bias\n",
      "6.0.fn.attention.proj_v.weight\n",
      "6.0.fn.attention.proj_v.bias\n",
      "6.0.fn.feedforward.fc1.weight\n",
      "6.0.fn.feedforward.fc1.bias\n",
      "6.0.fn.feedforward.fc2.weight\n",
      "6.0.fn.feedforward.fc2.bias\n",
      "6.0.fn.norm1.weight\n",
      "6.0.fn.norm1.bias\n",
      "6.0.fn.norm2.weight\n",
      "6.0.fn.norm2.bias\n",
      "6.0.fn.proj.weight\n",
      "6.0.fn.proj.bias\n",
      "6.1.attn_gate\n",
      "6.1.ff_gate\n",
      "6.1.attn.norm.weight\n",
      "6.1.attn.norm.bias\n",
      "6.1.attn.to_q.weight\n",
      "6.1.attn.to_kv.weight\n",
      "6.1.attn.to_out.weight\n",
      "6.1.ff.0.weight\n",
      "6.1.ff.0.bias\n",
      "6.1.ff.1.weight\n",
      "6.1.ff.3.weight\n",
      "7.0.fn.attention.proj_q.weight\n",
      "7.0.fn.attention.proj_q.bias\n",
      "7.0.fn.attention.proj_k.weight\n",
      "7.0.fn.attention.proj_k.bias\n",
      "7.0.fn.attention.proj_v.weight\n",
      "7.0.fn.attention.proj_v.bias\n",
      "7.0.fn.feedforward.fc1.weight\n",
      "7.0.fn.feedforward.fc1.bias\n",
      "7.0.fn.feedforward.fc2.weight\n",
      "7.0.fn.feedforward.fc2.bias\n",
      "7.0.fn.norm1.weight\n",
      "7.0.fn.norm1.bias\n",
      "7.0.fn.norm2.weight\n",
      "7.0.fn.norm2.bias\n",
      "7.0.fn.proj.weight\n",
      "7.0.fn.proj.bias\n",
      "8.0.fn.attention.proj_q.weight\n",
      "8.0.fn.attention.proj_q.bias\n",
      "8.0.fn.attention.proj_k.weight\n",
      "8.0.fn.attention.proj_k.bias\n",
      "8.0.fn.attention.proj_v.weight\n",
      "8.0.fn.attention.proj_v.bias\n",
      "8.0.fn.feedforward.fc1.weight\n",
      "8.0.fn.feedforward.fc1.bias\n",
      "8.0.fn.feedforward.fc2.weight\n",
      "8.0.fn.feedforward.fc2.bias\n",
      "8.0.fn.norm1.weight\n",
      "8.0.fn.norm1.bias\n",
      "8.0.fn.norm2.weight\n",
      "8.0.fn.norm2.bias\n",
      "8.0.fn.proj.weight\n",
      "8.0.fn.proj.bias\n",
      "9.0.fn.attention.proj_q.weight\n",
      "9.0.fn.attention.proj_q.bias\n",
      "9.0.fn.attention.proj_k.weight\n",
      "9.0.fn.attention.proj_k.bias\n",
      "9.0.fn.attention.proj_v.weight\n",
      "9.0.fn.attention.proj_v.bias\n",
      "9.0.fn.feedforward.fc1.weight\n",
      "9.0.fn.feedforward.fc1.bias\n",
      "9.0.fn.feedforward.fc2.weight\n",
      "9.0.fn.feedforward.fc2.bias\n",
      "9.0.fn.norm1.weight\n",
      "9.0.fn.norm1.bias\n",
      "9.0.fn.norm2.weight\n",
      "9.0.fn.norm2.bias\n",
      "9.0.fn.proj.weight\n",
      "9.0.fn.proj.bias\n",
      "9.1.attn_gate\n",
      "9.1.ff_gate\n",
      "9.1.attn.norm.weight\n",
      "9.1.attn.norm.bias\n",
      "9.1.attn.to_q.weight\n",
      "9.1.attn.to_kv.weight\n",
      "9.1.attn.to_out.weight\n",
      "9.1.ff.0.weight\n",
      "9.1.ff.0.bias\n",
      "9.1.ff.1.weight\n",
      "9.1.ff.3.weight\n",
      "10.0.fn.attention.proj_q.weight\n",
      "10.0.fn.attention.proj_q.bias\n",
      "10.0.fn.attention.proj_k.weight\n",
      "10.0.fn.attention.proj_k.bias\n",
      "10.0.fn.attention.proj_v.weight\n",
      "10.0.fn.attention.proj_v.bias\n",
      "10.0.fn.feedforward.fc1.weight\n",
      "10.0.fn.feedforward.fc1.bias\n",
      "10.0.fn.feedforward.fc2.weight\n",
      "10.0.fn.feedforward.fc2.bias\n",
      "10.0.fn.norm1.weight\n",
      "10.0.fn.norm1.bias\n",
      "10.0.fn.norm2.weight\n",
      "10.0.fn.norm2.bias\n",
      "10.0.fn.proj.weight\n",
      "10.0.fn.proj.bias\n",
      "11.0.fn.attention.proj_q.weight\n",
      "11.0.fn.attention.proj_q.bias\n",
      "11.0.fn.attention.proj_k.weight\n",
      "11.0.fn.attention.proj_k.bias\n",
      "11.0.fn.attention.proj_v.weight\n",
      "11.0.fn.attention.proj_v.bias\n",
      "11.0.fn.feedforward.fc1.weight\n",
      "11.0.fn.feedforward.fc1.bias\n",
      "11.0.fn.feedforward.fc2.weight\n",
      "11.0.fn.feedforward.fc2.bias\n",
      "11.0.fn.norm1.weight\n",
      "11.0.fn.norm1.bias\n",
      "11.0.fn.norm2.weight\n",
      "11.0.fn.norm2.bias\n",
      "11.0.fn.proj.weight\n",
      "11.0.fn.proj.bias\n"
     ]
    }
   ],
   "source": [
    "for k,v in model.flamingo_palm.layers.state_dict().items():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.flamingo_palm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "out,_ = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls \"/home/mlmi-matthias/Caghan/mlmi-vqa/notebooks/lightning_logs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#IMAGECLEF_CHECKPOINT_PATH ='/home/mlmi-matthias/Caghan/mlmi-vqa/notebooks/lightning_logs/version_95/checkpoints/\\\n",
    "#epoch=94-val_acc_epoch=0.41-val_total_loss_epoch=4.28-val_loss_generation_epoch=0.22-val_classification_loss_epoch=4.06.ckpt'\n",
    "#IMAGECLEF_CHECKPOINT_PATH = \"/home/mlmi-matthias/Caghan/mlmi-vqa/notebooks/lightning_logs/version_99/checkpoints/epoch=82-val_acc_epoch=0.39-val_total_loss_epoch=5.28-val_loss_generation_epoch=0.23-val_classification_loss_epoch=5.05.ckpt\"\n",
    "IMAGECLEF_CHECKPOINT_PATH = \"/home/mlmi-matthias/Caghan/mlmi-vqa/notebooks/lightning_logs/version_108/checkpoints/epoch=22-val_loss_generation_epoch=0.21.ckpt\"\n",
    "IMAGECLEF_CHECKPOINT_PATH = \"/Users/caghankoksal/Desktop/SS2022/lightning_logs/version_108/checkpoints/last.ckpt\"\n",
    "IMAGECLEF_CHECKPOINT_PATH = \"/Users/caghankoksal/Desktop/SS2022/lightning_logs/version_110/checkpoints/last.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_TRAINED_IMAGECLEF:\n",
    "    print(\"Pretrained Flamingo Model is loaded from checkpoint : \",IMAGECLEF_CHECKPOINT_PATH)\n",
    "    if os.getcwd().startswith('/home/mlmi-matthias'):\n",
    "        model.load_state_dict(torch.load(IMAGECLEF_CHECKPOINT_PATH)[\"state_dict\"],strict=False)\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(IMAGECLEF_CHECKPOINT_PATH,map_location=torch.device('cpu'))[\"state_dict\"],strict=False)\n",
    "        print(\"Checkpoint Weights are loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.load(IMAGECLEF_CHECKPOINT_PATH,map_location=torch.device('cpu'))[\"hyper_parameters\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val_img = imageclef_datamodule.val_dataset[3][\"image\"]\n",
    "val_qa_pair= imageclef_datamodule.val_dataset[3][\"qa_pair\"]\n",
    "val_qa_pair.split('answer')\n",
    "val_question =  imageclef_datamodule.val_dataset[3][\"question\"]\n",
    "val_question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from transformers import GPT2Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn as nn\n",
    "import torch.nn.functional as F\n",
    "def generate(image, context, cur_model, batch, ntok=20):\n",
    "    for _ in range(ntok):\n",
    "        out,_ = cur_model({'image': image,'input_ids': context, \"index_eoq\": batch[\"index_eoq\"],\n",
    "        \"targets\": batch[\"targets\"],\"label\": batch[\"label\"]})\n",
    "        logits = out[:, -1, :]\n",
    "        indices_to_remove = logits < torch.topk(logits, 10)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = np.NINF\n",
    "        #next_tok1 = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1).squeeze(1)\n",
    "        #print(next_tok1.shape)\n",
    "        softmax_out = F.softmax(logits, dim=-1)\n",
    "        #print(softmax_out.shape)\n",
    "        next_tok = torch.argmax(softmax_out,dim=-1,keepdim=False)\n",
    "        #print(next_tok.shape)\n",
    "        context = torch.cat([context, next_tok.unsqueeze(-1)], dim=-1)\n",
    "    return context\n",
    "\n",
    "\n",
    "tokenizer = datamodule.train_dataset.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = datamodule.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader_iter = iter(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch = next(val_loader_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"question\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.ToPILImage()(batch[\"image\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' <EOQ>'+ ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, batch, ntok=20)\n",
    "print(\"Question : \", batch[\"question\"][0])\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' <EOQ>'+ ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, batch, ntok=20)\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' <EOQ>'+ ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, batch, ntok=20)\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.ToPILImage()(batch[\"image\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context   = torch.tensor([tokenizer.encode(\n",
    "    \"<|endoftext|> <image> question: There is a pheochromocytoma in this image. Why there is a pheochromocytoma \"+\\\n",
    "            ' <EOQ>'+ ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, batch, ntok=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' <EOQ>'+ ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, batch, ntok=20)\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' <EOQ>'+ ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, batch, ntok=20)\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' <EOQ>'+ ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, batch, ntok=20)\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' <EOQ>'+ ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, batch, ntok=20)\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' <EOQ>'+ ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, batch, ntok=20)\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' <EOQ>'+ ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, batch, ntok=20)\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' <EOQ>'+ ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, batch, ntok=20)\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' <EOQ>'+ ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, batch, ntok=20)\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy_classification_task(dataloader, model):\n",
    "    # Calculate accuracy as a classification task\n",
    "    true_count = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        flamingo_logits, classification_logits = model(batch)\n",
    "        true_label = batch[\"label\"]\n",
    "        pred = torch.argmax(classification_logits,dim=1).item() \n",
    "        #print(\"Prediction is : \",pred)\n",
    "\n",
    "        if pred == true_label:\n",
    "            #print(\"true prediction\")\n",
    "            true_count +=1\n",
    "    print(\"Accuracy of the model on the classification task is \", true_count/len(dataloader) )\n",
    "    return true_count/len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_accuracy_classification_task(val_dataloader,model=model)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "val_dataloader = datamodule.val_dataloader()\n",
    "tokenizer = datamodule.train_dataset.tokenizer\n",
    "true_predictions = []\n",
    "false_predictions = []\n",
    "for batch in tqdm(val_dataloader):\n",
    "    #batch = next(val_loader_iter)\n",
    "    context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' <EOQ>'+ ' answer:')]) \n",
    "    out = generate(batch[\"image\"], context, model, batch, ntok=20)\n",
    "    models_answer = tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0].rstrip().strip()\n",
    "    correct_answer = batch[\"answer\"][0].rstrip().strip()\n",
    "    if models_answer == correct_answer:\n",
    "        correct += 1\n",
    "        true_predictions.append((models_answer, correct_answer))\n",
    "    else:\n",
    "        false_predictions.append((models_answer, correct_answer))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct/len(val_dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRUE_PREDS_FILE_NAME = 'correct_predictions_FT_ROCO_FlamingoModeON_ckpt_110.xlsx'\n",
    "FALSE_PREDS_FILE_NAME = \"false_predictions_FT_ROCO_FlamingoModeON_ckpt_110.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "correct_preds_pred = pd.DataFrame(true_predictions, columns=[\"models_answer\",\"correct_answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_preds_pred.to_excel(TRUE_PREDS_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_preds_pred = pd.DataFrame(false_predictions, columns=[\"models_answer\",\"correct_answer\"])\n",
    "false_preds_pred.to_excel(FALSE_PREDS_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_list = pd.concat([correct_preds_pred, false_preds_pred], axis=0).reset_index().drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import numpy as np\n",
    "def calculate_bleu_score():\n",
    "  bleu_per_answer = np.asarray( [ sentence_bleu(target.split(), pred.split(), weights = [1])  for pred, target in zip(concat_list[\"models_answer\"],concat_list[\"correct_answer\"])  ])\n",
    "  return np.mean(bleu_per_answer)\n",
    "\n",
    "cur_bleu = 0\n",
    "for pred, target in zip(concat_list[\"models_answer\"],concat_list[\"correct_answer\"]):\n",
    "    try:\n",
    "        cur_bleu += sentence_bleu( [target.split()], pred.split(), weights = [1])\n",
    "    except:\n",
    "        print(pred, target)\n",
    "\n",
    "cur_bleu/len(concat_list[\"models_answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEAM SEARCH DECODER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.utils import beam_decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_outs = beam_decode(cur_batch, model, tokenizer, top_k=2, beam_width=2, max_len=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_outs[0][-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in decoded_outs[0][0]:\n",
    "    #print(each)\n",
    "    print(tokenizer.decode(each[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_params = {\n",
    "    \"root\": \"/Users/caghankoksal/Desktop/imageclef/\",\n",
    "    \"batch_size\": 1,\n",
    "    \"tokenizer\": \"gpt2\",\n",
    "    \"return_size\": False,\n",
    "    \"num_data_workers\": 0,\n",
    "    \"limit_num_samples\" : None\n",
    "}\n",
    "imageclef_datamodule = ImageCLEF2021DataModule(**dataset_params,transforms=augmentations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE_OF_TOKENIZER = 50257 # mimic_datamodule.train_dataset.tokenizer.vocab_size\n",
    "LANGUAGE_MODEL = 'gpt2'\n",
    "NUM_TOKENS = VOCAB_SIZE_OF_TOKENIZER +3 if LANGUAGE_MODEL==\"gpt2\" else 31092\n",
    "FLAMINGO_EMBED_DIM = 768\n",
    "DEPTH = 12\n",
    "NUM_HEADS = 8\n",
    "ATT_HEAD_DIM = 64\n",
    "CROOS_ATT_EVERY=3\n",
    "MEDIA_TOKEN_ID = imageclef_datamodule.train_dataset.tokenizer.all_special_ids[imageclef_datamodule.train_dataset.tokenizer.all_special_tokens.index('<image>')]\n",
    "PERCEIVER_NUM_LATENTS = 64\n",
    "PERCEIVER_DEPTH = 2\n",
    "IMAGE_ENCODER = \"clip\"\n",
    "PRETRAINED_CLIP_PATH = '/Users/caghankoksal/Desktop/development/PubMedCLIP_ViT32.pth'\n",
    "PRETRAINED_GPT2_PATH = \"/Users/caghankoksal/Desktop/development/TransformerPlay/gpt2-pytorch_model.bin\"\n",
    "\n",
    "model_hyperparams = {\n",
    "    'pretrained_clip_path': PRETRAINED_CLIP_PATH,\n",
    "    'warmup_steps': 569,\n",
    "    'num_tokens': NUM_TOKENS,\n",
    "    'dim': FLAMINGO_EMBED_DIM,\n",
    "    'depth': DEPTH,\n",
    "    'num_heads': NUM_HEADS,\n",
    "    'dim_head': ATT_HEAD_DIM,\n",
    "    'cross_attn_every': CROOS_ATT_EVERY,\n",
    "    'media_token_id': MEDIA_TOKEN_ID,\n",
    "    'perceiver_num_latents': PERCEIVER_NUM_LATENTS,\n",
    "    'perceiver_depth': PERCEIVER_DEPTH,\n",
    "    'image_encoder': IMAGE_ENCODER,\n",
    "    'language_model': LANGUAGE_MODEL,\n",
    "    'pretrained_gpt2_path': PRETRAINED_GPT2_PATH,\n",
    "}\n",
    "\n",
    "for k,v in model_hyperparams.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "model = FlamingoModule(**model_hyperparams)\n",
    "#CHECKPOINT_PATH = \"/Users/caghankoksal/Desktop/SS2022/lightning_logs/version_23/checkpoints/epoch=34-val_loss=0.25-other_metric=0.00.ckpt\"\n",
    "# Pretrained on ROCO FineTuning on ImageCLEF 2021\n",
    "CHECKPOINT_PATH = \"/Users/caghankoksal/Desktop/SS2022/lightning_logs/version_31/checkpoints/epoch=28-val_loss=0.23-other_metric=0.00.ckpt\"\n",
    "model.load_state_dict(torch.load(CHECKPOINT_PATH,map_location=torch.device('cpu'))[\"state_dict\"])\n",
    "model.eval()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = imageclef_datamodule.val_dataloader()\n",
    "val_loader_iter = iter(val_dataloader)\n",
    "tokenizer = imageclef_datamodule.train_dataset.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "#batch = next(val_loader_iter)\n",
    "out = beam_decode(batch,model)\n",
    "\n",
    "models_answer = tokenizer.decode(out[0][-1][0][0]).split('answer:')[1].split('<EOC>')[0].rstrip().strip()\n",
    "correct_answer = batch[\"answer\"][0].rstrip().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_answer,correct_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE BEAM SEARCH\n",
    "\n",
    "\n",
    "\n",
    "correct = 0\n",
    "val_dataloader = imageclef_datamodule.val_dataloader()\n",
    "tokenizer = imageclef_datamodule.train_dataset.tokenizer\n",
    "true_predictions = []\n",
    "false_predictions = []\n",
    "for batch in tqdm(val_dataloader):\n",
    "    #batch = next(val_loader_iter)\n",
    "    out = beam_decode(batch,model,top_k=2,beam_width=2,max_len=100)\n",
    "\n",
    "    models_answer = tokenizer.decode(out[0][-1][0][0]).split('answer:')[1].split('<EOC>')[0].rstrip().strip()\n",
    "    correct_answer = batch[\"answer\"][0].rstrip().strip()\n",
    "    if models_answer == correct_answer:\n",
    "        correct += 1\n",
    "        true_predictions.append((models_answer, correct_answer))\n",
    "    else:\n",
    "        false_predictions.append((models_answer, correct_answer))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct/len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_predictions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "questions = [ batch[\"question\"] for batch  in imageclef_datamodule.val_dataloader() ]\n",
    "correct_preds_pred = pd.DataFrame(true_predictions, columns=[\"models_answer\",\"correct_answer\"])\n",
    "false_preds_pred = pd.DataFrame(false_predictions, columns=[\"models_answer\",\"correct_answer\"])\n",
    "questions = pd.DataFrame(questions, columns=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_false_col = pd.DataFrame([ \"True\"  if i <len(correct_preds_pred) else \"False\"  for  i in range(len(questions)) ], columns=[\"True/False\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "concat_list = pd.concat([ correct_preds_pred, false_preds_pred], axis=0).reset_index().drop(columns=[\"index\"])\n",
    "concat_list = pd.concat([questions,concat_list, true_false_col], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "cur_bleu = 0\n",
    "for pred, target in zip(concat_list[\"models_answer\"],concat_list[\"correct_answer\"]):\n",
    "    try:\n",
    "        cur_bleu += sentence_bleu( [target.split()], pred.split(), weights = [1])\n",
    "    except:\n",
    "        print(pred, target)\n",
    "\n",
    "cur_bleu/len(concat_list[\"models_answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_list.to_excel(\"../reports/predictions_all_imageclef_version31_roco_beam_decode_blue_0.1766_width_2_top_k_1_.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, install Gradio\n",
    "!pip install --quiet gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from transformers import GPT2Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn as nn\n",
    "import torch.nn.functional as F\n",
    "def generate_gradio(image, context, cur_model, ntok=20):\n",
    "    for _ in range(ntok):\n",
    "        out= cur_model({'image': image,'input_ids': context})\n",
    "        logits = out[:, -1, :]\n",
    "        indices_to_remove = logits < torch.topk(logits, 10)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = np.NINF\n",
    "        #next_tok1 = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1).squeeze(1)\n",
    "        #print(next_tok1.shape)\n",
    "        softmax_out = F.softmax(logits, dim=-1)\n",
    "        #print(softmax_out.shape)\n",
    "        next_tok = torch.argmax(softmax_out,dim=-1,keepdim=False)\n",
    "        #print(next_tok.shape)\n",
    "        context = torch.cat([context, next_tok.unsqueeze(-1)], dim=-1)\n",
    "    return context\n",
    "\n",
    "\n",
    "tokenizer = datamodule.train_dataset.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_gradio(image, question):\n",
    "    print(\"Input question\")\n",
    "    process_img = augmentations[\"val\"](image).unsqueeze(0)\n",
    "    print(\"Process_img succesfull\")\n",
    "    context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+question + ' <EOQ>'+ ' answer:')]) \n",
    "    out = generate_gradio( process_img,context, model, ntok=20)\n",
    "    #print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "    result = tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0]\n",
    "    return result\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the current disease\"\n",
    "read_im = Image.open(\"synpic16279.jpg\")\n",
    "process_img = augmentations[\"val\"](read_im).unsqueeze(0)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+question + ' <EOQ>'+ ' answer:')]) \n",
    "out = generate_gradio( process_img,context, model,20)\n",
    "    #print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "result = tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'pretrained_clip_path': PRETRAINED_CLIP_PATH,\n",
    "    'warmup_steps': 30,\n",
    "    'num_tokens': NUM_TOKENS,\n",
    "    'dim': FLAMINGO_EMBED_DIM,\n",
    "    'depth': DEPTH,\n",
    "    'num_heads': NUM_HEADS,\n",
    "    'dim_head': ATT_HEAD_DIM,\n",
    "    'cross_attn_every': CROOS_ATT_EVERY,\n",
    "    'media_token_id': MEDIA_TOKEN_ID,\n",
    "    'perceiver_num_latents': PERCEIVER_NUM_LATENTS,\n",
    "    'perceiver_depth': PERCEIVER_DEPTH,\n",
    "    'image_encoder': IMAGE_ENCODER,\n",
    "    'language_model': LANGUAGE_MODEL,\n",
    "    'pretrained_gpt2_path': PRETRAINED_GPT2_PATH,\n",
    "    'classification_mode': False,\n",
    "    'classification_num_classes': NUM_CLASSES,  # 332 if DATASET==\"IMAGECLEF\"\n",
    "    'flamingo_mode': FLAMINGO_MODE,\n",
    "    \"label_smoothing\": LABEL_SMOOTHING,\n",
    "    \"token_label_smoothing\": TOKEN_LABEL_SMOOTHING,\n",
    "    \"learning_rate\":LEARNING_RATE,\n",
    "    \"use_image_embeddings\": USE_IMAGE_EMBEDDINGS,\n",
    "    \"train_embedding_layer\": TRAIN_EMBEDDING_LAYER,\n",
    "    \"classifier_dropout\": CLASSIFIER_DROPOUT\n",
    "    }\n",
    "\n",
    "print_hyperparams(hyperparams)\n",
    "\n",
    "model = FlamingoModule(**hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_TRAINED_IMAGECLEF:\n",
    "    print(\"Pretrained Flamingo Model is loaded from checkpoint : \",IMAGECLEF_CHECKPOINT_PATH)\n",
    "    if os.getcwd().startswith('/home/mlmi-matthias'):\n",
    "        model.load_state_dict(torch.load(IMAGECLEF_CHECKPOINT_PATH)[\"state_dict\"],strict=False)\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(IMAGECLEF_CHECKPOINT_PATH,map_location=torch.device('cpu'))[\"state_dict\"],strict=False)\n",
    "        print(\"Checkpoint Weights are loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "title = \"Visual_Question_Answering on Medical Data ImageCLEF\"\n",
    "description = \"Gradio Demo for Medical Visual_Question_Answering. Upload your own image (high-resolution images are recommended) or click any one of the examples, and click \" \\\n",
    "              \"\\\"Submit\\\" and then wait for our's answer. \"\n",
    "article = \"<p style='text-align: center'><a href='https://github.com/OFA-Sys/OFA' target='_blank'>OFA Github \" \\\n",
    "          \"Repo</a></p> \"\n",
    "examples = [['demo_images/synpic16279.jpg','what is abnormal in the x-ray?' ], ['demo_images/synpic17959.jpg',  'what is most alarming about this mri?']]\n",
    "io = gr.Interface(fn=predict_gradio, inputs=[gr.inputs.Image(type='pil'), \"textbox\"], outputs=gr.outputs.Textbox(label=\"Answer\"),\n",
    "                 examples=examples,title=title, description=description,\n",
    "                  debug=True)\n",
    "io.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def calculator(num1, operation, num2):\n",
    "    if operation == \"add\":\n",
    "        return num1 + num2\n",
    "    elif operation == \"subtract\":\n",
    "        return num1 - num2\n",
    "    elif operation == \"multiply\":\n",
    "        return num1 * num2\n",
    "    elif operation == \"divide\":\n",
    "        return num1 / num2\n",
    "\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=calculator,\n",
    "    inputs=[\n",
    "            gr.Number(value=4), \n",
    "            gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \n",
    "            \"number\"\n",
    "            ],\n",
    "    outputs=\"number\",\n",
    "    examples=[\n",
    "        [5, \"add\", 3],\n",
    "        [4, \"divide\", 2],\n",
    "        [-4, \"multiply\", 2.5],\n",
    "        [0, \"subtract\", 1.2],\n",
    "    ],\n",
    "    title=\"test calculator\",\n",
    "    description=\"heres a sample **toy calculator**. enjoy!\",\n",
    ")\n",
    "\n",
    "demo.launch(share=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlmi",
   "language": "python",
   "name": "mlmi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Jun  1 2022, 11:38:51) \n[GCC 7.5.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "84dc3abdd12200a074307254a90847c7bebb194f1a3b9d1fe47efe2b88539b79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
