{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('..')\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "from src.models.multimodal.flamingo_module import FlamingoModule\n",
    "from src.datasets.imageclef_dataset import ImageCLEF2021DataModule\n",
    "from src.utils.utils import load_flamingo_weights, print_hyperparams\n",
    "\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "import torchvision.transforms as T\n",
    "from pytorch_lightning.loggers import CometLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len training dataset :  4500 Batch Size :  1 NUM_EPOCHS :  120\n",
      "Total training steps :  540000\n"
     ]
    }
   ],
   "source": [
    "augmentations = {\n",
    "        \n",
    "        'train': T.Compose([T.Resize((224,224)),\n",
    "                            T.ToTensor(),\n",
    "                            T.RandomHorizontalFlip(p=0.5),\n",
    "                            T.RandomRotation(degrees=10),\n",
    "                            T.Normalize(mean=(0.2570, 0.2570, 0.2570), std=(0.2710, 0.2710, 0.2710))\n",
    "                            ]),\n",
    "        'val': T.Compose([T.Resize((224,224)),\n",
    "                            T.ToTensor(),\n",
    "                            T.RandomHorizontalFlip(p=0.5),\n",
    "                            T.RandomRotation(degrees=10),\n",
    "                            T.Normalize(mean=(0.2570, 0.2570, 0.2570), std=(0.2710, 0.2710, 0.2710))\n",
    "                            ]),\n",
    "        'test': T.Compose([T.Resize((224,224)),\n",
    "                            T.ToTensor(),\n",
    "                            T.RandomHorizontalFlip(p=0.5),\n",
    "                            T.RandomRotation(degrees=10),   \n",
    "                            T.Normalize(mean=(0.2570, 0.2570, 0.2570), std=(0.2710, 0.2710, 0.2710))\n",
    "                            ])\n",
    "    }\n",
    "\n",
    "    \n",
    "# Hyperparameters\n",
    "NUM_DATA_WORKERS  = 2\n",
    "ONLY_IMAGES = False\n",
    "BATCH_SIZE = 1\n",
    "NUM_EPOCHS = 120\n",
    "LIMIT_NUM_SAMPLES = None\n",
    "DATASET = \"IMAGECLEF\"\n",
    "LOAD_TRAINED_IMAGECLEF = True\n",
    "\n",
    "\n",
    "if os.getcwd().startswith('/home/mlmi-matthias'):\n",
    "    ACCELERATOR = \"gpu\"\n",
    "    DEVICES = [6,7]\n",
    "    PRETRAINED_CLIP_PATH = '/home/mlmi-matthias/Caghan/pretrained_models/PubMedCLIP_ViT32.pth'\n",
    "    PRETRAINED_GPT2_PATH = \"/home/mlmi-matthias/Caghan/pretrained_models/gpt2-pytorch_model.bin\"\n",
    "    MIMIC_CXR_DCM_PATH = '/home/mlmi-matthias/physionet.org/files/mimic-cxr/2.0.0/files/'\n",
    "    MIMIC_CXR_JPG_PATH = \"/home/mlmi-matthias/physionet.org/files/mimic-cxr-jpg/2.0.0/files/\"\n",
    "    SPLIT_PATH = '/home/mlmi-matthias/Caghan/mlmi-vqa/data/external/'\n",
    "    IMAGECLEF_PATH ='/home/mlmi-matthias/imageclef/'\n",
    "    #CHECKPOINT_PATH = \"/home/mlmi-matthias/Caghan/mlmi-vqa/notebooks/lightning_logs/version_20/checkpoints/epoch=114-val_loss=0.84-other_metric=0.00.ckpt\"\n",
    "    # Latest ROCO Training \n",
    "    CHECKPOINT_PATH =\"/home/mlmi-matthias/Caghan/mlmi-vqa/notebooks/lightning_logs/version_77/checkpoints/epoch=61-val_loss_generation_epoch=1.80.ckpt\"\n",
    "    ANSWERS_LIST_PATH = '/home/mlmi-matthias/Caghan/mlmi-vqa//data/external/answer_list_imageclef.txt'\n",
    "    IMAGECLEF_CHECKPOINT_PATH = \"/home/mlmi-matthias/Caghan/mlmi-vqa/notebooks/lightning_logs/version_101/checkpoints/last.ckpt\"\n",
    "\n",
    "\n",
    "elif os.getcwd().startswith('/Users/caghankoksal'):\n",
    "    PRETRAINED_CLIP_PATH = '/Users/caghankoksal/Desktop/development/PubMedCLIP_ViT32.pth'\n",
    "    PRETRAINED_GPT2_PATH = \"/Users/caghankoksal/Desktop/development/TransformerPlay/gpt2-pytorch_model.bin\"\n",
    "    ACCELERATOR = \"cpu\"\n",
    "    DEVICES = 1\n",
    "    MIMIC_CXR_DCM_PATH = '/Users/caghankoksal/Desktop/development/Flamingo-playground/physionet.org/files/mimic-cxr/2.0.0/files/'\n",
    "    MIMIC_CXR_JPG_PATH = '/Users/caghankoksal/Desktop/development/physionet.org/files/mimic-cxr-jpg/2.0.0/files/'\n",
    "    SPLIT_PATH = '/Users/caghankoksal/Desktop/SS2022/mlmi-vqa/data/external/'\n",
    "    IMAGECLEF_PATH = \"/Users/caghankoksal/Desktop/imageclef/\"\n",
    "    CHECKPOINT_PATH = \"/Users/caghankoksal/Desktop/SS2022/lightning_logs/version_77/checkpoints/epoch=66-val_loss_generation_epoch=1.80.ckpt\"\n",
    "    ANSWERS_LIST_PATH = '/Users/caghankoksal/Desktop/SS2022/mlmi-vqa/data/external/answer_list_imageclef.txt'\n",
    "    IMAGECLEF_CHECKPOINT_PATH = \"/Users/caghankoksal/Desktop/SS2022/lightning_logs/version_102/checkpoints/last.ckpt\"\n",
    "\n",
    "\n",
    "IMAGE_TYPE = \"jpg\"\n",
    "TOKENIZER  = \"gpt2\"\n",
    "PREPROCESSED = True\n",
    "RETURN_IDX_EOC = True\n",
    "\n",
    "dataset_hyperparameters = {\n",
    "    \"root\": IMAGECLEF_PATH,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"tokenizer\": TOKENIZER,\n",
    "    \"num_data_workers\": NUM_DATA_WORKERS,\n",
    "    \"return_size\": False,\n",
    "    \"answers_list_path\": ANSWERS_LIST_PATH,\n",
    "    \"return_idx_answer_eoc\": RETURN_IDX_EOC,\n",
    "    \"transforms\": augmentations,\n",
    "    \"limit_num_samples\": LIMIT_NUM_SAMPLES,\n",
    "}\n",
    "\n",
    "\n",
    "datamodule = ImageCLEF2021DataModule(**dataset_hyperparameters)\n",
    "\n",
    "\n",
    "train_loader = datamodule.train_dataloader()\n",
    "val_loader = datamodule.val_dataloader()\n",
    "\n",
    "print(\"Len training dataset : \", len(datamodule.train_dataset),\n",
    "    \"Batch Size : \", BATCH_SIZE, \"NUM_EPOCHS : \",NUM_EPOCHS )\n",
    "print(\"Total training steps : \", len(datamodule.train_dataset)//BATCH_SIZE*NUM_EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    cur_images = batch['image']\n",
    "    print(cur_images.shape )\n",
    "    cur_batch = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|endoftext|> <image> question: is this a normal gastrointestinal image? <EOQ> answer: yes <EOC>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_batch['qa_pair']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_batch[\"index_answer\"]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['image', 'question', 'answer', 'label', 'index_answer', 'index_eoc', 'index_eoq', 'input_ids', 'token_type_ids', 'targets', 'ID', 'qa_pair'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_batch[\"input_ids\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import pytorch_lightning as pl\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM,PreTrainedTokenizerFast, GPT2Tokenizer\n",
    "from PIL import Image\n",
    "from tqdm import tqdm as tqdm\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from src.datasets.imageclef_dataset import ImageCLEF2021DataModule\n",
    "from src.models.multimodal.flamingo_module import FlamingoModule\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /var/folders/61/9c2llh9n2pjb81c4dmhmb67w0000gn/T/tmp0os2_09c\n",
      "INFO:torch.distributed.nn.jit.instantiator:Writing /var/folders/61/9c2llh9n2pjb81c4dmhmb67w0000gn/T/tmp0os2_09c/_remote_module_non_sriptable.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained_clip_path /Users/caghankoksal/Desktop/development/PubMedCLIP_ViT32.pth\n",
      "warmup_steps 30\n",
      "num_tokens 50261\n",
      "dim 768\n",
      "depth 12\n",
      "num_heads 8\n",
      "dim_head 64\n",
      "cross_attn_every 3\n",
      "media_token_id 50258\n",
      "perceiver_num_latents 64\n",
      "perceiver_depth 2\n",
      "image_encoder clip\n",
      "language_model gpt2\n",
      "pretrained_gpt2_path /Users/caghankoksal/Desktop/development/TransformerPlay/gpt2-pytorch_model.bin\n",
      "classification_mode True\n",
      "classification_num_classes 332\n",
      "flamingo_mode True\n",
      "label_smoothing 0.5\n",
      "token_label_smoothing 0.0\n",
      "learning_rate 0.0001\n",
      "use_image_embeddings True\n",
      "train_embedding_layer True\n",
      "classifier_dropout 0.5\n",
      "Clip architecture is being loaded\n",
      "Clip pretrained weights are being loaded\n",
      "Flamingo is being initialized with  gpt2  as language model\n",
      "GPT 2 Weights are loading...\n",
      "Loaded GPT2 weights and Embeddings num_weights loaded :  156\n",
      "Pretrained Flamingo Model is loaded from checkpoint :  /Users/caghankoksal/Desktop/SS2022/lightning_logs/version_77/checkpoints/epoch=66-val_loss_generation_epoch=1.80.ckpt\n"
     ]
    }
   ],
   "source": [
    "# MODEL HPRAMS\n",
    "VOCAB_SIZE_OF_TOKENIZER = 50257 # mimic_datamodule.train_dataset.tokenizer.vocab_size\n",
    "LANGUAGE_MODEL = 'gpt2'\n",
    "NUM_TOKENS = VOCAB_SIZE_OF_TOKENIZER +4 if LANGUAGE_MODEL==\"gpt2\" else 31092\n",
    "FLAMINGO_EMBED_DIM = 768\n",
    "DEPTH = 12\n",
    "NUM_HEADS = 8\n",
    "ATT_HEAD_DIM = 64\n",
    "CROOS_ATT_EVERY=3\n",
    "MEDIA_TOKEN_ID = datamodule.train_dataset.tokenizer.\\\n",
    "    all_special_ids[datamodule.train_dataset.tokenizer.all_special_tokens.index('<image>')]\n",
    "PERCEIVER_NUM_LATENTS = 64\n",
    "PERCEIVER_DEPTH = 2\n",
    "IMAGE_ENCODER = \"clip\"\n",
    "CLASSIFICATION_MODE = True\n",
    "NUM_CLASSES = 332\n",
    "FLAMINGO_MODE = True\n",
    "LABEL_SMOOTHING = 0.5\n",
    "# Label smoothing for classification task\n",
    "TOKEN_LABEL_SMOOTHING = 0.0\n",
    "GRADIENT_CLIP_VAL = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "USE_IMAGE_EMBEDDINGS = True\n",
    "TRAIN_EMBEDDING_LAYER = True\n",
    "CLASSIFIER_DROPOUT = 0.5\n",
    "\n",
    "\n",
    "hyperparams = {\n",
    "    'pretrained_clip_path': PRETRAINED_CLIP_PATH,\n",
    "    'warmup_steps': 30,\n",
    "    'num_tokens': NUM_TOKENS,\n",
    "    'dim': FLAMINGO_EMBED_DIM,\n",
    "    'depth': DEPTH,\n",
    "    'num_heads': NUM_HEADS,\n",
    "    'dim_head': ATT_HEAD_DIM,\n",
    "    'cross_attn_every': CROOS_ATT_EVERY,\n",
    "    'media_token_id': MEDIA_TOKEN_ID,\n",
    "    'perceiver_num_latents': PERCEIVER_NUM_LATENTS,\n",
    "    'perceiver_depth': PERCEIVER_DEPTH,\n",
    "    'image_encoder': IMAGE_ENCODER,\n",
    "    'language_model': LANGUAGE_MODEL,\n",
    "    'pretrained_gpt2_path': PRETRAINED_GPT2_PATH,\n",
    "    'classification_mode': CLASSIFICATION_MODE,\n",
    "    'classification_num_classes': NUM_CLASSES,  # 332 if DATASET==\"IMAGECLEF\"\n",
    "    'flamingo_mode': FLAMINGO_MODE,\n",
    "    \"label_smoothing\": LABEL_SMOOTHING,\n",
    "    \"token_label_smoothing\": TOKEN_LABEL_SMOOTHING,\n",
    "    \"learning_rate\":LEARNING_RATE,\n",
    "    \"use_image_embeddings\": USE_IMAGE_EMBEDDINGS,\n",
    "    \"train_embedding_layer\": TRAIN_EMBEDDING_LAYER,\n",
    "    \"classifier_dropout\": CLASSIFIER_DROPOUT\n",
    "    }\n",
    "\n",
    "print_hyperparams(hyperparams)\n",
    "\n",
    "model = FlamingoModule(**hyperparams)\n",
    "START_FROM_CHECKPOINT = True\n",
    "\n",
    "if START_FROM_CHECKPOINT:\n",
    "    print(\"Pretrained Flamingo Model is loaded from checkpoint : \",CHECKPOINT_PATH)\n",
    "    if os.getcwd().startswith('/home/mlmi-matthias'):\n",
    "        model.load_state_dict(torch.load(CHECKPOINT_PATH)[\"state_dict\"],strict=False)\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(CHECKPOINT_PATH,map_location=torch.device('cpu'))[\"state_dict\"],strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: /home/mlmi-matthias/Caghan/mlmi-vqa/notebooks/lightning_logs/version_101/checkpoints: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls \"/home/mlmi-matthias/Caghan/mlmi-vqa/notebooks/lightning_logs/version_101/checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/caghankoksal/Desktop/SS2022/mlmi-vqa/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#IMAGECLEF_CHECKPOINT_PATH ='/home/mlmi-matthias/Caghan/mlmi-vqa/notebooks/lightning_logs/version_95/checkpoints/\\\n",
    "#epoch=94-val_acc_epoch=0.41-val_total_loss_epoch=4.28-val_loss_generation_epoch=0.22-val_classification_loss_epoch=4.06.ckpt'\n",
    "#IMAGECLEF_CHECKPOINT_PATH = \"/home/mlmi-matthias/Caghan/mlmi-vqa/notebooks/lightning_logs/version_99/checkpoints/epoch=82-val_acc_epoch=0.39-val_total_loss_epoch=5.28-val_loss_generation_epoch=0.23-val_classification_loss_epoch=5.05.ckpt\"\n",
    "IMAGECLEF_CHECKPOINT_PATH = \"/home/mlmi-matthias/Caghan/mlmi-vqa/notebooks/lightning_logs/version_108/checkpoints/epoch=22-val_loss_generation_epoch=0.21.ckpt\"\n",
    "IMAGECLEF_CHECKPOINT_PATH = \"/Users/caghankoksal/Desktop/SS2022/lightning_logs/version_108/checkpoints/last.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained Flamingo Model is loaded from checkpoint :  /Users/caghankoksal/Desktop/SS2022/lightning_logs/version_108/checkpoints/last.ckpt\n",
      "Checkpoint Weights are loaded\n"
     ]
    }
   ],
   "source": [
    "if LOAD_TRAINED_IMAGECLEF:\n",
    "    print(\"Pretrained Flamingo Model is loaded from checkpoint : \",IMAGECLEF_CHECKPOINT_PATH)\n",
    "    if os.getcwd().startswith('/home/mlmi-matthias'):\n",
    "        model.load_state_dict(torch.load(IMAGECLEF_CHECKPOINT_PATH)[\"state_dict\"],strict=False)\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(IMAGECLEF_CHECKPOINT_PATH,map_location=torch.device('cpu'))[\"state_dict\"],strict=False)\n",
    "        print(\"Checkpoint Weights are loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pretrained_clip_path': '/home/mlmi-matthias/Caghan/pretrained_models/PubMedCLIP_ViT32.pth',\n",
       " 'warmup_steps': 30,\n",
       " 'num_tokens': 50261,\n",
       " 'dim': 768,\n",
       " 'depth': 12,\n",
       " 'num_heads': 8,\n",
       " 'dim_head': 64,\n",
       " 'media_token_id': 50258,\n",
       " 'cross_attn_every': 3,\n",
       " 'perceiver_num_latents': 64,\n",
       " 'perceiver_depth': 2,\n",
       " 'image_encoder': 'clip',\n",
       " 'language_model': 'gpt2',\n",
       " 'pretrained_gpt2_path': '/home/mlmi-matthias/Caghan/pretrained_models/gpt2-pytorch_model.bin',\n",
       " 'classification_mode': False,\n",
       " 'classification_num_classes': 332,\n",
       " 'flamingo_mode': False,\n",
       " 'label_smoothing': 0.2,\n",
       " 'token_label_smoothing': 0.0,\n",
       " 'learning_rate': 0.0001,\n",
       " 'use_image_embeddings': True,\n",
       " 'train_embedding_layer': True,\n",
       " 'classifier_dropout': 0.2,\n",
       " 'use_positional_embedding': True}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(IMAGECLEF_CHECKPOINT_PATH,map_location=torch.device('cpu'))[\"hyper_parameters\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FlamingoModule(\n",
       "  (flamingo_palm): FlamingoModel(\n",
       "    (token_emb): Embedding(50261, 768)\n",
       "    (wpe): Embedding(256, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (img_encoder): VisionTransformer(\n",
       "      (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (transformer): Transformer(\n",
       "        (resblocks): ModuleList(\n",
       "          (0): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (perceiver_resampler): PerceiverResampler(\n",
       "      (layers): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): PerceiverAttention(\n",
       "            (norm_media): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm_latents): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (to_q): Linear(in_features=768, out_features=512, bias=False)\n",
       "            (to_kv): Linear(in_features=768, out_features=1024, bias=False)\n",
       "            (to_out): Linear(in_features=512, out_features=768, bias=False)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (2): GELU()\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (1): ModuleList(\n",
       "          (0): PerceiverAttention(\n",
       "            (norm_media): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm_latents): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (to_q): Linear(in_features=768, out_features=512, bias=False)\n",
       "            (to_kv): Linear(in_features=768, out_features=1024, bias=False)\n",
       "            (to_out): Linear(in_features=512, out_features=768, bias=False)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (2): GELU()\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (img_encoder_outdim_layer): Linear(in_features=512, out_features=768, bias=True)\n",
       "    (layers): ModuleList(\n",
       "      (0): ModuleList(\n",
       "        (0): Residual(\n",
       "          (fn): TransformerBlockGPT2(\n",
       "            (attn): Attention(\n",
       "              (c_attn): Conv1D()\n",
       "              (softmax): Softmax(dim=-1)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (c_proj): Conv1D()\n",
       "            )\n",
       "            (feedforward): FeedForward(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): GatedCrossAttentionBlock(\n",
       "          (attn): MaskedCrossAttention(\n",
       "            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (to_q): Linear(in_features=768, out_features=512, bias=False)\n",
       "            (to_kv): Linear(in_features=768, out_features=1024, bias=False)\n",
       "            (to_out): Linear(in_features=512, out_features=768, bias=False)\n",
       "          )\n",
       "          (ff): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (2): GELU()\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): ModuleList(\n",
       "        (0): Residual(\n",
       "          (fn): TransformerBlockGPT2(\n",
       "            (attn): Attention(\n",
       "              (c_attn): Conv1D()\n",
       "              (softmax): Softmax(dim=-1)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (c_proj): Conv1D()\n",
       "            )\n",
       "            (feedforward): FeedForward(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): None\n",
       "      )\n",
       "      (2): ModuleList(\n",
       "        (0): Residual(\n",
       "          (fn): TransformerBlockGPT2(\n",
       "            (attn): Attention(\n",
       "              (c_attn): Conv1D()\n",
       "              (softmax): Softmax(dim=-1)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (c_proj): Conv1D()\n",
       "            )\n",
       "            (feedforward): FeedForward(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): None\n",
       "      )\n",
       "      (3): ModuleList(\n",
       "        (0): Residual(\n",
       "          (fn): TransformerBlockGPT2(\n",
       "            (attn): Attention(\n",
       "              (c_attn): Conv1D()\n",
       "              (softmax): Softmax(dim=-1)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (c_proj): Conv1D()\n",
       "            )\n",
       "            (feedforward): FeedForward(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): GatedCrossAttentionBlock(\n",
       "          (attn): MaskedCrossAttention(\n",
       "            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (to_q): Linear(in_features=768, out_features=512, bias=False)\n",
       "            (to_kv): Linear(in_features=768, out_features=1024, bias=False)\n",
       "            (to_out): Linear(in_features=512, out_features=768, bias=False)\n",
       "          )\n",
       "          (ff): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (2): GELU()\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): ModuleList(\n",
       "        (0): Residual(\n",
       "          (fn): TransformerBlockGPT2(\n",
       "            (attn): Attention(\n",
       "              (c_attn): Conv1D()\n",
       "              (softmax): Softmax(dim=-1)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (c_proj): Conv1D()\n",
       "            )\n",
       "            (feedforward): FeedForward(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): None\n",
       "      )\n",
       "      (5): ModuleList(\n",
       "        (0): Residual(\n",
       "          (fn): TransformerBlockGPT2(\n",
       "            (attn): Attention(\n",
       "              (c_attn): Conv1D()\n",
       "              (softmax): Softmax(dim=-1)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (c_proj): Conv1D()\n",
       "            )\n",
       "            (feedforward): FeedForward(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): None\n",
       "      )\n",
       "      (6): ModuleList(\n",
       "        (0): Residual(\n",
       "          (fn): TransformerBlockGPT2(\n",
       "            (attn): Attention(\n",
       "              (c_attn): Conv1D()\n",
       "              (softmax): Softmax(dim=-1)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (c_proj): Conv1D()\n",
       "            )\n",
       "            (feedforward): FeedForward(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): GatedCrossAttentionBlock(\n",
       "          (attn): MaskedCrossAttention(\n",
       "            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (to_q): Linear(in_features=768, out_features=512, bias=False)\n",
       "            (to_kv): Linear(in_features=768, out_features=1024, bias=False)\n",
       "            (to_out): Linear(in_features=512, out_features=768, bias=False)\n",
       "          )\n",
       "          (ff): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (2): GELU()\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): ModuleList(\n",
       "        (0): Residual(\n",
       "          (fn): TransformerBlockGPT2(\n",
       "            (attn): Attention(\n",
       "              (c_attn): Conv1D()\n",
       "              (softmax): Softmax(dim=-1)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (c_proj): Conv1D()\n",
       "            )\n",
       "            (feedforward): FeedForward(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): None\n",
       "      )\n",
       "      (8): ModuleList(\n",
       "        (0): Residual(\n",
       "          (fn): TransformerBlockGPT2(\n",
       "            (attn): Attention(\n",
       "              (c_attn): Conv1D()\n",
       "              (softmax): Softmax(dim=-1)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (c_proj): Conv1D()\n",
       "            )\n",
       "            (feedforward): FeedForward(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): None\n",
       "      )\n",
       "      (9): ModuleList(\n",
       "        (0): Residual(\n",
       "          (fn): TransformerBlockGPT2(\n",
       "            (attn): Attention(\n",
       "              (c_attn): Conv1D()\n",
       "              (softmax): Softmax(dim=-1)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (c_proj): Conv1D()\n",
       "            )\n",
       "            (feedforward): FeedForward(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): GatedCrossAttentionBlock(\n",
       "          (attn): MaskedCrossAttention(\n",
       "            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (to_q): Linear(in_features=768, out_features=512, bias=False)\n",
       "            (to_kv): Linear(in_features=768, out_features=1024, bias=False)\n",
       "            (to_out): Linear(in_features=512, out_features=768, bias=False)\n",
       "          )\n",
       "          (ff): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (2): GELU()\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): ModuleList(\n",
       "        (0): Residual(\n",
       "          (fn): TransformerBlockGPT2(\n",
       "            (attn): Attention(\n",
       "              (c_attn): Conv1D()\n",
       "              (softmax): Softmax(dim=-1)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (c_proj): Conv1D()\n",
       "            )\n",
       "            (feedforward): FeedForward(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): None\n",
       "      )\n",
       "      (11): ModuleList(\n",
       "        (0): Residual(\n",
       "          (fn): TransformerBlockGPT2(\n",
       "            (attn): Attention(\n",
       "              (c_attn): Conv1D()\n",
       "              (softmax): Softmax(dim=-1)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (c_proj): Conv1D()\n",
       "            )\n",
       "            (feedforward): FeedForward(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): None\n",
       "      )\n",
       "    )\n",
       "    (to_logits): Sequential(\n",
       "      (0): LayerNorm()\n",
       "      (1): Linear(in_features=768, out_features=50261, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): LayerNorm()\n",
       "    (1): Dropout(p=0.1, inplace=False)\n",
       "    (2): Linear(in_features=1536, out_features=332, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val_img = imageclef_datamodule.val_dataset[3][\"image\"]\n",
    "val_qa_pair= imageclef_datamodule.val_dataset[3][\"qa_pair\"]\n",
    "val_qa_pair.split('answer')\n",
    "val_question =  imageclef_datamodule.val_dataset[3][\"question\"]\n",
    "val_question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from transformers import GPT2Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn as nn\n",
    "import torch.nn.functional as F\n",
    "def generate(image, context, cur_model, batch, ntok=20):\n",
    "    for _ in range(ntok):\n",
    "        out,_ = cur_model({'image': image,'input_ids': context, \"index_eoq\": batch[\"index_eoq\"],\n",
    "        \"targets\": batch[\"targets\"],\"label\": batch[\"label\"]})\n",
    "        logits = out[:, -1, :]\n",
    "        indices_to_remove = logits < torch.topk(logits, 10)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = np.NINF\n",
    "        #next_tok1 = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1).squeeze(1)\n",
    "        #print(next_tok1.shape)\n",
    "        softmax_out = F.softmax(logits, dim=-1)\n",
    "        #print(softmax_out.shape)\n",
    "        next_tok = torch.argmax(softmax_out,dim=-1,keepdim=False)\n",
    "        #print(next_tok.shape)\n",
    "        context = torch.cat([context, next_tok.unsqueeze(-1)], dim=-1)\n",
    "    return context\n",
    "\n",
    "\n",
    "tokenizer = datamodule.train_dataset.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = datamodule.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader_iter = iter(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch = next(val_loader_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch[\"question\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch[\"image\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's answer :   acute appendicitis \n",
      "Correct Answer: transitional cell carcinoma, bladder\n"
     ]
    }
   ],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' <EOQ>'+ ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, batch, ntok=20)\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's answer :   acute appendicitis \n",
      "Correct Answer: acute appendicitis\n"
     ]
    }
   ],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' <EOQ>'+ ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, batch, ntok=20)\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's answer :   neuroblastoma \n",
      "Correct Answer: hemangioma, soft-tissue\n"
     ]
    }
   ],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' <EOQ>'+ ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, batch, ntok=20)\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's answer :   renal cell carcinoma \n",
      "Correct Answer: renal cell carcinoma\n"
     ]
    }
   ],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' <EOQ>'+ ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, batch, ntok=20)\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's answer :   liver hemangioma \n",
      "Correct Answer: renal arteriovenous malformation\n"
     ]
    }
   ],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' <EOQ>'+ ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, batch, ntok=20)\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's answer :   pericardial cyst \n",
      "Correct Answer: pericardial cyst\n"
     ]
    }
   ],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' <EOQ>'+ ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, batch, ntok=20)\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's answer :   cholelithiasis \n",
      "Correct Answer: acute appendicitis\n"
     ]
    }
   ],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' <EOQ>'+ ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, batch, ntok=20)\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's answer :   sarcoidosis \n",
      "Correct Answer: neurofibromatosis-1, nf1, nf-1\n"
     ]
    }
   ],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' <EOQ>'+ ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, batch, ntok=20)\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's answer :   liver hemangioma \n",
      "Correct Answer: adrenal adenoma\n"
     ]
    }
   ],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' <EOQ>'+ ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, batch, ntok=20)\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's answer :   osteochondroma \n",
      "Correct Answer: cavernous hemangioma\n"
     ]
    }
   ],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' <EOQ>'+ ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, batch, ntok=20)\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's answer :   hepatic hemangioma \n",
      "Correct Answer: liver hemangioma\n"
     ]
    }
   ],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' <EOQ>'+ ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, batch, ntok=20)\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy_classification_task(dataloader, model):\n",
    "    # Calculate accuracy as a classification task\n",
    "    true_count = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        flamingo_logits, classification_logits = model(batch)\n",
    "        true_label = batch[\"label\"]\n",
    "        pred = torch.argmax(classification_logits,dim=1).item() \n",
    "        #print(\"Prediction is : \",pred)\n",
    "\n",
    "        if pred == true_label:\n",
    "            #print(\"true prediction\")\n",
    "            true_count +=1\n",
    "    print(\"Accuracy of the model on the classification task is \", true_count/len(dataloader) )\n",
    "    return true_count/len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:21<00:00,  6.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the classification task is  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_accuracy_classification_task(val_dataloader,model=model)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [16:59<00:00,  2.04s/it]\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "val_dataloader = datamodule.val_dataloader()\n",
    "tokenizer = datamodule.train_dataset.tokenizer\n",
    "true_predictions = []\n",
    "false_predictions = []\n",
    "for batch in tqdm(val_dataloader):\n",
    "    #batch = next(val_loader_iter)\n",
    "    context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' <EOQ>'+ ' answer:')]) \n",
    "    out = generate(batch[\"image\"], context, model, batch, ntok=20)\n",
    "    models_answer = tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0].rstrip().strip()\n",
    "    correct_answer = batch[\"answer\"][0].rstrip().strip()\n",
    "    if models_answer == correct_answer:\n",
    "        correct += 1\n",
    "        true_predictions.append((models_answer, correct_answer))\n",
    "    else:\n",
    "        false_predictions.append((models_answer, correct_answer))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.244"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct/len(val_dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRUE_PREDS_FILE_NAME = 'correct_predictions_FT_ROCO_FlamingoModeON_ckpt_108.xlsx'\n",
    "FALSE_PREDS_FILE_NAME = \"false_predictions_FT_ROCO_FlamingoModeON_ckpt_108.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "correct_preds_pred = pd.DataFrame(true_predictions, columns=[\"models_answer\",\"correct_answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_preds_pred.to_excel(TRUE_PREDS_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_preds_pred = pd.DataFrame(false_predictions, columns=[\"models_answer\",\"correct_answer\"])\n",
    "false_preds_pred.to_excel(FALSE_PREDS_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_list = pd.concat([correct_preds_pred, false_preds_pred], axis=0).reset_index().drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2732949067943755"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import numpy as np\n",
    "def calculate_bleu_score():\n",
    "  bleu_per_answer = np.asarray( [ sentence_bleu(target.split(), pred.split(), weights = [1])  for pred, target in zip(concat_list[\"models_answer\"],concat_list[\"correct_answer\"])  ])\n",
    "  return np.mean(bleu_per_answer)\n",
    "\n",
    "cur_bleu = 0\n",
    "for pred, target in zip(concat_list[\"models_answer\"],concat_list[\"correct_answer\"]):\n",
    "    try:\n",
    "        cur_bleu += sentence_bleu( [target.split()], pred.split(), weights = [1])\n",
    "    except:\n",
    "        print(pred, target)\n",
    "\n",
    "cur_bleu/len(concat_list[\"models_answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEAM SEARCH DECODER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.utils import beam_decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_outs = beam_decode(cur_batch, model, tokenizer, top_k=2, beam_width=2, max_len=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_outs[0][-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in decoded_outs[0][0]:\n",
    "    #print(each)\n",
    "    print(tokenizer.decode(each[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_params = {\n",
    "    \"root\": \"/Users/caghankoksal/Desktop/imageclef/\",\n",
    "    \"batch_size\": 1,\n",
    "    \"tokenizer\": \"gpt2\",\n",
    "    \"return_size\": False,\n",
    "    \"num_data_workers\": 0,\n",
    "    \"limit_num_samples\" : None\n",
    "}\n",
    "imageclef_datamodule = ImageCLEF2021DataModule(**dataset_params,transforms=augmentations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE_OF_TOKENIZER = 50257 # mimic_datamodule.train_dataset.tokenizer.vocab_size\n",
    "LANGUAGE_MODEL = 'gpt2'\n",
    "NUM_TOKENS = VOCAB_SIZE_OF_TOKENIZER +3 if LANGUAGE_MODEL==\"gpt2\" else 31092\n",
    "FLAMINGO_EMBED_DIM = 768\n",
    "DEPTH = 12\n",
    "NUM_HEADS = 8\n",
    "ATT_HEAD_DIM = 64\n",
    "CROOS_ATT_EVERY=3\n",
    "MEDIA_TOKEN_ID = imageclef_datamodule.train_dataset.tokenizer.all_special_ids[imageclef_datamodule.train_dataset.tokenizer.all_special_tokens.index('<image>')]\n",
    "PERCEIVER_NUM_LATENTS = 64\n",
    "PERCEIVER_DEPTH = 2\n",
    "IMAGE_ENCODER = \"clip\"\n",
    "PRETRAINED_CLIP_PATH = '/Users/caghankoksal/Desktop/development/PubMedCLIP_ViT32.pth'\n",
    "PRETRAINED_GPT2_PATH = \"/Users/caghankoksal/Desktop/development/TransformerPlay/gpt2-pytorch_model.bin\"\n",
    "\n",
    "model_hyperparams = {\n",
    "    'pretrained_clip_path': PRETRAINED_CLIP_PATH,\n",
    "    'warmup_steps': 569,\n",
    "    'num_tokens': NUM_TOKENS,\n",
    "    'dim': FLAMINGO_EMBED_DIM,\n",
    "    'depth': DEPTH,\n",
    "    'num_heads': NUM_HEADS,\n",
    "    'dim_head': ATT_HEAD_DIM,\n",
    "    'cross_attn_every': CROOS_ATT_EVERY,\n",
    "    'media_token_id': MEDIA_TOKEN_ID,\n",
    "    'perceiver_num_latents': PERCEIVER_NUM_LATENTS,\n",
    "    'perceiver_depth': PERCEIVER_DEPTH,\n",
    "    'image_encoder': IMAGE_ENCODER,\n",
    "    'language_model': LANGUAGE_MODEL,\n",
    "    'pretrained_gpt2_path': PRETRAINED_GPT2_PATH,\n",
    "}\n",
    "\n",
    "for k,v in model_hyperparams.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "model = FlamingoModule(**model_hyperparams)\n",
    "#CHECKPOINT_PATH = \"/Users/caghankoksal/Desktop/SS2022/lightning_logs/version_23/checkpoints/epoch=34-val_loss=0.25-other_metric=0.00.ckpt\"\n",
    "# Pretrained on ROCO FineTuning on ImageCLEF 2021\n",
    "CHECKPOINT_PATH = \"/Users/caghankoksal/Desktop/SS2022/lightning_logs/version_31/checkpoints/epoch=28-val_loss=0.23-other_metric=0.00.ckpt\"\n",
    "model.load_state_dict(torch.load(CHECKPOINT_PATH,map_location=torch.device('cpu'))[\"state_dict\"])\n",
    "model.eval()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = imageclef_datamodule.val_dataloader()\n",
    "val_loader_iter = iter(val_dataloader)\n",
    "tokenizer = imageclef_datamodule.train_dataset.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "#batch = next(val_loader_iter)\n",
    "out = beam_decode(batch,model)\n",
    "\n",
    "models_answer = tokenizer.decode(out[0][-1][0][0]).split('answer:')[1].split('<EOC>')[0].rstrip().strip()\n",
    "correct_answer = batch[\"answer\"][0].rstrip().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_answer,correct_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE BEAM SEARCH\n",
    "\n",
    "\n",
    "\n",
    "correct = 0\n",
    "val_dataloader = imageclef_datamodule.val_dataloader()\n",
    "tokenizer = imageclef_datamodule.train_dataset.tokenizer\n",
    "true_predictions = []\n",
    "false_predictions = []\n",
    "for batch in tqdm(val_dataloader):\n",
    "    #batch = next(val_loader_iter)\n",
    "    out = beam_decode(batch,model,top_k=2,beam_width=2,max_len=100)\n",
    "\n",
    "    models_answer = tokenizer.decode(out[0][-1][0][0]).split('answer:')[1].split('<EOC>')[0].rstrip().strip()\n",
    "    correct_answer = batch[\"answer\"][0].rstrip().strip()\n",
    "    if models_answer == correct_answer:\n",
    "        correct += 1\n",
    "        true_predictions.append((models_answer, correct_answer))\n",
    "    else:\n",
    "        false_predictions.append((models_answer, correct_answer))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct/len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_predictions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "questions = [ batch[\"question\"] for batch  in imageclef_datamodule.val_dataloader() ]\n",
    "correct_preds_pred = pd.DataFrame(true_predictions, columns=[\"models_answer\",\"correct_answer\"])\n",
    "false_preds_pred = pd.DataFrame(false_predictions, columns=[\"models_answer\",\"correct_answer\"])\n",
    "questions = pd.DataFrame(questions, columns=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_false_col = pd.DataFrame([ \"True\"  if i <len(correct_preds_pred) else \"False\"  for  i in range(len(questions)) ], columns=[\"True/False\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "concat_list = pd.concat([ correct_preds_pred, false_preds_pred], axis=0).reset_index().drop(columns=[\"index\"])\n",
    "concat_list = pd.concat([questions,concat_list, true_false_col], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "cur_bleu = 0\n",
    "for pred, target in zip(concat_list[\"models_answer\"],concat_list[\"correct_answer\"]):\n",
    "    try:\n",
    "        cur_bleu += sentence_bleu( [target.split()], pred.split(), weights = [1])\n",
    "    except:\n",
    "        print(pred, target)\n",
    "\n",
    "cur_bleu/len(concat_list[\"models_answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_list.to_excel(\"../reports/predictions_all_imageclef_version31_roco_beam_decode_blue_0.1766_width_2_top_k_1_.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "892a7f8aeabe86b99d45932805d162784b758c544538f3ce4737e4a115db3cfd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
