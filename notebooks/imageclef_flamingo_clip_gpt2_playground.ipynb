{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import pytorch_lightning as pl\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM,PreTrainedTokenizerFast, GPT2Tokenizer\n",
    "from PIL import Image\n",
    "from tqdm import tqdm as tqdm\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from src.datasets.imageclef_dataset import ImageCLEF2021DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "augmentations = {\n",
    "    \n",
    "    'train': T.Compose([T.Resize((224,224)),\n",
    "                        T.ToTensor(),\n",
    "                        #T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "                        ]),\n",
    "    'val': T.Compose([T.Resize((224,224)),\n",
    "                        T.ToTensor(),\n",
    "                        T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))]),\n",
    "    'test': T.Compose([T.Resize((224,224)),\n",
    "                        T.ToTensor(),\n",
    "                        T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_params = {\n",
    "    \"root\": \"/Users/caghankoksal/Desktop/imageclef/\",\n",
    "    \"batch_size\": 32,\n",
    "    \"tokenizer\": \"gpt2\",\n",
    "    \"return_size\": False,\n",
    "    \"num_data_workers\": 0,\n",
    "    \"limit_num_samples\" : 128\n",
    "}\n",
    "imageclef_datamodule = ImageCLEF2021DataModule(**dataset_params,transforms=augmentations,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = imageclef_datamodule.train_dataloader()\n",
    "val_loader = imageclef_datamodule.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    cur_images = batch['image']\n",
    "    print(cur_images.shape )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [00:36<00:00,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: tensor([-0.9954, -0.8883, -0.6623])\n",
      "std:  tensor([1.1835, 1.2106, 1.2051])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# placeholders\n",
    "psum    = torch.tensor([0.0, 0.0, 0.0])\n",
    "psum_sq = torch.tensor([0.0, 0.0, 0.0])\n",
    "\n",
    "# loop through images\n",
    "for batch in tqdm(train_loader):\n",
    "    psum    += batch[\"image\"].sum(axis = [0, 2, 3])\n",
    "    psum_sq += (batch[\"image\"] ** 2).sum(axis = [0, 2, 3])\n",
    "\n",
    "###### FINAL CALCULATIONS\n",
    "\n",
    "# pixel count\n",
    "count = len(imageclef_datamodule.train_dataset) * 224 * 224\n",
    "\n",
    "# mean and std\n",
    "total_mean = psum / count\n",
    "total_var  = (psum_sq / count) - (total_mean ** 2)\n",
    "total_std  = torch.sqrt(total_var)\n",
    "\n",
    "# output\n",
    "print('mean: '  + str(total_mean))\n",
    "print('std:  '  + str(total_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"image\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.multimodal.flamingo_module import FlamingoModule\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE_OF_TOKENIZER = 50257 # mimic_datamodule.train_dataset.tokenizer.vocab_size\n",
    "LANGUAGE_MODEL = 'gpt2'\n",
    "NUM_TOKENS = VOCAB_SIZE_OF_TOKENIZER +3 if LANGUAGE_MODEL==\"gpt2\" else 31092\n",
    "FLAMINGO_EMBED_DIM = 768\n",
    "DEPTH = 12\n",
    "NUM_HEADS = 8\n",
    "ATT_HEAD_DIM = 64\n",
    "CROOS_ATT_EVERY=3\n",
    "MEDIA_TOKEN_ID = imageclef_datamodule.train_dataset.tokenizer.all_special_ids[imageclef_datamodule.train_dataset.tokenizer.all_special_tokens.index('<image>')]\n",
    "PERCEIVER_NUM_LATENTS = 64\n",
    "PERCEIVER_DEPTH = 2\n",
    "IMAGE_ENCODER = \"clip\"\n",
    "PRETRAINED_CLIP_PATH = '/Users/caghankoksal/Desktop/development/PubMedCLIP_ViT32.pth'\n",
    "PRETRAINED_GPT2_PATH = \"/Users/caghankoksal/Desktop/development/TransformerPlay/gpt2-pytorch_model.bin\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hyperparams = {\n",
    "    'pretrained_clip_path': PRETRAINED_CLIP_PATH,\n",
    "    'warmup_steps': 569,\n",
    "    'num_tokens': NUM_TOKENS,\n",
    "    'dim': FLAMINGO_EMBED_DIM,\n",
    "    'depth': DEPTH,\n",
    "    'num_heads': NUM_HEADS,\n",
    "    'dim_head': ATT_HEAD_DIM,\n",
    "    'cross_attn_every': CROOS_ATT_EVERY,\n",
    "    'media_token_id': MEDIA_TOKEN_ID,\n",
    "    'perceiver_num_latents': PERCEIVER_NUM_LATENTS,\n",
    "    'perceiver_depth': PERCEIVER_DEPTH,\n",
    "    'image_encoder': IMAGE_ENCODER,\n",
    "    'language_model': LANGUAGE_MODEL,\n",
    "    'pretrained_gpt2_path': PRETRAINED_GPT2_PATH,\n",
    "}\n",
    "\n",
    "for k,v in model_hyperparams.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FlamingoModule(**model_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "trainer = pl.Trainer(max_epochs=6,\n",
    "                     accelerator=\"cpu\", devices=1,\n",
    "                     callbacks=[lr_monitor],\n",
    "                     log_every_n_steps=1,\n",
    "                      )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = \"/Users/caghankoksal/Desktop/SS2022/lightning_logs/version_21/checkpoints/epoch=56-val_loss=0.39-other_metric=0.00.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(CHECKPOINT_PATH,map_location=torch.device('cpu'))[\"state_dict\"])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_img = imageclef_datamodule.val_dataset[3][\"image\"]\n",
    "val_qa_pair= imageclef_datamodule.val_dataset[3][\"qa_pair\"]\n",
    "val_qa_pair.split('answer')\n",
    "val_question =  imageclef_datamodule.val_dataset[3][\"question\"]\n",
    "val_question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_answer =  imageclef_datamodule.val_dataset[3][\"answer\"]\n",
    "val_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from transformers import GPT2Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn as nn\n",
    "import torch.nn.functional as F\n",
    "def generate(image,context, cur_model, ntok=20):\n",
    "    for _ in range(ntok):\n",
    "        out = cur_model({'image': image,'input_ids': context })\n",
    "        logits = out[:, -1, :]\n",
    "        indices_to_remove = logits < torch.topk(logits, 10)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = np.NINF\n",
    "        next_tok = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1).squeeze(1)\n",
    "        context = torch.cat([context, next_tok.unsqueeze(-1)], dim=-1)\n",
    "    return context\n",
    "\n",
    "\n",
    "tokenizer = imageclef_datamodule.train_dataset.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_params = {\n",
    "    \"root\": \"/Users/caghankoksal/Desktop/imageclef/\",\n",
    "    \"batch_size\": 1,\n",
    "    \"tokenizer\": \"gpt2\",\n",
    "    \"return_size\": False,\n",
    "    \"num_data_workers\": 0,\n",
    "    \"limit_num_samples\" : None\n",
    "}\n",
    "imageclef_datamodule = ImageCLEF2021DataModule(**dataset_params,transforms=augmentations,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = imageclef_datamodule.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader_iter = iter(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"question\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' answer:')]) \n",
    "out = generate(val_img.unsqueeze(0), context, model, ntok=20)\n",
    "print(\"Model out : \",tokenizer.decode(out[0]))\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' answer:')]) \n",
    "out = generate(val_img.unsqueeze(0), context, model, ntok=20)\n",
    "print(\"Model out : \",tokenizer.decode(out[0]))\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' answer:')]) \n",
    "out = generate(val_img.unsqueeze(0), context, model, ntok=20)\n",
    "print(\"Model out : \",tokenizer.decode(out[0]))\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' answer:')]) \n",
    "out = generate(val_img.unsqueeze(0), context, model, ntok=20)\n",
    "print(\"Model out : \",tokenizer.decode(out[0]))\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' answer:')]) \n",
    "out = generate(val_img.unsqueeze(0), context, model, ntok=20)\n",
    "print(\"Model out : \",tokenizer.decode(out[0]))\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' answer:')]) \n",
    "out = generate(val_img.unsqueeze(0), context, model, ntok=20)\n",
    "print(\"Model out : \",tokenizer.decode(out[0]))\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' answer:')]) \n",
    "out = generate(val_img.unsqueeze(0), context, model, ntok=20)\n",
    "print(\"Model out : \",tokenizer.decode(out[0]))\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' answer:')]) \n",
    "out = generate(val_img.unsqueeze(0), context, model, ntok=20)\n",
    "print(\"Model out : \",tokenizer.decode(out[0]))\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' answer:')]) \n",
    "out = generate(val_img.unsqueeze(0), context, model, ntok=20)\n",
    "print(\"Model out : \",tokenizer.decode(out[0]))\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' answer:')]) \n",
    "out = generate(val_img.unsqueeze(0), context, model, ntok=20)\n",
    "print(\"Model out : \",tokenizer.decode(out[0]))\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' answer:')]) \n",
    "out = generate(val_img.unsqueeze(0), context, model, ntok=20)\n",
    "print(\"Model out : \",tokenizer.decode(out[0]))\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' answer:')]) \n",
    "out = generate(val_img.unsqueeze(0), context, model, ntok=20)\n",
    "print(\"Model out : \",tokenizer.decode(out[0]))\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('torch-nightly')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "892a7f8aeabe86b99d45932805d162784b758c544538f3ce4737e4a115db3cfd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
