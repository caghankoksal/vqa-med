{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os\n",
    "sys.path.append('..')\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/home/koksal/.conda/envs/mlmi/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "from src.models.multimodal.flamingo_module import FlamingoModule\n",
    "from src.datasets.imageclef_dataset import ImageCLEF2021DataModule\n",
    "from src.utils.utils import load_flamingo_weights, print_hyperparams\n",
    "\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "import torchvision.transforms as T\n",
    "from pytorch_lightning.loggers import CometLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len training dataset :  4500 Batch Size :  1 NUM_EPOCHS :  120\n",
      "Total training steps :  540000\n"
     ]
    }
   ],
   "source": [
    "augmentations = {\n",
    "        \n",
    "        'train': T.Compose([T.Resize((224,224)),\n",
    "                            T.ToTensor(),\n",
    "                            T.RandomHorizontalFlip(p=0.5),\n",
    "                            T.RandomRotation(degrees=10),\n",
    "                            T.Normalize(mean=(0.2570, 0.2570, 0.2570), std=(0.2710, 0.2710, 0.2710))\n",
    "                            ]),\n",
    "        'val': T.Compose([T.Resize((224,224)),\n",
    "                            T.ToTensor(),\n",
    "                            T.RandomHorizontalFlip(p=0.5),\n",
    "                            T.RandomRotation(degrees=10),\n",
    "                            T.Normalize(mean=(0.2570, 0.2570, 0.2570), std=(0.2710, 0.2710, 0.2710))\n",
    "                            ]),\n",
    "        'test': T.Compose([T.Resize((224,224)),\n",
    "                            T.ToTensor(),\n",
    "                            T.RandomHorizontalFlip(p=0.5),\n",
    "                            T.RandomRotation(degrees=10),   \n",
    "                            T.Normalize(mean=(0.2570, 0.2570, 0.2570), std=(0.2710, 0.2710, 0.2710))\n",
    "                            ])\n",
    "    }\n",
    "\n",
    "    \n",
    "# Hyperparameters\n",
    "NUM_DATA_WORKERS  = 2\n",
    "ONLY_IMAGES = False\n",
    "BATCH_SIZE = 1\n",
    "NUM_EPOCHS = 120\n",
    "LIMIT_NUM_SAMPLES = None\n",
    "DATASET = \"IMAGECLEF\"\n",
    "LOAD_TRAINED_IMAGECLEF = True\n",
    "\n",
    "\n",
    "if os.getcwd().startswith('/u/home/koksal'):\n",
    "    ACCELERATOR = \"gpu\"\n",
    "    DEVICES = [6,7]\n",
    "    PRETRAINED_CLIP_PATH = None#'/home/mlmi-matthias/Caghan/pretrained_models/PubMedCLIP_ViT32.pth'\n",
    "    PRETRAINED_GPT2_PATH = \"/u/home/koksal/mlmi-vqa/models/gpt2-pytorch_model.bin\"\n",
    "    MIMIC_CXR_DCM_PATH = '/home/mlmi-matthias/physionet.org/files/mimic-cxr/2.0.0/files/'\n",
    "    MIMIC_CXR_JPG_PATH = \"/home/mlmi-matthias/physionet.org/files/mimic-cxr-jpg/2.0.0/files/\"\n",
    "    SPLIT_PATH = '/home/mlmi-matthias/Caghan/mlmi-vqa/data/external/'\n",
    "    IMAGECLEF_PATH ='/u/home/koksal/mlmi-vqa/data/external/imageclef/'\n",
    "    #CHECKPOINT_PATH = \"/home/mlmi-matthias/Caghan/mlmi-vqa/notebooks/lightning_logs/version_20/checkpoints/epoch=114-val_loss=0.84-other_metric=0.00.ckpt\"\n",
    "    #Â Latest ROCO Training \n",
    "    CHECKPOINT_PATH =\"/home/mlmi-matthias/Caghan/mlmi-vqa/notebooks/lightning_logs/version_77/checkpoints/epoch=61-val_loss_generation_epoch=1.80.ckpt\"\n",
    "    ANSWERS_LIST_PATH = '/u/home/koksal/mlmi-vqa//data/external/answer_list_imageclef.txt'\n",
    "    IMAGECLEF_CHECKPOINT_PATH = \"/home/mlmi-matthias/Caghan/mlmi-vqa/notebooks/lightning_logs/version_101/checkpoints/last.ckpt\"\n",
    "\n",
    "\n",
    "elif os.getcwd().startswith('/Users/caghankoksal'):\n",
    "    PRETRAINED_CLIP_PATH = '/Users/caghankoksal/Desktop/development/PubMedCLIP_ViT32.pth'\n",
    "    PRETRAINED_GPT2_PATH = \"/Users/caghankoksal/Desktop/development/TransformerPlay/gpt2-pytorch_model.bin\"\n",
    "    ACCELERATOR = \"cpu\"\n",
    "    DEVICES = 1\n",
    "    MIMIC_CXR_DCM_PATH = '/Users/caghankoksal/Desktop/development/Flamingo-playground/physionet.org/files/mimic-cxr/2.0.0/files/'\n",
    "    MIMIC_CXR_JPG_PATH = '/Users/caghankoksal/Desktop/development/physionet.org/files/mimic-cxr-jpg/2.0.0/files/'\n",
    "    SPLIT_PATH = '/Users/caghankoksal/Desktop/SS2022/mlmi-vqa/data/external/'\n",
    "    IMAGECLEF_PATH = \"/Users/caghankoksal/Desktop/imageclef/\"\n",
    "    CHECKPOINT_PATH = \"/Users/caghankoksal/Desktop/SS2022/lightning_logs/version_77/checkpoints/epoch=66-val_loss_generation_epoch=1.80.ckpt\"\n",
    "    ANSWERS_LIST_PATH = '/Users/caghankoksal/Desktop/SS2022/mlmi-vqa/data/external/answer_list_imageclef.txt'\n",
    "    IMAGECLEF_CHECKPOINT_PATH = \"/Users/caghankoksal/Desktop/SS2022/lightning_logs/version_102/checkpoints/last.ckpt\"\n",
    "\n",
    "\n",
    "IMAGE_TYPE = \"jpg\"\n",
    "TOKENIZER  = \"gpt2\"\n",
    "PREPROCESSED = True\n",
    "RETURN_IDX_EOC = True\n",
    "\n",
    "dataset_hyperparameters = {\n",
    "    \"root\": IMAGECLEF_PATH,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"tokenizer\": TOKENIZER,\n",
    "    \"num_data_workers\": NUM_DATA_WORKERS,\n",
    "    \"return_size\": False,\n",
    "    \"answers_list_path\": ANSWERS_LIST_PATH,\n",
    "    \"return_idx_answer_eoc\": RETURN_IDX_EOC,\n",
    "    \"transforms\": augmentations,\n",
    "    \"limit_num_samples\": LIMIT_NUM_SAMPLES,\n",
    "}\n",
    "\n",
    "\n",
    "datamodule = ImageCLEF2021DataModule(**dataset_hyperparameters)\n",
    "\n",
    "\n",
    "train_loader = datamodule.train_dataloader()\n",
    "val_loader = datamodule.val_dataloader()\n",
    "\n",
    "print(\"Len training dataset : \", len(datamodule.train_dataset),\n",
    "    \"Batch Size : \", BATCH_SIZE, \"NUM_EPOCHS : \",NUM_EPOCHS )\n",
    "print(\"Total training steps : \", len(datamodule.train_dataset)//BATCH_SIZE*NUM_EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    cur_images = batch['image']\n",
    "    print(cur_images.shape )\n",
    "    cur_batch = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|endoftext|> <image> question: is this a normal gastrointestinal image? <EOQ> answer: yes <EOC>']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_batch['qa_pair']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_batch[\"index_answer\"]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['image', 'question', 'answer', 'label', 'index_answer', 'index_eoc', 'index_eoq', 'input_ids', 'token_type_ids', 'targets', 'ID', 'qa_pair'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_batch[\"input_ids\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import pytorch_lightning as pl\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM,PreTrainedTokenizerFast, GPT2Tokenizer\n",
    "from PIL import Image\n",
    "from tqdm import tqdm as tqdm\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from src.datasets.imageclef_dataset import ImageCLEF2021DataModule\n",
    "from src.models.multimodal.flamingo_module import FlamingoModule\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained_clip_path None\n",
      "warmup_steps 30\n",
      "num_tokens 50261\n",
      "dim 768\n",
      "depth 12\n",
      "num_heads 8\n",
      "dim_head 64\n",
      "cross_attn_every 3\n",
      "media_token_id 3\n",
      "perceiver_num_latents 64\n",
      "perceiver_depth 2\n",
      "image_encoder clip\n",
      "language_model gpt2\n",
      "pretrained_gpt2_path /u/home/koksal/mlmi-vqa/models/gpt2-pytorch_model.bin\n",
      "classification_mode True\n",
      "classification_num_classes 332\n",
      "flamingo_mode True\n",
      "label_smoothing 0.5\n",
      "token_label_smoothing 0.0\n",
      "learning_rate 0.0001\n",
      "use_image_embeddings True\n",
      "train_embedding_layer True\n",
      "classifier_dropout 0.5\n",
      "Vit is started from scratch\n",
      "Vit is initialized\n",
      "Flamingo is being initialized with  gpt2  as language model\n",
      "GPT 2 Weights are loading...\n",
      "Loaded GPT2 weights and Embeddings num_weights loaded :  156\n",
      "Pretrained Flamingo Model is loaded from checkpoint :  /home/mlmi-matthias/Caghan/mlmi-vqa/notebooks/lightning_logs/version_77/checkpoints/epoch=61-val_loss_generation_epoch=1.80.ckpt\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/mlmi-matthias/Caghan/mlmi-vqa/notebooks/lightning_logs/version_77/checkpoints/epoch=61-val_loss_generation_epoch=1.80.ckpt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/u/home/koksal/mlmi-vqa/notebooks/imageclef_flamingo_clip_gpt2_playground.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 60>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B131.159.110.3/u/home/koksal/mlmi-vqa/notebooks/imageclef_flamingo_clip_gpt2_playground.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m     model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(CHECKPOINT_PATH)[\u001b[39m\"\u001b[39m\u001b[39mstate_dict\u001b[39m\u001b[39m\"\u001b[39m],strict\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B131.159.110.3/u/home/koksal/mlmi-vqa/notebooks/imageclef_flamingo_clip_gpt2_playground.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B131.159.110.3/u/home/koksal/mlmi-vqa/notebooks/imageclef_flamingo_clip_gpt2_playground.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=64'>65</a>\u001b[0m     model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(CHECKPOINT_PATH,map_location\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mdevice(\u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m))[\u001b[39m\"\u001b[39m\u001b[39mstate_dict\u001b[39m\u001b[39m\"\u001b[39m],strict\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/mlmi/lib/python3.9/site-packages/torch/serialization.py:699\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    697\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 699\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    700\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    701\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    702\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/.conda/envs/mlmi/lib/python3.9/site-packages/torch/serialization.py:231\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    230\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 231\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    232\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/.conda/envs/mlmi/lib/python3.9/site-packages/torch/serialization.py:212\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 212\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/mlmi-matthias/Caghan/mlmi-vqa/notebooks/lightning_logs/version_77/checkpoints/epoch=61-val_loss_generation_epoch=1.80.ckpt'"
     ]
    }
   ],
   "source": [
    "#Â MODEL HPRAMS\n",
    "VOCAB_SIZE_OF_TOKENIZER = 50257 #Â mimic_datamodule.train_dataset.tokenizer.vocab_size\n",
    "LANGUAGE_MODEL = 'gpt2'\n",
    "NUM_TOKENS = VOCAB_SIZE_OF_TOKENIZER +4 if LANGUAGE_MODEL==\"gpt2\" else 31092\n",
    "FLAMINGO_EMBED_DIM = 768\n",
    "DEPTH = 12\n",
    "NUM_HEADS = 8\n",
    "ATT_HEAD_DIM = 64\n",
    "CROOS_ATT_EVERY=3\n",
    "#MEDIA_TOKEN_ID = datamodule.train_dataset.tokenizer.\\\n",
    "#    all_special_ids[datamodule.train_dataset.tokenizer.all_special_tokens.index('<image>')]\n",
    "MEDIA_TOKEN_ID=3\n",
    "PERCEIVER_NUM_LATENTS = 64\n",
    "PERCEIVER_DEPTH = 2\n",
    "IMAGE_ENCODER = \"clip\"\n",
    "CLASSIFICATION_MODE = True\n",
    "NUM_CLASSES = 332\n",
    "FLAMINGO_MODE = True\n",
    "LABEL_SMOOTHING = 0.5\n",
    "#Â Label smoothing for classification task\n",
    "TOKEN_LABEL_SMOOTHING = 0.0\n",
    "GRADIENT_CLIP_VAL = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "USE_IMAGE_EMBEDDINGS = True\n",
    "TRAIN_EMBEDDING_LAYER = True\n",
    "CLASSIFIER_DROPOUT = 0.5\n",
    "\n",
    "\n",
    "hyperparams = {\n",
    "    'pretrained_clip_path': None,#PRETRAINED_CLIP_PATH,\n",
    "    'warmup_steps': 30,\n",
    "    'num_tokens': NUM_TOKENS,\n",
    "    'dim': FLAMINGO_EMBED_DIM,\n",
    "    'depth': DEPTH,\n",
    "    'num_heads': NUM_HEADS,\n",
    "    'dim_head': ATT_HEAD_DIM,\n",
    "    'cross_attn_every': CROOS_ATT_EVERY,\n",
    "    'media_token_id': MEDIA_TOKEN_ID,\n",
    "    'perceiver_num_latents': PERCEIVER_NUM_LATENTS,\n",
    "    'perceiver_depth': PERCEIVER_DEPTH,\n",
    "    'image_encoder': IMAGE_ENCODER,\n",
    "    'language_model': LANGUAGE_MODEL,\n",
    "    'pretrained_gpt2_path': PRETRAINED_GPT2_PATH,\n",
    "    'classification_mode': CLASSIFICATION_MODE,\n",
    "    'classification_num_classes': NUM_CLASSES,  # 332 if DATASET==\"IMAGECLEF\"\n",
    "    'flamingo_mode': FLAMINGO_MODE,\n",
    "    \"label_smoothing\": LABEL_SMOOTHING,\n",
    "    \"token_label_smoothing\": TOKEN_LABEL_SMOOTHING,\n",
    "    \"learning_rate\":LEARNING_RATE,\n",
    "    \"use_image_embeddings\": USE_IMAGE_EMBEDDINGS,\n",
    "    \"train_embedding_layer\": TRAIN_EMBEDDING_LAYER,\n",
    "    \"classifier_dropout\": CLASSIFIER_DROPOUT\n",
    "    }\n",
    "\n",
    "print_hyperparams(hyperparams)\n",
    "\n",
    "model = FlamingoModule(**hyperparams)\n",
    "START_FROM_CHECKPOINT = True\n",
    "\n",
    "if START_FROM_CHECKPOINT:\n",
    "    print(\"Pretrained Flamingo Model is loaded from checkpoint : \",CHECKPOINT_PATH)\n",
    "    if os.getcwd().startswith('/home/mlmi-matthias'):\n",
    "        model.load_state_dict(torch.load(CHECKPOINT_PATH)[\"state_dict\"],strict=False)\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(CHECKPOINT_PATH,map_location=torch.device('cpu'))[\"state_dict\"],strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams['language_model'] ='bert'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vit is started from scratch\n",
      "Vit is initialized\n",
      "Flamingo is being initialized with  bert  as language model\n"
     ]
    }
   ],
   "source": [
    "model = FlamingoModule(**hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FlamingoModel(\n",
       "  (token_emb): Embedding(50261, 768)\n",
       "  (wpe): Embedding(256, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (img_encoder): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (perceiver_resampler): PerceiverResampler(\n",
       "    (layers): ModuleList(\n",
       "      (0): ModuleList(\n",
       "        (0): PerceiverAttention(\n",
       "          (norm_media): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm_latents): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (to_q): Linear(in_features=768, out_features=512, bias=False)\n",
       "          (to_kv): Linear(in_features=768, out_features=1024, bias=False)\n",
       "          (to_out): Linear(in_features=512, out_features=768, bias=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (2): GELU()\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1): ModuleList(\n",
       "        (0): PerceiverAttention(\n",
       "          (norm_media): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm_latents): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (to_q): Linear(in_features=768, out_features=512, bias=False)\n",
       "          (to_kv): Linear(in_features=768, out_features=1024, bias=False)\n",
       "          (to_out): Linear(in_features=512, out_features=768, bias=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (2): GELU()\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (img_encoder_outdim_layer): Linear(in_features=512, out_features=768, bias=True)\n",
       "  (layers): ModuleList(\n",
       "    (0): ModuleList(\n",
       "      (0): Residual(\n",
       "        (fn): TransformerBlockBERT(\n",
       "          (attention): MultiHeadedSelfAttention(\n",
       "            (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (feedforward): PositionWiseFeedForward(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop1): Dropout(p=0.3, inplace=False)\n",
       "          (drop2): Dropout(p=0.3, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): GatedCrossAttentionBlock(\n",
       "        (attn): MaskedCrossAttention(\n",
       "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (to_q): Linear(in_features=768, out_features=512, bias=False)\n",
       "          (to_kv): Linear(in_features=768, out_features=1024, bias=False)\n",
       "          (to_out): Linear(in_features=512, out_features=768, bias=False)\n",
       "        )\n",
       "        (ff): Sequential(\n",
       "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (2): GELU()\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): ModuleList(\n",
       "      (0): Residual(\n",
       "        (fn): TransformerBlockBERT(\n",
       "          (attention): MultiHeadedSelfAttention(\n",
       "            (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (feedforward): PositionWiseFeedForward(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop1): Dropout(p=0.3, inplace=False)\n",
       "          (drop2): Dropout(p=0.3, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): None\n",
       "    )\n",
       "    (2): ModuleList(\n",
       "      (0): Residual(\n",
       "        (fn): TransformerBlockBERT(\n",
       "          (attention): MultiHeadedSelfAttention(\n",
       "            (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (feedforward): PositionWiseFeedForward(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop1): Dropout(p=0.3, inplace=False)\n",
       "          (drop2): Dropout(p=0.3, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): None\n",
       "    )\n",
       "    (3): ModuleList(\n",
       "      (0): Residual(\n",
       "        (fn): TransformerBlockBERT(\n",
       "          (attention): MultiHeadedSelfAttention(\n",
       "            (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (feedforward): PositionWiseFeedForward(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop1): Dropout(p=0.3, inplace=False)\n",
       "          (drop2): Dropout(p=0.3, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): GatedCrossAttentionBlock(\n",
       "        (attn): MaskedCrossAttention(\n",
       "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (to_q): Linear(in_features=768, out_features=512, bias=False)\n",
       "          (to_kv): Linear(in_features=768, out_features=1024, bias=False)\n",
       "          (to_out): Linear(in_features=512, out_features=768, bias=False)\n",
       "        )\n",
       "        (ff): Sequential(\n",
       "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (2): GELU()\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): ModuleList(\n",
       "      (0): Residual(\n",
       "        (fn): TransformerBlockBERT(\n",
       "          (attention): MultiHeadedSelfAttention(\n",
       "            (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (feedforward): PositionWiseFeedForward(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop1): Dropout(p=0.3, inplace=False)\n",
       "          (drop2): Dropout(p=0.3, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): None\n",
       "    )\n",
       "    (5): ModuleList(\n",
       "      (0): Residual(\n",
       "        (fn): TransformerBlockBERT(\n",
       "          (attention): MultiHeadedSelfAttention(\n",
       "            (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (feedforward): PositionWiseFeedForward(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop1): Dropout(p=0.3, inplace=False)\n",
       "          (drop2): Dropout(p=0.3, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): None\n",
       "    )\n",
       "    (6): ModuleList(\n",
       "      (0): Residual(\n",
       "        (fn): TransformerBlockBERT(\n",
       "          (attention): MultiHeadedSelfAttention(\n",
       "            (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (feedforward): PositionWiseFeedForward(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop1): Dropout(p=0.3, inplace=False)\n",
       "          (drop2): Dropout(p=0.3, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): GatedCrossAttentionBlock(\n",
       "        (attn): MaskedCrossAttention(\n",
       "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (to_q): Linear(in_features=768, out_features=512, bias=False)\n",
       "          (to_kv): Linear(in_features=768, out_features=1024, bias=False)\n",
       "          (to_out): Linear(in_features=512, out_features=768, bias=False)\n",
       "        )\n",
       "        (ff): Sequential(\n",
       "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (2): GELU()\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): ModuleList(\n",
       "      (0): Residual(\n",
       "        (fn): TransformerBlockBERT(\n",
       "          (attention): MultiHeadedSelfAttention(\n",
       "            (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (feedforward): PositionWiseFeedForward(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop1): Dropout(p=0.3, inplace=False)\n",
       "          (drop2): Dropout(p=0.3, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): None\n",
       "    )\n",
       "    (8): ModuleList(\n",
       "      (0): Residual(\n",
       "        (fn): TransformerBlockBERT(\n",
       "          (attention): MultiHeadedSelfAttention(\n",
       "            (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (feedforward): PositionWiseFeedForward(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop1): Dropout(p=0.3, inplace=False)\n",
       "          (drop2): Dropout(p=0.3, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): None\n",
       "    )\n",
       "    (9): ModuleList(\n",
       "      (0): Residual(\n",
       "        (fn): TransformerBlockBERT(\n",
       "          (attention): MultiHeadedSelfAttention(\n",
       "            (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (feedforward): PositionWiseFeedForward(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop1): Dropout(p=0.3, inplace=False)\n",
       "          (drop2): Dropout(p=0.3, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): GatedCrossAttentionBlock(\n",
       "        (attn): MaskedCrossAttention(\n",
       "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (to_q): Linear(in_features=768, out_features=512, bias=False)\n",
       "          (to_kv): Linear(in_features=768, out_features=1024, bias=False)\n",
       "          (to_out): Linear(in_features=512, out_features=768, bias=False)\n",
       "        )\n",
       "        (ff): Sequential(\n",
       "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (2): GELU()\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): ModuleList(\n",
       "      (0): Residual(\n",
       "        (fn): TransformerBlockBERT(\n",
       "          (attention): MultiHeadedSelfAttention(\n",
       "            (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (feedforward): PositionWiseFeedForward(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop1): Dropout(p=0.3, inplace=False)\n",
       "          (drop2): Dropout(p=0.3, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): None\n",
       "    )\n",
       "    (11): ModuleList(\n",
       "      (0): Residual(\n",
       "        (fn): TransformerBlockBERT(\n",
       "          (attention): MultiHeadedSelfAttention(\n",
       "            (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (feedforward): PositionWiseFeedForward(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop1): Dropout(p=0.3, inplace=False)\n",
       "          (drop2): Dropout(p=0.3, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): None\n",
       "    )\n",
       "  )\n",
       "  (to_logits): Sequential(\n",
       "    (0): LayerNorm()\n",
       "    (1): Linear(in_features=768, out_features=50261, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.flamingo_palm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "out,_ = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 50261])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/home/mlmi-matthias/Caghan/mlmi-vqa/notebooks/lightning_logs/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls \"/home/mlmi-matthias/Caghan/mlmi-vqa/notebooks/lightning_logs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': tensor([[[[-0.9483, -0.9483, -0.9483,  ..., -0.9483, -0.9483, -0.9483],\n",
       "           [-0.9483, -0.9483, -0.9483,  ..., -0.9483, -0.9483, -0.9483],\n",
       "           [-0.9483, -0.9483, -0.9483,  ..., -0.9483, -0.9483, -0.9483],\n",
       "           ...,\n",
       "           [-0.9483, -0.9483, -0.9483,  ..., -0.9483, -0.9483, -0.9483],\n",
       "           [-0.9483, -0.9483, -0.9483,  ..., -0.9483, -0.9483, -0.9483],\n",
       "           [-0.9483, -0.9483, -0.9483,  ..., -0.9483, -0.9483, -0.9483]],\n",
       " \n",
       "          [[-0.9483, -0.9483, -0.9483,  ..., -0.9483, -0.9483, -0.9483],\n",
       "           [-0.9483, -0.9483, -0.9483,  ..., -0.9483, -0.9483, -0.9483],\n",
       "           [-0.9483, -0.9483, -0.9483,  ..., -0.9483, -0.9483, -0.9483],\n",
       "           ...,\n",
       "           [-0.9483, -0.9483, -0.9483,  ..., -0.9483, -0.9483, -0.9483],\n",
       "           [-0.9483, -0.9483, -0.9483,  ..., -0.9483, -0.9483, -0.9483],\n",
       "           [-0.9483, -0.9483, -0.9483,  ..., -0.9483, -0.9483, -0.9483]],\n",
       " \n",
       "          [[-0.9483, -0.9483, -0.9483,  ..., -0.9483, -0.9483, -0.9483],\n",
       "           [-0.9483, -0.9483, -0.9483,  ..., -0.9483, -0.9483, -0.9483],\n",
       "           [-0.9483, -0.9483, -0.9483,  ..., -0.9483, -0.9483, -0.9483],\n",
       "           ...,\n",
       "           [-0.9483, -0.9483, -0.9483,  ..., -0.9483, -0.9483, -0.9483],\n",
       "           [-0.9483, -0.9483, -0.9483,  ..., -0.9483, -0.9483, -0.9483],\n",
       "           [-0.9483, -0.9483, -0.9483,  ..., -0.9483, -0.9483, -0.9483]]]]),\n",
       " 'question': ['is this a normal gastrointestinal image?'],\n",
       " 'answer': ['yes'],\n",
       " 'label': tensor([155]),\n",
       " 'index_answer': tensor([14]),\n",
       " 'index_eoc': tensor([18]),\n",
       " 'index_eoq': tensor([13]),\n",
       " 'input_ids': tensor([[[50256,   220, 50258,  1808,    25,   318,   428,   257,  3487, 40887,\n",
       "            2939,    30,   220, 50260,  3280,    25,  3763,   220, 50259, 50257,\n",
       "           50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "           50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "           50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "           50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "           50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "           50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "           50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "           50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "           50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "           50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "           50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257]]]),\n",
       " 'token_type_ids': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]),\n",
       " 'targets': tensor([[[  220, 50258,  1808,    25,   318,   428,   257,  3487, 40887,  2939,\n",
       "              30,   220, 50260,  3280,    25,  3763,   220, 50259, 50257, 50257,\n",
       "           50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "           50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "           50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "           50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "           50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "           50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "           50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "           50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "           50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "           50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "           50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257]]]),\n",
       " 'ID': ['synpic25788'],\n",
       " 'qa_pair': ['<|endoftext|> <image> question: is this a normal gastrointestinal image? <EOQ> answer: yes <EOC>']}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#IMAGECLEF_CHECKPOINT_PATH ='/home/mlmi-matthias/Caghan/mlmi-vqa/notebooks/lightning_logs/version_95/checkpoints/\\\n",
    "#epoch=94-val_acc_epoch=0.41-val_total_loss_epoch=4.28-val_loss_generation_epoch=0.22-val_classification_loss_epoch=4.06.ckpt'\n",
    "#IMAGECLEF_CHECKPOINT_PATH = \"/home/mlmi-matthias/Caghan/mlmi-vqa/notebooks/lightning_logs/version_99/checkpoints/epoch=82-val_acc_epoch=0.39-val_total_loss_epoch=5.28-val_loss_generation_epoch=0.23-val_classification_loss_epoch=5.05.ckpt\"\n",
    "IMAGECLEF_CHECKPOINT_PATH = \"/home/mlmi-matthias/Caghan/mlmi-vqa/notebooks/lightning_logs/version_108/checkpoints/epoch=22-val_loss_generation_epoch=0.21.ckpt\"\n",
    "IMAGECLEF_CHECKPOINT_PATH = \"/Users/caghankoksal/Desktop/SS2022/lightning_logs/version_108/checkpoints/last.ckpt\"\n",
    "IMAGECLEF_CHECKPOINT_PATH = \"/Users/caghankoksal/Desktop/SS2022/lightning_logs/version_110/checkpoints/last.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_TRAINED_IMAGECLEF:\n",
    "    print(\"Pretrained Flamingo Model is loaded from checkpoint : \",IMAGECLEF_CHECKPOINT_PATH)\n",
    "    if os.getcwd().startswith('/home/mlmi-matthias'):\n",
    "        model.load_state_dict(torch.load(IMAGECLEF_CHECKPOINT_PATH)[\"state_dict\"],strict=False)\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(IMAGECLEF_CHECKPOINT_PATH,map_location=torch.device('cpu'))[\"state_dict\"],strict=False)\n",
    "        print(\"Checkpoint Weights are loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.load(IMAGECLEF_CHECKPOINT_PATH,map_location=torch.device('cpu'))[\"hyper_parameters\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val_img = imageclef_datamodule.val_dataset[3][\"image\"]\n",
    "val_qa_pair= imageclef_datamodule.val_dataset[3][\"qa_pair\"]\n",
    "val_qa_pair.split('answer')\n",
    "val_question =  imageclef_datamodule.val_dataset[3][\"question\"]\n",
    "val_question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from transformers import GPT2Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn as nn\n",
    "import torch.nn.functional as F\n",
    "def generate(image, context, cur_model, batch, ntok=20):\n",
    "    for _ in range(ntok):\n",
    "        out,_ = cur_model({'image': image,'input_ids': context, \"index_eoq\": batch[\"index_eoq\"],\n",
    "        \"targets\": batch[\"targets\"],\"label\": batch[\"label\"]})\n",
    "        logits = out[:, -1, :]\n",
    "        indices_to_remove = logits < torch.topk(logits, 10)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = np.NINF\n",
    "        #next_tok1 = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1).squeeze(1)\n",
    "        #print(next_tok1.shape)\n",
    "        softmax_out = F.softmax(logits, dim=-1)\n",
    "        #print(softmax_out.shape)\n",
    "        next_tok = torch.argmax(softmax_out,dim=-1,keepdim=False)\n",
    "        #print(next_tok.shape)\n",
    "        context = torch.cat([context, next_tok.unsqueeze(-1)], dim=-1)\n",
    "    return context\n",
    "\n",
    "\n",
    "tokenizer = datamodule.train_dataset.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = datamodule.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader_iter = iter(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch = next(val_loader_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"question\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.ToPILImage()(batch[\"image\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' <EOQ>'+ ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, batch, ntok=20)\n",
    "print(\"Question : \", batch[\"question\"][0])\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' <EOQ>'+ ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, batch, ntok=20)\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' <EOQ>'+ ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, batch, ntok=20)\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.ToPILImage()(batch[\"image\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context   = torch.tensor([tokenizer.encode(\n",
    "    \"<|endoftext|> <image> question: There is a pheochromocytoma in this image. Why there is a pheochromocytoma \"+\\\n",
    "            ' <EOQ>'+ ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, batch, ntok=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' <EOQ>'+ ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, batch, ntok=20)\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' <EOQ>'+ ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, batch, ntok=20)\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' <EOQ>'+ ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, batch, ntok=20)\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' <EOQ>'+ ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, batch, ntok=20)\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' <EOQ>'+ ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, batch, ntok=20)\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' <EOQ>'+ ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, batch, ntok=20)\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' <EOQ>'+ ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, batch, ntok=20)\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' <EOQ>'+ ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, batch, ntok=20)\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy_classification_task(dataloader, model):\n",
    "    # Calculate accuracy as a classification task\n",
    "    true_count = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        flamingo_logits, classification_logits = model(batch)\n",
    "        true_label = batch[\"label\"]\n",
    "        pred = torch.argmax(classification_logits,dim=1).item() \n",
    "        #print(\"Prediction is : \",pred)\n",
    "\n",
    "        if pred == true_label:\n",
    "            #print(\"true prediction\")\n",
    "            true_count +=1\n",
    "    print(\"Accuracy of the model on the classification task is \", true_count/len(dataloader) )\n",
    "    return true_count/len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_accuracy_classification_task(val_dataloader,model=model)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "val_dataloader = datamodule.val_dataloader()\n",
    "tokenizer = datamodule.train_dataset.tokenizer\n",
    "true_predictions = []\n",
    "false_predictions = []\n",
    "for batch in tqdm(val_dataloader):\n",
    "    #batch = next(val_loader_iter)\n",
    "    context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' <EOQ>'+ ' answer:')]) \n",
    "    out = generate(batch[\"image\"], context, model, batch, ntok=20)\n",
    "    models_answer = tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0].rstrip().strip()\n",
    "    correct_answer = batch[\"answer\"][0].rstrip().strip()\n",
    "    if models_answer == correct_answer:\n",
    "        correct += 1\n",
    "        true_predictions.append((models_answer, correct_answer))\n",
    "    else:\n",
    "        false_predictions.append((models_answer, correct_answer))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct/len(val_dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRUE_PREDS_FILE_NAME = 'correct_predictions_FT_ROCO_FlamingoModeON_ckpt_110.xlsx'\n",
    "FALSE_PREDS_FILE_NAME = \"false_predictions_FT_ROCO_FlamingoModeON_ckpt_110.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "correct_preds_pred = pd.DataFrame(true_predictions, columns=[\"models_answer\",\"correct_answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_preds_pred.to_excel(TRUE_PREDS_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_preds_pred = pd.DataFrame(false_predictions, columns=[\"models_answer\",\"correct_answer\"])\n",
    "false_preds_pred.to_excel(FALSE_PREDS_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_list = pd.concat([correct_preds_pred, false_preds_pred], axis=0).reset_index().drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import numpy as np\n",
    "def calculate_bleu_score():\n",
    "  bleu_per_answer = np.asarray( [ sentence_bleu(target.split(), pred.split(), weights = [1])  for pred, target in zip(concat_list[\"models_answer\"],concat_list[\"correct_answer\"])  ])\n",
    "  return np.mean(bleu_per_answer)\n",
    "\n",
    "cur_bleu = 0\n",
    "for pred, target in zip(concat_list[\"models_answer\"],concat_list[\"correct_answer\"]):\n",
    "    try:\n",
    "        cur_bleu += sentence_bleu( [target.split()], pred.split(), weights = [1])\n",
    "    except:\n",
    "        print(pred, target)\n",
    "\n",
    "cur_bleu/len(concat_list[\"models_answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â BEAM SEARCH DECODER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.utils import beam_decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_outs = beam_decode(cur_batch, model, tokenizer, top_k=2, beam_width=2, max_len=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_outs[0][-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in decoded_outs[0][0]:\n",
    "    #print(each)\n",
    "    print(tokenizer.decode(each[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_params = {\n",
    "    \"root\": \"/Users/caghankoksal/Desktop/imageclef/\",\n",
    "    \"batch_size\": 1,\n",
    "    \"tokenizer\": \"gpt2\",\n",
    "    \"return_size\": False,\n",
    "    \"num_data_workers\": 0,\n",
    "    \"limit_num_samples\" : None\n",
    "}\n",
    "imageclef_datamodule = ImageCLEF2021DataModule(**dataset_params,transforms=augmentations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE_OF_TOKENIZER = 50257 #Â mimic_datamodule.train_dataset.tokenizer.vocab_size\n",
    "LANGUAGE_MODEL = 'gpt2'\n",
    "NUM_TOKENS = VOCAB_SIZE_OF_TOKENIZER +3 if LANGUAGE_MODEL==\"gpt2\" else 31092\n",
    "FLAMINGO_EMBED_DIM = 768\n",
    "DEPTH = 12\n",
    "NUM_HEADS = 8\n",
    "ATT_HEAD_DIM = 64\n",
    "CROOS_ATT_EVERY=3\n",
    "MEDIA_TOKEN_ID = imageclef_datamodule.train_dataset.tokenizer.all_special_ids[imageclef_datamodule.train_dataset.tokenizer.all_special_tokens.index('<image>')]\n",
    "PERCEIVER_NUM_LATENTS = 64\n",
    "PERCEIVER_DEPTH = 2\n",
    "IMAGE_ENCODER = \"clip\"\n",
    "PRETRAINED_CLIP_PATH = '/Users/caghankoksal/Desktop/development/PubMedCLIP_ViT32.pth'\n",
    "PRETRAINED_GPT2_PATH = \"/Users/caghankoksal/Desktop/development/TransformerPlay/gpt2-pytorch_model.bin\"\n",
    "\n",
    "model_hyperparams = {\n",
    "    'pretrained_clip_path': PRETRAINED_CLIP_PATH,\n",
    "    'warmup_steps': 569,\n",
    "    'num_tokens': NUM_TOKENS,\n",
    "    'dim': FLAMINGO_EMBED_DIM,\n",
    "    'depth': DEPTH,\n",
    "    'num_heads': NUM_HEADS,\n",
    "    'dim_head': ATT_HEAD_DIM,\n",
    "    'cross_attn_every': CROOS_ATT_EVERY,\n",
    "    'media_token_id': MEDIA_TOKEN_ID,\n",
    "    'perceiver_num_latents': PERCEIVER_NUM_LATENTS,\n",
    "    'perceiver_depth': PERCEIVER_DEPTH,\n",
    "    'image_encoder': IMAGE_ENCODER,\n",
    "    'language_model': LANGUAGE_MODEL,\n",
    "    'pretrained_gpt2_path': PRETRAINED_GPT2_PATH,\n",
    "}\n",
    "\n",
    "for k,v in model_hyperparams.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "model = FlamingoModule(**model_hyperparams)\n",
    "#CHECKPOINT_PATH = \"/Users/caghankoksal/Desktop/SS2022/lightning_logs/version_23/checkpoints/epoch=34-val_loss=0.25-other_metric=0.00.ckpt\"\n",
    "# Pretrained on ROCO FineTuning on ImageCLEF 2021\n",
    "CHECKPOINT_PATH = \"/Users/caghankoksal/Desktop/SS2022/lightning_logs/version_31/checkpoints/epoch=28-val_loss=0.23-other_metric=0.00.ckpt\"\n",
    "model.load_state_dict(torch.load(CHECKPOINT_PATH,map_location=torch.device('cpu'))[\"state_dict\"])\n",
    "model.eval()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = imageclef_datamodule.val_dataloader()\n",
    "val_loader_iter = iter(val_dataloader)\n",
    "tokenizer = imageclef_datamodule.train_dataset.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "#batch = next(val_loader_iter)\n",
    "out = beam_decode(batch,model)\n",
    "\n",
    "models_answer = tokenizer.decode(out[0][-1][0][0]).split('answer:')[1].split('<EOC>')[0].rstrip().strip()\n",
    "correct_answer = batch[\"answer\"][0].rstrip().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_answer,correct_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE BEAM SEARCH\n",
    "\n",
    "\n",
    "\n",
    "correct = 0\n",
    "val_dataloader = imageclef_datamodule.val_dataloader()\n",
    "tokenizer = imageclef_datamodule.train_dataset.tokenizer\n",
    "true_predictions = []\n",
    "false_predictions = []\n",
    "for batch in tqdm(val_dataloader):\n",
    "    #batch = next(val_loader_iter)\n",
    "    out = beam_decode(batch,model,top_k=2,beam_width=2,max_len=100)\n",
    "\n",
    "    models_answer = tokenizer.decode(out[0][-1][0][0]).split('answer:')[1].split('<EOC>')[0].rstrip().strip()\n",
    "    correct_answer = batch[\"answer\"][0].rstrip().strip()\n",
    "    if models_answer == correct_answer:\n",
    "        correct += 1\n",
    "        true_predictions.append((models_answer, correct_answer))\n",
    "    else:\n",
    "        false_predictions.append((models_answer, correct_answer))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct/len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_predictions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "questions = [ batch[\"question\"] for batch  in imageclef_datamodule.val_dataloader() ]\n",
    "correct_preds_pred = pd.DataFrame(true_predictions, columns=[\"models_answer\",\"correct_answer\"])\n",
    "false_preds_pred = pd.DataFrame(false_predictions, columns=[\"models_answer\",\"correct_answer\"])\n",
    "questions = pd.DataFrame(questions, columns=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_false_col = pd.DataFrame([ \"True\"  if i <len(correct_preds_pred) else \"False\"  for  i in range(len(questions)) ], columns=[\"True/False\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "concat_list = pd.concat([ correct_preds_pred, false_preds_pred], axis=0).reset_index().drop(columns=[\"index\"])\n",
    "concat_list = pd.concat([questions,concat_list, true_false_col], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "cur_bleu = 0\n",
    "for pred, target in zip(concat_list[\"models_answer\"],concat_list[\"correct_answer\"]):\n",
    "    try:\n",
    "        cur_bleu += sentence_bleu( [target.split()], pred.split(), weights = [1])\n",
    "    except:\n",
    "        print(pred, target)\n",
    "\n",
    "cur_bleu/len(concat_list[\"models_answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_list.to_excel(\"../reports/predictions_all_imageclef_version31_roco_beam_decode_blue_0.1766_width_2_top_k_1_.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, install Gradio\n",
    "!pip install --quiet gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from transformers import GPT2Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn as nn\n",
    "import torch.nn.functional as F\n",
    "def generate_gradio(image, context, cur_model, ntok=20):\n",
    "    for _ in range(ntok):\n",
    "        out= cur_model({'image': image,'input_ids': context})\n",
    "        logits = out[:, -1, :]\n",
    "        indices_to_remove = logits < torch.topk(logits, 10)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = np.NINF\n",
    "        #next_tok1 = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1).squeeze(1)\n",
    "        #print(next_tok1.shape)\n",
    "        softmax_out = F.softmax(logits, dim=-1)\n",
    "        #print(softmax_out.shape)\n",
    "        next_tok = torch.argmax(softmax_out,dim=-1,keepdim=False)\n",
    "        #print(next_tok.shape)\n",
    "        context = torch.cat([context, next_tok.unsqueeze(-1)], dim=-1)\n",
    "    return context\n",
    "\n",
    "\n",
    "tokenizer = datamodule.train_dataset.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_gradio(image, question):\n",
    "    print(\"Input question\")\n",
    "    process_img = augmentations[\"val\"](image).unsqueeze(0)\n",
    "    print(\"Process_img succesfull\")\n",
    "    context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+question + ' <EOQ>'+ ' answer:')]) \n",
    "    out = generate_gradio( process_img,context, model, ntok=20)\n",
    "    #print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "    result = tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0]\n",
    "    return result\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the current disease\"\n",
    "read_im = Image.open(\"synpic16279.jpg\")\n",
    "process_img = augmentations[\"val\"](read_im).unsqueeze(0)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+question + ' <EOQ>'+ ' answer:')]) \n",
    "out = generate_gradio( process_img,context, model,20)\n",
    "    #print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "result = tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'pretrained_clip_path': PRETRAINED_CLIP_PATH,\n",
    "    'warmup_steps': 30,\n",
    "    'num_tokens': NUM_TOKENS,\n",
    "    'dim': FLAMINGO_EMBED_DIM,\n",
    "    'depth': DEPTH,\n",
    "    'num_heads': NUM_HEADS,\n",
    "    'dim_head': ATT_HEAD_DIM,\n",
    "    'cross_attn_every': CROOS_ATT_EVERY,\n",
    "    'media_token_id': MEDIA_TOKEN_ID,\n",
    "    'perceiver_num_latents': PERCEIVER_NUM_LATENTS,\n",
    "    'perceiver_depth': PERCEIVER_DEPTH,\n",
    "    'image_encoder': IMAGE_ENCODER,\n",
    "    'language_model': LANGUAGE_MODEL,\n",
    "    'pretrained_gpt2_path': PRETRAINED_GPT2_PATH,\n",
    "    'classification_mode': False,\n",
    "    'classification_num_classes': NUM_CLASSES,  # 332 if DATASET==\"IMAGECLEF\"\n",
    "    'flamingo_mode': FLAMINGO_MODE,\n",
    "    \"label_smoothing\": LABEL_SMOOTHING,\n",
    "    \"token_label_smoothing\": TOKEN_LABEL_SMOOTHING,\n",
    "    \"learning_rate\":LEARNING_RATE,\n",
    "    \"use_image_embeddings\": USE_IMAGE_EMBEDDINGS,\n",
    "    \"train_embedding_layer\": TRAIN_EMBEDDING_LAYER,\n",
    "    \"classifier_dropout\": CLASSIFIER_DROPOUT\n",
    "    }\n",
    "\n",
    "print_hyperparams(hyperparams)\n",
    "\n",
    "model = FlamingoModule(**hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_TRAINED_IMAGECLEF:\n",
    "    print(\"Pretrained Flamingo Model is loaded from checkpoint : \",IMAGECLEF_CHECKPOINT_PATH)\n",
    "    if os.getcwd().startswith('/home/mlmi-matthias'):\n",
    "        model.load_state_dict(torch.load(IMAGECLEF_CHECKPOINT_PATH)[\"state_dict\"],strict=False)\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(IMAGECLEF_CHECKPOINT_PATH,map_location=torch.device('cpu'))[\"state_dict\"],strict=False)\n",
    "        print(\"Checkpoint Weights are loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "title = \"Visual_Question_Answering on Medical Data ImageCLEF\"\n",
    "description = \"Gradio Demo for Medical Visual_Question_Answering. Upload your own image (high-resolution images are recommended) or click any one of the examples, and click \" \\\n",
    "              \"\\\"Submit\\\" and then wait for our's answer. \"\n",
    "article = \"<p style='text-align: center'><a href='https://github.com/OFA-Sys/OFA' target='_blank'>OFA Github \" \\\n",
    "          \"Repo</a></p> \"\n",
    "examples = [['demo_images/synpic16279.jpg','what is abnormal in the x-ray?' ], ['demo_images/synpic17959.jpg',  'what is most alarming about this mri?']]\n",
    "io = gr.Interface(fn=predict_gradio, inputs=[gr.inputs.Image(type='pil'), \"textbox\"], outputs=gr.outputs.Textbox(label=\"Answer\"),\n",
    "                 examples=examples,title=title, description=description,\n",
    "                  debug=True)\n",
    "io.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def calculator(num1, operation, num2):\n",
    "    if operation == \"add\":\n",
    "        return num1 + num2\n",
    "    elif operation == \"subtract\":\n",
    "        return num1 - num2\n",
    "    elif operation == \"multiply\":\n",
    "        return num1 * num2\n",
    "    elif operation == \"divide\":\n",
    "        return num1 / num2\n",
    "\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=calculator,\n",
    "    inputs=[\n",
    "            gr.Number(value=4), \n",
    "            gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \n",
    "            \"number\"\n",
    "            ],\n",
    "    outputs=\"number\",\n",
    "    examples=[\n",
    "        [5, \"add\", 3],\n",
    "        [4, \"divide\", 2],\n",
    "        [-4, \"multiply\", 2.5],\n",
    "        [0, \"subtract\", 1.2],\n",
    "    ],\n",
    "    title=\"test calculator\",\n",
    "    description=\"heres a sample **toy calculator**. enjoy!\",\n",
    ")\n",
    "\n",
    "demo.launch(share=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlmi",
   "language": "python",
   "name": "mlmi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
