{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import pytorch_lightning as pl\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM,PreTrainedTokenizerFast, GPT2Tokenizer\n",
    "from PIL import Image\n",
    "from tqdm import tqdm as tqdm\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from src.datasets.imageclef_dataset import ImageCLEF2021DataModule\n",
    "from src.models.multimodal.flamingo_module import FlamingoModule\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "augmentations = {\n",
    "    \n",
    "    'train': T.Compose([T.Resize((224,224)),\n",
    "                        T.ToTensor(),\n",
    "                        #T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "                        ]),\n",
    "    'val': T.Compose([T.Resize((224,224)),\n",
    "                        T.ToTensor(),\n",
    "                        T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))]),\n",
    "    'test': T.Compose([T.Resize((224,224)),\n",
    "                        T.ToTensor(),\n",
    "                        T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_params = {\n",
    "    \"root\": \"/Users/caghankoksal/Desktop/imageclef/\",\n",
    "    \"batch_size\": 32,\n",
    "    \"tokenizer\": \"gpt2\",\n",
    "    \"return_size\": False,\n",
    "    \"num_data_workers\": 0,\n",
    "    \"limit_num_samples\" : None\n",
    "}\n",
    "imageclef_datamodule = ImageCLEF2021DataModule(**dataset_params,transforms=augmentations,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = imageclef_datamodule.train_dataloader()\n",
    "val_loader = imageclef_datamodule.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    cur_images = batch['image']\n",
    "    print(cur_images.shape )\n",
    "    cur_batch = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|endoftext|> <image> question: is this a normal gastrointestinal image? answer: yes <EOC>',\n",
       " '<|endoftext|> <image> question: is the x-ray normal? answer: yes <EOC>',\n",
       " '<|endoftext|> <image> question: is this image normal? answer: yes <EOC>',\n",
       " '<|endoftext|> <image> question: is there an abnormality in the x-ray? answer: no <EOC>',\n",
       " '<|endoftext|> <image> question: is there evidence of any abnormalities? answer: no <EOC>',\n",
       " '<|endoftext|> <image> question: is there something wrong in the image? answer: no <EOC>',\n",
       " '<|endoftext|> <image> question: is there an abnormality in the mri? answer: no <EOC>',\n",
       " '<|endoftext|> <image> question: is there evidence of any abnormalities? answer: no <EOC>',\n",
       " '<|endoftext|> <image> question: is this a normal mri? answer: yes <EOC>',\n",
       " '<|endoftext|> <image> question: is the mri normal? answer: yes <EOC>',\n",
       " '<|endoftext|> <image> question: are there abnormalities in this ultrasound? answer: no <EOC>',\n",
       " '<|endoftext|> <image> question: is this a normal mri? answer: yes <EOC>',\n",
       " '<|endoftext|> <image> question: does this image look normal? answer: yes <EOC>',\n",
       " '<|endoftext|> <image> question: are there abnormalities in this ct scan? answer: no <EOC>',\n",
       " '<|endoftext|> <image> question: is there evidence of any abnormalities? answer: no <EOC>',\n",
       " '<|endoftext|> <image> question: is this image normal? answer: yes <EOC>',\n",
       " '<|endoftext|> <image> question: is this a normal mri? answer: yes <EOC>',\n",
       " '<|endoftext|> <image> question: is this image normal? answer: yes <EOC>',\n",
       " '<|endoftext|> <image> question: is the mri normal? answer: yes <EOC>',\n",
       " '<|endoftext|> <image> question: is the mri normal? answer: yes <EOC>',\n",
       " '<|endoftext|> <image> question: is the ultrasound normal? answer: yes <EOC>',\n",
       " '<|endoftext|> <image> question: is there evidence of any abnormalities? answer: no <EOC>',\n",
       " '<|endoftext|> <image> question: is there something wrong in the image? answer: no <EOC>',\n",
       " '<|endoftext|> <image> question: is this a normal x-ray? answer: yes <EOC>',\n",
       " '<|endoftext|> <image> question: is the x-ray normal? answer: yes <EOC>',\n",
       " '<|endoftext|> <image> question: is there an abnormality in the mri? answer: no <EOC>',\n",
       " '<|endoftext|> <image> question: is there something wrong in the image? answer: no <EOC>',\n",
       " '<|endoftext|> <image> question: is there something wrong in the image? answer: no <EOC>',\n",
       " '<|endoftext|> <image> question: is there something wrong in the image? answer: no <EOC>',\n",
       " '<|endoftext|> <image> question: is this a normal gastrointestinal image? answer: yes <EOC>',\n",
       " '<|endoftext|> <image> question: is this a normal x-ray? answer: no <EOC>',\n",
       " '<|endoftext|> <image> question: is this a normal mri? answer: no <EOC>']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_batch['qa_pair']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_batch[\"index_answer\"]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_batch[\"input_ids\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageclef_datamodule.train_dataset.tokenizer.decode(cur_batch[\"input_ids\"][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(0,15).view(3,-1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.tensor([1,2,3])\n",
    "\n",
    "idx = torch.column_stack([idx, idx+1])\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_batch[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anymatrix = torch.randn((3,3,3))\n",
    "anymatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anymatrix[range(1,2),:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(cur_batch[\"index_answer\"],cur_batch[\"index_eoc\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rangesss = torch.tensor([range(k+1,v) for k,v in zip(cur_batch[\"index_answer\"],cur_batch[\"index_eoc\"])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_batch[\"input_ids\"][:,:,:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_batch[\"qa_pair\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageclef_datamodule.train_dataset.tokenizer.decode([50256,   220, 50258,  1808,    25,   318,   428,  2939,  3487,    30,\n",
    "           3280,    25,  3763,   220, 50259, 50257, 50257, 50257, 50257, 50257][12:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_batch[\"token_type_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rangesss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack(rangesss,dim=1).transpose(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_batch[\"token_type_ids\"].squeeze(1)[torch.stack(rangesss,dim=1).transpose(1,0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_batch[\"token_type_ids\"].squeeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rangesss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_batch[\"token_type_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rangesss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fours.squeeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fours = 4*torch.ones_like(cur_batch[\"token_type_ids\"])\n",
    "out = (cur_batch[\"token_type_ids\"].squeeze(1)).scatter_(1, rangesss, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[0,10: 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rangesss = [range(k+1,v+1) for k,v in zip(cur_batch[\"index_answer\"],cur_batch[\"index_eoc\"])]\n",
    "rangesss = torch.tensor(rangesss)\n",
    "torch.gather(cur_batch[\"token_type_ids\"].squeeze(1), 1, rangesss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_indices = torch.column_stack(range([cur_batch[\"index_answer\"]+1,cur_batch[\"index_eoc\"]))]) \n",
    "answer_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_batch[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.gather(cur_batch[\"input_ids\"], 2, answer_indices.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_batch[\"index_answer\"].int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_batch[\"token_type_ids\"][torch.arange(32),0,cur_batch[\"index_answer\"]:cur_batch[\"index_answer\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_batch[\"token_type_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_batch[\"token_type_ids\"][cur_batch[\"index_answer\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import pytorch_lightning as pl\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM,PreTrainedTokenizerFast, GPT2Tokenizer\n",
    "from PIL import Image\n",
    "from tqdm import tqdm as tqdm\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from src.datasets.imageclef_dataset import ImageCLEF2021DataModule\n",
    "from src.models.multimodal.flamingo_module import FlamingoModule\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE_OF_TOKENIZER = 50257 #Â mimic_datamodule.train_dataset.tokenizer.vocab_size\n",
    "LANGUAGE_MODEL = 'gpt2'\n",
    "NUM_TOKENS = VOCAB_SIZE_OF_TOKENIZER +3 if LANGUAGE_MODEL==\"gpt2\" else 31092\n",
    "FLAMINGO_EMBED_DIM = 768\n",
    "DEPTH = 12\n",
    "NUM_HEADS = 8\n",
    "ATT_HEAD_DIM = 64\n",
    "CROOS_ATT_EVERY=3\n",
    "MEDIA_TOKEN_ID = imageclef_datamodule.train_dataset.tokenizer.all_special_ids[imageclef_datamodule.train_dataset.tokenizer.all_special_tokens.index('<image>')]\n",
    "PERCEIVER_NUM_LATENTS = 64\n",
    "PERCEIVER_DEPTH = 2\n",
    "IMAGE_ENCODER = \"clip\"\n",
    "PRETRAINED_CLIP_PATH = '/Users/caghankoksal/Desktop/development/PubMedCLIP_ViT32.pth'\n",
    "PRETRAINED_GPT2_PATH = \"/Users/caghankoksal/Desktop/development/TransformerPlay/gpt2-pytorch_model.bin\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained_clip_path: /Users/caghankoksal/Desktop/development/PubMedCLIP_ViT32.pth\n",
      "warmup_steps: 569\n",
      "num_tokens: 50260\n",
      "dim: 768\n",
      "depth: 12\n",
      "num_heads: 8\n",
      "dim_head: 64\n",
      "cross_attn_every: 3\n",
      "media_token_id: 50258\n",
      "perceiver_num_latents: 64\n",
      "perceiver_depth: 2\n",
      "image_encoder: clip\n",
      "language_model: gpt2\n",
      "pretrained_gpt2_path: /Users/caghankoksal/Desktop/development/TransformerPlay/gpt2-pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model_hyperparams = {\n",
    "    'pretrained_clip_path': PRETRAINED_CLIP_PATH,\n",
    "    'warmup_steps': 569,\n",
    "    'num_tokens': NUM_TOKENS,\n",
    "    'dim': FLAMINGO_EMBED_DIM,\n",
    "    'depth': DEPTH,\n",
    "    'num_heads': NUM_HEADS,\n",
    "    'dim_head': ATT_HEAD_DIM,\n",
    "    'cross_attn_every': CROOS_ATT_EVERY,\n",
    "    'media_token_id': MEDIA_TOKEN_ID,\n",
    "    'perceiver_num_latents': PERCEIVER_NUM_LATENTS,\n",
    "    'perceiver_depth': PERCEIVER_DEPTH,\n",
    "    'image_encoder': IMAGE_ENCODER,\n",
    "    'language_model': LANGUAGE_MODEL,\n",
    "    'pretrained_gpt2_path': PRETRAINED_GPT2_PATH,\n",
    "}\n",
    "\n",
    "for k,v in model_hyperparams.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /var/folders/61/9c2llh9n2pjb81c4dmhmb67w0000gn/T/tmp6nmvpxcn\n",
      "INFO:torch.distributed.nn.jit.instantiator:Writing /var/folders/61/9c2llh9n2pjb81c4dmhmb67w0000gn/T/tmp6nmvpxcn/_remote_module_non_sriptable.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained clip is being loaded\n",
      "Flamingo is being initialized with  gpt2  as language model\n",
      "GPT 2 Weights are loading...\n",
      "Loaded GPT2 weights and Embeddings num_weights loaded :  156\n"
     ]
    }
   ],
   "source": [
    "model = FlamingoModule(**model_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "trainer = pl.Trainer(max_epochs=6,\n",
    "                     accelerator=\"cpu\", devices=1,\n",
    "                     callbacks=[lr_monitor],\n",
    "                     log_every_n_steps=1,\n",
    "                      )\n",
    "\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FlamingoModule(\n",
       "  (flamingo_palm): FlamingoModel(\n",
       "    (token_emb): Embedding(50260, 768)\n",
       "    (img_encoder): VisionTransformer(\n",
       "      (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (transformer): Transformer(\n",
       "        (resblocks): ModuleList(\n",
       "          (0): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (perceiver_resampler): PerceiverResampler(\n",
       "      (layers): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): PerceiverAttention(\n",
       "            (norm_media): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm_latents): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (to_q): Linear(in_features=768, out_features=512, bias=False)\n",
       "            (to_kv): Linear(in_features=768, out_features=1024, bias=False)\n",
       "            (to_out): Linear(in_features=512, out_features=768, bias=False)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (2): GELU()\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (1): ModuleList(\n",
       "          (0): PerceiverAttention(\n",
       "            (norm_media): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm_latents): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (to_q): Linear(in_features=768, out_features=512, bias=False)\n",
       "            (to_kv): Linear(in_features=768, out_features=1024, bias=False)\n",
       "            (to_out): Linear(in_features=512, out_features=768, bias=False)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (2): GELU()\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (img_encoder_outdim_layer): Linear(in_features=512, out_features=768, bias=True)\n",
       "    (layers): ModuleList(\n",
       "      (0): ModuleList(\n",
       "        (0): Residual(\n",
       "          (fn): TransformerBlockGPT2(\n",
       "            (attn): Attention(\n",
       "              (c_attn): Conv1D()\n",
       "              (softmax): Softmax(dim=-1)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (c_proj): Conv1D()\n",
       "            )\n",
       "            (feedforward): FeedForward(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): GatedCrossAttentionBlock(\n",
       "          (attn): MaskedCrossAttention(\n",
       "            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (to_q): Linear(in_features=768, out_features=512, bias=False)\n",
       "            (to_kv): Linear(in_features=768, out_features=1024, bias=False)\n",
       "            (to_out): Linear(in_features=512, out_features=768, bias=False)\n",
       "          )\n",
       "          (ff): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (2): GELU()\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): ModuleList(\n",
       "        (0): Residual(\n",
       "          (fn): TransformerBlockGPT2(\n",
       "            (attn): Attention(\n",
       "              (c_attn): Conv1D()\n",
       "              (softmax): Softmax(dim=-1)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (c_proj): Conv1D()\n",
       "            )\n",
       "            (feedforward): FeedForward(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): None\n",
       "      )\n",
       "      (2): ModuleList(\n",
       "        (0): Residual(\n",
       "          (fn): TransformerBlockGPT2(\n",
       "            (attn): Attention(\n",
       "              (c_attn): Conv1D()\n",
       "              (softmax): Softmax(dim=-1)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (c_proj): Conv1D()\n",
       "            )\n",
       "            (feedforward): FeedForward(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): None\n",
       "      )\n",
       "      (3): ModuleList(\n",
       "        (0): Residual(\n",
       "          (fn): TransformerBlockGPT2(\n",
       "            (attn): Attention(\n",
       "              (c_attn): Conv1D()\n",
       "              (softmax): Softmax(dim=-1)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (c_proj): Conv1D()\n",
       "            )\n",
       "            (feedforward): FeedForward(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): GatedCrossAttentionBlock(\n",
       "          (attn): MaskedCrossAttention(\n",
       "            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (to_q): Linear(in_features=768, out_features=512, bias=False)\n",
       "            (to_kv): Linear(in_features=768, out_features=1024, bias=False)\n",
       "            (to_out): Linear(in_features=512, out_features=768, bias=False)\n",
       "          )\n",
       "          (ff): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (2): GELU()\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): ModuleList(\n",
       "        (0): Residual(\n",
       "          (fn): TransformerBlockGPT2(\n",
       "            (attn): Attention(\n",
       "              (c_attn): Conv1D()\n",
       "              (softmax): Softmax(dim=-1)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (c_proj): Conv1D()\n",
       "            )\n",
       "            (feedforward): FeedForward(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): None\n",
       "      )\n",
       "      (5): ModuleList(\n",
       "        (0): Residual(\n",
       "          (fn): TransformerBlockGPT2(\n",
       "            (attn): Attention(\n",
       "              (c_attn): Conv1D()\n",
       "              (softmax): Softmax(dim=-1)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (c_proj): Conv1D()\n",
       "            )\n",
       "            (feedforward): FeedForward(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): None\n",
       "      )\n",
       "      (6): ModuleList(\n",
       "        (0): Residual(\n",
       "          (fn): TransformerBlockGPT2(\n",
       "            (attn): Attention(\n",
       "              (c_attn): Conv1D()\n",
       "              (softmax): Softmax(dim=-1)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (c_proj): Conv1D()\n",
       "            )\n",
       "            (feedforward): FeedForward(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): GatedCrossAttentionBlock(\n",
       "          (attn): MaskedCrossAttention(\n",
       "            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (to_q): Linear(in_features=768, out_features=512, bias=False)\n",
       "            (to_kv): Linear(in_features=768, out_features=1024, bias=False)\n",
       "            (to_out): Linear(in_features=512, out_features=768, bias=False)\n",
       "          )\n",
       "          (ff): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (2): GELU()\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): ModuleList(\n",
       "        (0): Residual(\n",
       "          (fn): TransformerBlockGPT2(\n",
       "            (attn): Attention(\n",
       "              (c_attn): Conv1D()\n",
       "              (softmax): Softmax(dim=-1)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (c_proj): Conv1D()\n",
       "            )\n",
       "            (feedforward): FeedForward(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): None\n",
       "      )\n",
       "      (8): ModuleList(\n",
       "        (0): Residual(\n",
       "          (fn): TransformerBlockGPT2(\n",
       "            (attn): Attention(\n",
       "              (c_attn): Conv1D()\n",
       "              (softmax): Softmax(dim=-1)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (c_proj): Conv1D()\n",
       "            )\n",
       "            (feedforward): FeedForward(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): None\n",
       "      )\n",
       "      (9): ModuleList(\n",
       "        (0): Residual(\n",
       "          (fn): TransformerBlockGPT2(\n",
       "            (attn): Attention(\n",
       "              (c_attn): Conv1D()\n",
       "              (softmax): Softmax(dim=-1)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (c_proj): Conv1D()\n",
       "            )\n",
       "            (feedforward): FeedForward(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): GatedCrossAttentionBlock(\n",
       "          (attn): MaskedCrossAttention(\n",
       "            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (to_q): Linear(in_features=768, out_features=512, bias=False)\n",
       "            (to_kv): Linear(in_features=768, out_features=1024, bias=False)\n",
       "            (to_out): Linear(in_features=512, out_features=768, bias=False)\n",
       "          )\n",
       "          (ff): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (2): GELU()\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): ModuleList(\n",
       "        (0): Residual(\n",
       "          (fn): TransformerBlockGPT2(\n",
       "            (attn): Attention(\n",
       "              (c_attn): Conv1D()\n",
       "              (softmax): Softmax(dim=-1)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (c_proj): Conv1D()\n",
       "            )\n",
       "            (feedforward): FeedForward(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): None\n",
       "      )\n",
       "      (11): ModuleList(\n",
       "        (0): Residual(\n",
       "          (fn): TransformerBlockGPT2(\n",
       "            (attn): Attention(\n",
       "              (c_attn): Conv1D()\n",
       "              (softmax): Softmax(dim=-1)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (c_proj): Conv1D()\n",
       "            )\n",
       "            (feedforward): FeedForward(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): None\n",
       "      )\n",
       "    )\n",
       "    (to_logits): Sequential(\n",
       "      (0): LayerNorm()\n",
       "      (1): Linear(in_features=768, out_features=50260, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHECKPOINT_PATH = \"/Users/caghankoksal/Desktop/SS2022/lightning_logs/version_23/checkpoints/epoch=34-val_loss=0.25-other_metric=0.00.ckpt\"\n",
    "model.load_state_dict(torch.load(CHECKPOINT_PATH,map_location=torch.device('cpu'))[\"state_dict\"])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val_img = imageclef_datamodule.val_dataset[3][\"image\"]\n",
    "val_qa_pair= imageclef_datamodule.val_dataset[3][\"qa_pair\"]\n",
    "val_qa_pair.split('answer')\n",
    "val_question =  imageclef_datamodule.val_dataset[3][\"question\"]\n",
    "val_question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_answer =  imageclef_datamodule.val_dataset[3][\"answer\"]\n",
    "val_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from transformers import GPT2Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn as nn\n",
    "import torch.nn.functional as F\n",
    "def generate(image,context, cur_model, ntok=20):\n",
    "    for _ in range(ntok):\n",
    "        out = cur_model({'image': image,'input_ids': context })\n",
    "        logits = out[:, -1, :]\n",
    "        indices_to_remove = logits < torch.topk(logits, 10)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = np.NINF\n",
    "        #next_tok1 = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1).squeeze(1)\n",
    "        #print(next_tok1.shape)\n",
    "        softmax_out = F.softmax(logits, dim=-1)\n",
    "        next_tok = torch.argmax(softmax_out)\n",
    "        print(next_tok.shape)\n",
    "        context = torch.cat([context, next_tok.unsqueeze(-1)], dim=-1)\n",
    "    return context\n",
    "\n",
    "\n",
    "tokenizer = imageclef_datamodule.train_dataset.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_params = {\n",
    "    \"root\": \"/Users/caghankoksal/Desktop/imageclef/\",\n",
    "    \"batch_size\": 1,\n",
    "    \"tokenizer\": \"gpt2\",\n",
    "    \"return_size\": False,\n",
    "    \"num_data_workers\": 0,\n",
    "    \"limit_num_samples\" : None\n",
    "}\n",
    "imageclef_datamodule = ImageCLEF2021DataModule(**dataset_params,transforms=augmentations,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = imageclef_datamodule.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader_iter = iter(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what abnormality is seen in the image?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"question\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"image\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 2 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/caghankoksal/Desktop/SS2022/mlmi-vqa/notebooks/imageclef_flamingo_clip_gpt2_playground.ipynb Cell 66'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/caghankoksal/Desktop/SS2022/mlmi-vqa/notebooks/imageclef_flamingo_clip_gpt2_playground.ipynb#ch0000065?line=0'>1</a>\u001b[0m batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(val_loader_iter)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/caghankoksal/Desktop/SS2022/mlmi-vqa/notebooks/imageclef_flamingo_clip_gpt2_playground.ipynb#ch0000065?line=1'>2</a>\u001b[0m context   \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([tokenizer\u001b[39m.\u001b[39mencode(\u001b[39m\"\u001b[39m\u001b[39m<|endoftext|> <image> question: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mbatch[\u001b[39m\"\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m answer:\u001b[39m\u001b[39m'\u001b[39m)]) \n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/caghankoksal/Desktop/SS2022/mlmi-vqa/notebooks/imageclef_flamingo_clip_gpt2_playground.ipynb#ch0000065?line=2'>3</a>\u001b[0m out \u001b[39m=\u001b[39m generate(batch[\u001b[39m\"\u001b[39;49m\u001b[39mimage\u001b[39;49m\u001b[39m\"\u001b[39;49m], context, model, ntok\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/caghankoksal/Desktop/SS2022/mlmi-vqa/notebooks/imageclef_flamingo_clip_gpt2_playground.ipynb#ch0000065?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mModel\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms answer : \u001b[39m\u001b[39m\"\u001b[39m,tokenizer\u001b[39m.\u001b[39mdecode(out[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39manswer:\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m<EOC>\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/caghankoksal/Desktop/SS2022/mlmi-vqa/notebooks/imageclef_flamingo_clip_gpt2_playground.ipynb#ch0000065?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCorrect Answer: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m])\n",
      "\u001b[1;32m/Users/caghankoksal/Desktop/SS2022/mlmi-vqa/notebooks/imageclef_flamingo_clip_gpt2_playground.ipynb Cell 59'\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(image, context, cur_model, ntok)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/caghankoksal/Desktop/SS2022/mlmi-vqa/notebooks/imageclef_flamingo_clip_gpt2_playground.ipynb#ch0000058?line=17'>18</a>\u001b[0m     next_tok \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(F\u001b[39m.\u001b[39msoftmax(logits, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/caghankoksal/Desktop/SS2022/mlmi-vqa/notebooks/imageclef_flamingo_clip_gpt2_playground.ipynb#ch0000058?line=18'>19</a>\u001b[0m     \u001b[39mprint\u001b[39m(next_tok\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/caghankoksal/Desktop/SS2022/mlmi-vqa/notebooks/imageclef_flamingo_clip_gpt2_playground.ipynb#ch0000058?line=19'>20</a>\u001b[0m     context \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat([context, next_tok\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)], dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/caghankoksal/Desktop/SS2022/mlmi-vqa/notebooks/imageclef_flamingo_clip_gpt2_playground.ipynb#ch0000058?line=20'>21</a>\u001b[0m \u001b[39mreturn\u001b[39;00m context\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 2 and 1"
     ]
    }
   ],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, ntok=20)\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, ntok=20)\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, ntok=20)\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, ntok=20)\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, ntok=20)\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, ntok=20)\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, ntok=20)\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, ntok=20)\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, ntok=20)\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, ntok=20)\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, ntok=20)\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_loader_iter)\n",
    "context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' answer:')]) \n",
    "out = generate(batch[\"image\"], context, model, ntok=20)\n",
    "print(\"Model's answer : \",tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0])\n",
    "print(\"Correct Answer: \" + batch[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_params = {\n",
    "    \"root\": \"/Users/caghankoksal/Desktop/imageclef/\",\n",
    "    \"batch_size\": 1,\n",
    "    \"tokenizer\": \"gpt2\",\n",
    "    \"return_size\": False,\n",
    "    \"num_data_workers\": 0,\n",
    "    \"limit_num_samples\" : None\n",
    "}\n",
    "imageclef_datamodule = ImageCLEF2021DataModule(**dataset_params,transforms=augmentations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE_OF_TOKENIZER = 50257 #Â mimic_datamodule.train_dataset.tokenizer.vocab_size\n",
    "LANGUAGE_MODEL = 'gpt2'\n",
    "NUM_TOKENS = VOCAB_SIZE_OF_TOKENIZER +3 if LANGUAGE_MODEL==\"gpt2\" else 31092\n",
    "FLAMINGO_EMBED_DIM = 768\n",
    "DEPTH = 12\n",
    "NUM_HEADS = 8\n",
    "ATT_HEAD_DIM = 64\n",
    "CROOS_ATT_EVERY=3\n",
    "MEDIA_TOKEN_ID = imageclef_datamodule.train_dataset.tokenizer.all_special_ids[imageclef_datamodule.train_dataset.tokenizer.all_special_tokens.index('<image>')]\n",
    "PERCEIVER_NUM_LATENTS = 64\n",
    "PERCEIVER_DEPTH = 2\n",
    "IMAGE_ENCODER = \"clip\"\n",
    "PRETRAINED_CLIP_PATH = '/Users/caghankoksal/Desktop/development/PubMedCLIP_ViT32.pth'\n",
    "PRETRAINED_GPT2_PATH = \"/Users/caghankoksal/Desktop/development/TransformerPlay/gpt2-pytorch_model.bin\"\n",
    "\n",
    "model_hyperparams = {\n",
    "    'pretrained_clip_path': PRETRAINED_CLIP_PATH,\n",
    "    'warmup_steps': 569,\n",
    "    'num_tokens': NUM_TOKENS,\n",
    "    'dim': FLAMINGO_EMBED_DIM,\n",
    "    'depth': DEPTH,\n",
    "    'num_heads': NUM_HEADS,\n",
    "    'dim_head': ATT_HEAD_DIM,\n",
    "    'cross_attn_every': CROOS_ATT_EVERY,\n",
    "    'media_token_id': MEDIA_TOKEN_ID,\n",
    "    'perceiver_num_latents': PERCEIVER_NUM_LATENTS,\n",
    "    'perceiver_depth': PERCEIVER_DEPTH,\n",
    "    'image_encoder': IMAGE_ENCODER,\n",
    "    'language_model': LANGUAGE_MODEL,\n",
    "    'pretrained_gpt2_path': PRETRAINED_GPT2_PATH,\n",
    "}\n",
    "\n",
    "for k,v in model_hyperparams.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' answer:'\n",
    "print({x : tokenizer.encode(x) for x in sentence.split()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = tokenizer(sentence, return_offsets_mapping=True)\n",
    "for token_id, pos in zip(encodings['input_ids'], encodings['offset_mapping']):\n",
    "    print(token_id, pos, sentence[pos[0]:pos[1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print({x : tokenizer.encode(x) for x in sentence.split()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FlamingoModule(**model_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = \"/Users/caghankoksal/Desktop/SS2022/lightning_logs/version_23/checkpoints/epoch=34-val_loss=0.25-other_metric=0.00.ckpt\"\n",
    "model.load_state_dict(torch.load(CHECKPOINT_PATH,map_location=torch.device('cpu'))[\"state_dict\"])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from transformers import GPT2Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn as nn\n",
    "import torch.nn.functional as F\n",
    "def generate(image,context, cur_model, ntok=20):\n",
    "    for _ in range(ntok):\n",
    "        out = cur_model({'image': image,'input_ids': context })\n",
    "        logits = out[:, -1, :]\n",
    "        indices_to_remove = logits < torch.topk(logits, 10)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = np.NINF\n",
    "        next_tok = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1).squeeze(1)\n",
    "        context = torch.cat([context, next_tok.unsqueeze(-1)], dim=-1)\n",
    "    return context\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "val_dataloader = imageclef_datamodule.val_dataloader()\n",
    "tokenizer = imageclef_datamodule.train_dataset.tokenizer\n",
    "true_predictions = []\n",
    "false_predictions = []\n",
    "for batch in tqdm(val_dataloader):\n",
    "    #batch = next(val_loader_iter)\n",
    "    context   = torch.tensor([tokenizer.encode(\"<|endoftext|> <image> question: \"+batch[\"question\"][0] + ' answer:')]) \n",
    "    out = generate(batch[\"image\"], context, model, ntok=20)\n",
    "    models_answer = tokenizer.decode(out[0]).split('answer:')[1].split('<EOC>')[0].rstrip().strip()\n",
    "    correct_answer = batch[\"answer\"][0].rstrip().strip()\n",
    "    if models_answer == correct_answer:\n",
    "        correct += 1\n",
    "        true_predictions.append((models_answer, correct_answer))\n",
    "    else:\n",
    "        false_predictions.append((models_answer, correct_answer))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct/len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "correct_preds_pred = pd.DataFrame(true_predictions, columns=[\"models_answer\",\"correct_answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_preds_pred.to_excel('correct_predictions2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_preds_pred = pd.DataFrame(false_predictions, columns=[\"models_answer\",\"correct_answer\"]).to_excel(\"false_predictions2.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('torch-nightly')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "892a7f8aeabe86b99d45932805d162784b758c544538f3ce4737e4a115db3cfd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
